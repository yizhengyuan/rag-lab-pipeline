#!/usr/bin/env python3
"""
Tokené™åˆ¶ä¿®å¤æµ‹è¯•è„šæœ¬
=====================================

ç”¨äºéªŒè¯chunkingæ¨¡å—çš„tokené™åˆ¶ä¿®å¤æ˜¯å¦æœ‰æ•ˆ
"""

import sys
import logging
from pathlib import Path
from llama_index.core import Document

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—
from core.chunking import SemanticChunker
from utils.config_loader import ConfigLoader

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('token_fix_test.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

def test_token_limit_fix():
    """æµ‹è¯•tokené™åˆ¶ä¿®å¤"""
    
    logger.info("ğŸ§ª å¼€å§‹Tokené™åˆ¶ä¿®å¤æµ‹è¯•")
    logger.info("=" * 60)
    
    try:
        # 1. åŠ è½½é…ç½®
        logger.info("ğŸ“‹ æ­¥éª¤1: åŠ è½½é…ç½®...")
        config = ConfigLoader.load_config()
        logger.info(f"âœ… é…ç½®åŠ è½½æˆåŠŸ")
        logger.info(f"   - æœ€å¤§tokens: {config.get('chunking.max_tokens_per_chunk', 6000)}")
        logger.info(f"   - æœ€å¤§å­—ç¬¦: {config.get('chunking.max_char_per_chunk', 24000)}")
        logger.info(f"   - å¯ç”¨éªŒè¯: {config.get('chunking.enable_token_validation', True)}")
        
        # 2. åˆ›å»ºæµ‹è¯•æ–‡æ¡£
        logger.info("\nğŸ“„ æ­¥éª¤2: åˆ›å»ºæµ‹è¯•æ–‡æ¡£...")
        
        # åˆ›å»ºä¸€ä¸ªè¶…é•¿çš„æµ‹è¯•æ–‡æ¡£
        large_text = """
è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£ï¼Œç”¨äºéªŒè¯tokené™åˆ¶ä¿®å¤ã€‚""" * 2000  # é‡å¤å¾ˆå¤šæ¬¡æ¥åˆ›å»ºè¶…é•¿æ–‡æœ¬
        
        small_text = "è¿™æ˜¯ä¸€ä¸ªæ­£å¸¸å¤§å°çš„æµ‹è¯•æ–‡æ¡£ã€‚"
        
        test_documents = [
            Document(
                text=large_text,
                metadata={"source": "large_test_doc", "type": "stress_test"}
            ),
            Document(
                text=small_text,
                metadata={"source": "small_test_doc", "type": "normal_test"}
            )
        ]
        
        logger.info(f"âœ… åˆ›å»ºäº† {len(test_documents)} ä¸ªæµ‹è¯•æ–‡æ¡£")
        logger.info(f"   - å¤§æ–‡æ¡£å­—ç¬¦æ•°: {len(large_text)}")
        logger.info(f"   - å°æ–‡æ¡£å­—ç¬¦æ•°: {len(small_text)}")
        
        # 3. åˆå§‹åŒ–Chunker
        logger.info("\nğŸ”§ æ­¥éª¤3: åˆå§‹åŒ–è¯­ä¹‰åˆ†å—å™¨...")
        chunker = SemanticChunker(config)
        logger.info("âœ… åˆ†å—å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # 4. æ‰§è¡Œåˆ†å—æµ‹è¯•
        logger.info("\nâœ‚ï¸ æ­¥éª¤4: æ‰§è¡Œåˆ†å—å’Œæ¦‚å¿µæå–...")
        chunk_nodes = chunker.chunk_and_extract_concepts(test_documents)
        
        # 5. åˆ†æç»“æœ
        logger.info(f"\nğŸ“Š æ­¥éª¤5: åˆ†ææµ‹è¯•ç»“æœ...")
        logger.info(f"âœ… æˆåŠŸç”Ÿæˆ {len(chunk_nodes)} ä¸ªchunk")
        
        if chunk_nodes:
            # åˆ†æchunkå¤§å°åˆ†å¸ƒ
            token_counts = []
            char_counts = []
            oversized_chunks = 0
            max_tokens = config.get('chunking.max_tokens_per_chunk', 6000)
            
            for i, node in enumerate(chunk_nodes):
                token_count = chunker._count_tokens(node.text)
                char_count = len(node.text)
                
                token_counts.append(token_count)
                char_counts.append(char_count)
                
                if token_count > max_tokens:
                    oversized_chunks += 1
                    logger.warning(f"âš ï¸ Chunk {i} ä»ç„¶è¶…è¿‡é™åˆ¶: {token_count} tokens")
                else:
                    logger.debug(f"âœ… Chunk {i}: {token_count} tokens, {char_count} å­—ç¬¦")
            
            # ç»Ÿè®¡ä¿¡æ¯
            logger.info(f"\nğŸ“ˆ Chunkå¤§å°ç»Ÿè®¡:")
            logger.info(f"   - æ€»chunkæ•°: {len(chunk_nodes)}")
            logger.info(f"   - å¹³å‡tokenæ•°: {sum(token_counts) / len(token_counts):.1f}")
            logger.info(f"   - æœ€å¤§tokenæ•°: {max(token_counts)}")
            logger.info(f"   - æœ€å°tokenæ•°: {min(token_counts)}")
            logger.info(f"   - è¶…è¿‡é™åˆ¶çš„chunkæ•°: {oversized_chunks}")
            logger.info(f"   - æˆåŠŸç‡: {((len(chunk_nodes) - oversized_chunks) / len(chunk_nodes) * 100):.1f}%")
            
            # æ£€æŸ¥ç´¢å¼•åˆ›å»º
            logger.info(f"\nğŸ” æ­¥éª¤6: æ£€æŸ¥å‘é‡ç´¢å¼•...")
            if chunker.chunk_index is not None:
                logger.info("âœ… å‘é‡ç´¢å¼•åˆ›å»ºæˆåŠŸ")
            else:
                logger.error("âŒ å‘é‡ç´¢å¼•åˆ›å»ºå¤±è´¥")
            
        else:
            logger.error("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•chunk")
            return False
        
        # 6. æµ‹è¯•ç»“è®º
        logger.info(f"\nğŸ‰ æµ‹è¯•ç»“è®º:")
        if oversized_chunks == 0 and chunker.chunk_index is not None:
            logger.info("âœ… Tokené™åˆ¶ä¿®å¤æµ‹è¯•é€šè¿‡ï¼")
            logger.info("   - æ‰€æœ‰chunkéƒ½åœ¨tokené™åˆ¶å†…")
            logger.info("   - å‘é‡ç´¢å¼•åˆ›å»ºæˆåŠŸ")
            logger.info("   - æ²¡æœ‰å‡ºç°'NoneType' object is not iterableé”™è¯¯")
            return True
        else:
            logger.error("âŒ Tokené™åˆ¶ä¿®å¤æµ‹è¯•å¤±è´¥")
            if oversized_chunks > 0:
                logger.error(f"   - ä»æœ‰ {oversized_chunks} ä¸ªchunkè¶…è¿‡é™åˆ¶")
            if chunker.chunk_index is None:
                logger.error("   - å‘é‡ç´¢å¼•åˆ›å»ºå¤±è´¥")
            return False
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª Tokené™åˆ¶ä¿®å¤æµ‹è¯•")
    print("=" * 40)
    
    success = test_token_limit_fix()
    
    if success:
        print("\nğŸ‰ æµ‹è¯•æˆåŠŸï¼ä¿®å¤ç”Ÿæ•ˆã€‚")
        sys.exit(0)
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚")
        sys.exit(1)

if __name__ == "__main__":
    main() 