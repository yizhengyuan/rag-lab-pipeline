#!/usr/bin/env python3
"""
æ­¥éª¤4: æ¦‚å¿µåˆå¹¶ä¸ä¼˜åŒ– - å¢å¼ºç‰ˆ
===============================

åŠŸèƒ½ï¼š
1. ä»æ­¥éª¤3çš„ç»“æœä¸­è¯»å–æ¦‚å¿µæ•°æ®
2. æ‰§è¡Œæ™ºèƒ½æ¦‚å¿µåˆå¹¶å’Œå»é‡
3. ä¼˜åŒ–æ¦‚å¿µè´¨é‡å’Œå±‚æ¬¡ç»“æ„
4. ä¿å­˜åˆ°åŒä¸€ä¸ªå®éªŒæ–‡ä»¶å¤¹

ç”¨æ³•: 
- python step4.py <step3è¾“å‡ºæ–‡ä»¶.txt>
- python step4.py <å®éªŒæ–‡ä»¶å¤¹è·¯å¾„>

æ–°åŠŸèƒ½ï¼š
- âœ… è‡ªåŠ¨è¯†åˆ«å¹¶ä½¿ç”¨step3çš„å®éªŒæ–‡ä»¶å¤¹
- âœ… æ™ºèƒ½æ¦‚å¿µåˆå¹¶å’Œç›¸ä¼¼åº¦è®¡ç®—
- âœ… æ¦‚å¿µå±‚æ¬¡ç»“æ„ä¼˜åŒ–
- âœ… ç»Ÿä¸€çš„å®éªŒç®¡ç†
- âœ… æ¦‚å¿µè´¨é‡è¯„ä¼°å’Œè¿‡æ»¤
"""

import sys
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Set, Tuple
from collections import Counter, defaultdict
import logging

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
sys.path.append(str(Path(__file__).parent))
from llama_index.core import Document
from llama_index.core.schema import TextNode

# å¯¼å…¥æ ¸å¿ƒå¤„ç†æ¨¡å—
from config.settings import load_config_from_yaml
from core.nodes import ConceptNode
from core.concept_merger import ConceptMerger

# å¯¼å…¥å®éªŒç®¡ç†å™¨
from utils.experiment_manager import ExperimentManager
from utils.helpers import FileHelper

logger = logging.getLogger(__name__)

def load_step3_result(step3_file_or_dir: str) -> tuple:
    """
    ä»æ­¥éª¤3çš„è¾“å‡ºæ–‡ä»¶æˆ–å®éªŒæ–‡ä»¶å¤¹ä¸­åŠ è½½ç»“æœ
    
    Args:
        step3_file_or_dir: æ­¥éª¤3çš„è¾“å‡ºæ–‡ä»¶æˆ–å®éªŒæ–‡ä»¶å¤¹è·¯å¾„
        
    Returns:
        tuple: (step3_result, experiment_manager)
    """
    step3_path = Path(step3_file_or_dir)
    
    if step3_path.is_file():
        # æƒ…å†µ1ï¼šç›´æ¥æŒ‡å®šäº†step3çš„è¾“å‡ºæ–‡ä»¶
        if step3_path.name.startswith("step3") and step3_path.suffix == ".txt":
            experiment_dir = step3_path.parent
            experiment_manager = ExperimentManager.load_experiment(str(experiment_dir))
            
            # ä»txtæ–‡ä»¶åŠ è½½ç»“æœ
            step3_result = load_result_from_txt(str(step3_path))
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {step3_path}")
            
    elif step3_path.is_dir():
        # æƒ…å†µ2ï¼šç›´æ¥æŒ‡å®šäº†å®éªŒæ–‡ä»¶å¤¹
        experiment_manager = ExperimentManager.load_experiment(str(step3_path))
        
        # æŸ¥æ‰¾step3çš„è¾“å‡ºæ–‡ä»¶
        step3_txt_path = experiment_manager.get_step_output_path(3, "txt")
        if not step3_txt_path.exists():
            raise FileNotFoundError(f"å®éªŒæ–‡ä»¶å¤¹ä¸­æ‰¾ä¸åˆ°step3è¾“å‡ºæ–‡ä»¶: {step3_txt_path}")
        
        step3_result = load_result_from_txt(str(step3_txt_path))
        
    else:
        raise FileNotFoundError(f"è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {step3_path}")
    
    return step3_result, experiment_manager

def load_result_from_txt(input_file: str) -> Dict[str, Any]:
    """ä»txtæ–‡ä»¶ä¸­åŠ è½½ç»“æœæ•°æ®"""
    with open(input_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    start_marker = "# JSON_DATA_START\n"
    end_marker = "\n# JSON_DATA_END"
    
    start_idx = content.find(start_marker)
    end_idx = content.find(end_marker)
    
    if start_idx == -1 or end_idx == -1:
        raise ValueError("æ— æ³•ä»txtæ–‡ä»¶ä¸­è§£ææ•°æ®")
    
    json_str = content[start_idx + len(start_marker):end_idx]
    return json.loads(json_str)

def load_previous_steps_data(experiment_manager: ExperimentManager) -> Dict[str, Any]:
    """
    åŠ è½½ä¹‹å‰æ­¥éª¤çš„æ•°æ®
    
    Args:
        experiment_manager: å®éªŒç®¡ç†å™¨
        
    Returns:
        Dict[str, Any]: åŒ…å«ä¹‹å‰æ­¥éª¤æ•°æ®çš„å­—å…¸
    """
    previous_data = {}
    
    # åŠ è½½æ­¥éª¤2çš„æ•°æ®ï¼ˆéœ€è¦åˆ†å—ä¿¡æ¯ï¼‰
    step2_path = experiment_manager.get_step_output_path(2, "txt")
    if step2_path.exists():
        try:
            step2_result = load_result_from_txt(str(step2_path))
            previous_data["step2"] = step2_result
            print(f"âœ… åŠ è½½æ­¥éª¤2æ•°æ®: {step2_path}")
        except Exception as e:
            print(f"âš ï¸ åŠ è½½æ­¥éª¤2æ•°æ®å¤±è´¥: {e}")
    
    return previous_data

def extract_concepts_from_step3(step3_result: Dict[str, Any]) -> Dict[str, Any]:
    """
    ä»æ­¥éª¤3ç»“æœä¸­æå–æ¦‚å¿µæ•°æ®
    
    Args:
        step3_result: æ­¥éª¤3çš„ç»“æœ
        
    Returns:
        Dict[str, Any]: æå–çš„æ¦‚å¿µæ•°æ®
    """
    print("ğŸ“Š ä»æ­¥éª¤3ç»“æœä¸­æå–æ¦‚å¿µæ•°æ®...")
    
    concept_analysis = step3_result.get("concept_analysis", {})
    
    if not concept_analysis:
        raise ValueError("æ­¥éª¤3ç»“æœä¸­æ²¡æœ‰æ‰¾åˆ°æ¦‚å¿µåˆ†ææ•°æ®")
    
    # æå–æ¦‚å¿µä¿¡æ¯
    all_concepts = concept_analysis.get("all_concepts", [])
    unique_concepts = concept_analysis.get("unique_concepts", [])
    concept_frequency = concept_analysis.get("concept_frequency", {})
    quality_scores = concept_analysis.get("quality_scores", {})
    high_quality_concepts = concept_analysis.get("high_quality_concepts", [])
    chunk_concept_map = concept_analysis.get("chunk_concept_map", {})
    
    print(f"   - æ€»æ¦‚å¿µæ•°: {len(all_concepts)}")
    print(f"   - å”¯ä¸€æ¦‚å¿µæ•°: {len(unique_concepts)}")
    print(f"   - é«˜è´¨é‡æ¦‚å¿µæ•°: {len(high_quality_concepts)}")
    
    return {
        "all_concepts": all_concepts,
        "unique_concepts": unique_concepts,
        "concept_frequency": concept_frequency,
        "quality_scores": quality_scores,
        "high_quality_concepts": high_quality_concepts,
        "chunk_concept_map": chunk_concept_map
    }

def merge_similar_concepts(concepts_data: Dict[str, Any], config) -> Dict[str, Any]:
    """
    åˆå¹¶ç›¸ä¼¼çš„æ¦‚å¿µ
    
    Args:
        concepts_data: æ¦‚å¿µæ•°æ®
        config: é…ç½®å¯¹è±¡
        
    Returns:
        Dict[str, Any]: åˆå¹¶åçš„æ¦‚å¿µæ•°æ®
    """
    print("ğŸ”— å¼€å§‹æ™ºèƒ½æ¦‚å¿µåˆå¹¶...")
    
    unique_concepts = concepts_data["unique_concepts"]
    concept_frequency = concepts_data["concept_frequency"]
    quality_scores = concepts_data["quality_scores"]
    
    # 1. åŸºäºæ–‡æœ¬ç›¸ä¼¼åº¦è¿›è¡Œåˆæ­¥åˆ†ç»„
    concept_groups = group_similar_concepts(unique_concepts)
    print(f"   åˆ†ç»„å®Œæˆ: {len(unique_concepts)} â†’ {len(concept_groups)} ç»„")
    
    # 2. ä¸ºæ¯ç»„é€‰æ‹©æœ€ä½³ä»£è¡¨æ¦‚å¿µ
    merged_concepts = []
    merge_mapping = {}  # åŸæ¦‚å¿µ -> åˆå¹¶åæ¦‚å¿µ
    
    for group in concept_groups:
        if len(group) == 1:
            # å•ç‹¬çš„æ¦‚å¿µç›´æ¥ä¿ç•™
            best_concept = group[0]
        else:
            # ä»ç»„ä¸­é€‰æ‹©æœ€ä½³æ¦‚å¿µä½œä¸ºä»£è¡¨
            best_concept = select_best_concept_from_group(
                group, concept_frequency, quality_scores
            )
        
        merged_concepts.append(best_concept)
        
        # è®°å½•æ˜ å°„å…³ç³»
        for concept in group:
            merge_mapping[concept] = best_concept
    
    print(f"   åˆå¹¶å®Œæˆ: {len(merged_concepts)} ä¸ªåˆå¹¶åæ¦‚å¿µ")
    
    # 3. é‡æ–°è®¡ç®—åˆå¹¶åçš„ç»Ÿè®¡ä¿¡æ¯
    merged_frequency = defaultdict(int)
    for original_concept, frequency in concept_frequency.items():
        merged_concept = merge_mapping[original_concept]
        merged_frequency[merged_concept] += frequency
    
    # 4. è®¡ç®—åˆå¹¶åçš„è´¨é‡åˆ†æ•°
    merged_quality_scores = {}
    for merged_concept in merged_concepts:
        # å–æ‰€æœ‰æ˜ å°„åˆ°æ­¤æ¦‚å¿µçš„åŸæ¦‚å¿µçš„æœ€é«˜è´¨é‡åˆ†æ•°
        mapped_scores = [
            quality_scores.get(orig_concept, 0) 
            for orig_concept, merged in merge_mapping.items() 
            if merged == merged_concept
        ]
        merged_quality_scores[merged_concept] = max(mapped_scores) if mapped_scores else 0
    
    # 5. æŒ‰è´¨é‡å’Œé¢‘ç‡é‡æ–°æ’åº
    sorted_by_quality = sorted(
        merged_concepts, 
        key=lambda x: (merged_quality_scores.get(x, 0), merged_frequency.get(x, 0)), 
        reverse=True
    )
    
    sorted_by_frequency = sorted(
        merged_frequency.items(), 
        key=lambda x: x[1], 
        reverse=True
    )
    
    return {
        "merged_concepts": merged_concepts,
        "merge_mapping": dict(merge_mapping),
        "merged_frequency": dict(merged_frequency),
        "merged_quality_scores": merged_quality_scores,
        "sorted_by_quality": sorted_by_quality,
        "sorted_by_frequency": sorted_by_frequency,
        "concept_groups": concept_groups,
        "compression_ratio": len(merged_concepts) / len(unique_concepts) if unique_concepts else 1.0
    }

def group_similar_concepts(concepts: List[str], similarity_threshold: float = 0.7) -> List[List[str]]:
    """
    åŸºäºæ–‡æœ¬ç›¸ä¼¼åº¦å¯¹æ¦‚å¿µè¿›è¡Œåˆ†ç»„
    
    Args:
        concepts: æ¦‚å¿µåˆ—è¡¨
        similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        
    Returns:
        List[List[str]]: æ¦‚å¿µåˆ†ç»„
    """
    from difflib import SequenceMatcher
    
    groups = []
    used = set()
    
    for i, concept1 in enumerate(concepts):
        if concept1 in used:
            continue
        
        group = [concept1]
        used.add(concept1)
        
        for j, concept2 in enumerate(concepts[i+1:], i+1):
            if concept2 in used:
                continue
            
            # è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦
            similarity = calculate_text_similarity(concept1, concept2)
            
            if similarity >= similarity_threshold:
                group.append(concept2)
                used.add(concept2)
        
        groups.append(group)
    
    return groups

def calculate_text_similarity(text1: str, text2: str) -> float:
    """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦"""
    from difflib import SequenceMatcher
    
    # åŸºç¡€å­—ç¬¦ä¸²ç›¸ä¼¼åº¦
    char_similarity = SequenceMatcher(None, text1.lower(), text2.lower()).ratio()
    
    # è¯æ±‡é‡å ç›¸ä¼¼åº¦
    words1 = set(text1.lower().split())
    words2 = set(text2.lower().split())
    
    if not words1 and not words2:
        word_similarity = 1.0
    elif not words1 or not words2:
        word_similarity = 0.0
    else:
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        word_similarity = len(intersection) / len(union)
    
    # åŒ…å«å…³ç³»æ£€æŸ¥
    containment_similarity = 0.0
    if text1.lower() in text2.lower() or text2.lower() in text1.lower():
        containment_similarity = 0.5
    
    # ç»¼åˆç›¸ä¼¼åº¦
    final_similarity = max(
        char_similarity * 0.4 + word_similarity * 0.6,
        containment_similarity
    )
    
    return final_similarity

def select_best_concept_from_group(group: List[str], 
                                 concept_frequency: Dict[str, int], 
                                 quality_scores: Dict[str, float]) -> str:
    """
    ä»æ¦‚å¿µç»„ä¸­é€‰æ‹©æœ€ä½³ä»£è¡¨æ¦‚å¿µ
    
    Args:
        group: æ¦‚å¿µç»„
        concept_frequency: æ¦‚å¿µé¢‘ç‡
        quality_scores: è´¨é‡åˆ†æ•°
        
    Returns:
        str: æœ€ä½³æ¦‚å¿µ
    """
    # è®¡ç®—ç»¼åˆåˆ†æ•°ï¼šè´¨é‡åˆ†æ•° + é¢‘ç‡æƒé‡
    best_concept = group[0]
    best_score = 0
    
    for concept in group:
        quality = quality_scores.get(concept, 0)
        frequency = concept_frequency.get(concept, 0)
        
        # ç»¼åˆåˆ†æ•°ï¼šè´¨é‡ä¸ºä¸»ï¼Œé¢‘ç‡ä¸ºè¾…
        combined_score = quality * 0.7 + (frequency / 10) * 0.3
        
        if combined_score > best_score:
            best_score = combined_score
            best_concept = concept
    
    return best_concept

def create_concept_nodes(merged_data: Dict[str, Any], 
                        concepts_data: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    åˆ›å»ºæ¦‚å¿µèŠ‚ç‚¹æ•°æ®ç»“æ„
    
    Args:
        merged_data: åˆå¹¶åçš„æ¦‚å¿µæ•°æ®
        concepts_data: åŸå§‹æ¦‚å¿µæ•°æ®
        
    Returns:
        List[Dict[str, Any]]: æ¦‚å¿µèŠ‚ç‚¹åˆ—è¡¨
    """
    print("ğŸ—ï¸ åˆ›å»ºæ¦‚å¿µèŠ‚ç‚¹...")
    
    merged_concepts = merged_data["merged_concepts"]
    merge_mapping = merged_data["merge_mapping"]
    merged_frequency = merged_data["merged_frequency"]
    merged_quality_scores = merged_data["merged_quality_scores"]
    chunk_concept_map = concepts_data["chunk_concept_map"]
    
    concept_nodes = []
    
    for i, concept in enumerate(merged_concepts):
        # æ‰¾åˆ°æ‰€æœ‰æ˜ å°„åˆ°æ­¤æ¦‚å¿µçš„åŸæ¦‚å¿µ
        source_concepts = [
            orig for orig, merged in merge_mapping.items() 
            if merged == concept
        ]
        
        # æ‰¾åˆ°åŒ…å«æ­¤æ¦‚å¿µçš„æ‰€æœ‰chunks
        source_chunks = []
        for chunk_id, chunk_concepts in chunk_concept_map.items():
            if any(orig_concept in chunk_concepts for orig_concept in source_concepts):
                source_chunks.append(chunk_id)
        
        # è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°
        frequency = merged_frequency.get(concept, 0)
        quality = merged_quality_scores.get(concept, 0)
        coverage = len(source_chunks)
        
        # ç½®ä¿¡åº¦ = (è´¨é‡åˆ†æ•° + é¢‘ç‡æƒé‡ + è¦†ç›–åº¦æƒé‡) / 3
        confidence_score = (
            quality / 5.0 +  # è´¨é‡åˆ†æ•°æ ‡å‡†åŒ–åˆ°0-1
            min(frequency / 10.0, 1.0) +  # é¢‘ç‡æƒé‡ï¼Œæœ€å¤§ä¸º1
            min(coverage / 5.0, 1.0)  # è¦†ç›–åº¦æƒé‡ï¼Œæœ€å¤§ä¸º1
        ) / 3.0
        
        # åˆ›å»ºæ¦‚å¿µèŠ‚ç‚¹
        concept_node = {
            "concept_id": f"merged_concept_{i}",
            "concept_text": concept,
            "concept_name": concept[:50],  # é™åˆ¶åç§°é•¿åº¦
            "concept_length": len(concept),
            "source_concepts": source_concepts,
            "source_chunks": source_chunks,
            "frequency": frequency,
            "quality_score": quality,
            "confidence_score": confidence_score,
            "coverage": coverage,
            "merge_group_size": len(source_concepts),
            "created_at": datetime.now().isoformat()
        }
        
        concept_nodes.append(concept_node)
    
    # æŒ‰ç½®ä¿¡åº¦æ’åº
    concept_nodes.sort(key=lambda x: x["confidence_score"], reverse=True)
    
    print(f"   åˆ›å»ºäº† {len(concept_nodes)} ä¸ªæ¦‚å¿µèŠ‚ç‚¹")
    
    return concept_nodes

def process_step4_concept_merging(step3_result: Dict[str, Any], 
                                 previous_data: Dict[str, Any],
                                 config_path: str = "config.yml") -> Dict[str, Any]:
    """
    æ‰§è¡Œæ­¥éª¤4çš„æ¦‚å¿µåˆå¹¶å¤„ç†
    
    Args:
        step3_result: æ­¥éª¤3çš„ç»“æœ
        previous_data: ä¹‹å‰æ­¥éª¤çš„æ•°æ®
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        Dict[str, Any]: æ­¥éª¤4çš„å¤„ç†ç»“æœ
    """
    start_time = time.time()
    
    try:
        # 1. åŠ è½½é…ç½®
        print("ğŸ“‹ åŠ è½½é…ç½®...")
        config = load_config_from_yaml(config_path)
        
        # 2. æå–æ¦‚å¿µæ•°æ®
        print("ğŸ“Š æå–æ¦‚å¿µæ•°æ®...")
        concepts_data = extract_concepts_from_step3(step3_result)
        
        # 3. æ‰§è¡Œæ¦‚å¿µåˆå¹¶
        print("ğŸ”— æ‰§è¡Œæ¦‚å¿µåˆå¹¶...")
        merged_data = merge_similar_concepts(concepts_data, config)
        
        # 4. åˆ›å»ºæ¦‚å¿µèŠ‚ç‚¹
        print("ğŸ—ï¸ åˆ›å»ºæ¦‚å¿µèŠ‚ç‚¹...")
        concept_nodes = create_concept_nodes(merged_data, concepts_data)
        
        # 5. ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        original_count = len(concepts_data["unique_concepts"])
        merged_count = len(merged_data["merged_concepts"])
        compression_ratio = merged_count / original_count if original_count > 0 else 1.0
        
        avg_confidence = sum(node["confidence_score"] for node in concept_nodes) / len(concept_nodes) if concept_nodes else 0
        avg_concept_length = sum(node["concept_length"] for node in concept_nodes) / len(concept_nodes) if concept_nodes else 0
        total_source_chunks = len(set().union(*(node["source_chunks"] for node in concept_nodes)))
        
        statistics = {
            "original_concept_count": original_count,
            "merged_concept_count": merged_count,
            "compression_ratio": compression_ratio,
            "avg_confidence": avg_confidence,
            "avg_concept_length": avg_concept_length,
            "total_source_chunks": total_source_chunks,
            "concept_groups_count": len(merged_data["concept_groups"]),
            "high_confidence_count": len([n for n in concept_nodes if n["confidence_score"] > 0.7]),
            "medium_confidence_count": len([n for n in concept_nodes if 0.4 <= n["confidence_score"] <= 0.7]),
            "low_confidence_count": len([n for n in concept_nodes if n["confidence_score"] < 0.4])
        }
        
        processing_time = time.time() - start_time
        
        # 6. æ„å»ºç»“æœ
        result = {
            "success": True,
            "step": 4,
            "step_name": "æ¦‚å¿µåˆå¹¶ä¸ä¼˜åŒ–",
            "concept_nodes": concept_nodes,
            "merged_data": merged_data,
            "input_statistics": {
                "original_concept_count": original_count,
                "compression_ratio": compression_ratio
            },
            "statistics": statistics,
            "processing_time": processing_time,
            "timestamp": datetime.now().isoformat(),
            "config_used": {
                "similarity_threshold": config.get("concept_merging.similarity_threshold", 0.7),
                "max_document_concepts": config.get("concept_merging.max_document_concepts", 10)
            }
        }
        
        return result
        
    except Exception as e:
        error_msg = f"æ­¥éª¤4å¤„ç†å¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        
        result = {
            "success": False,
            "step": 4,
            "step_name": "æ¦‚å¿µåˆå¹¶ä¸ä¼˜åŒ–",
            "error": error_msg,
            "processing_time": time.time() - start_time,
            "timestamp": datetime.now().isoformat()
        }
        
        return result

def main():
    if len(sys.argv) != 2:
        print("ç”¨æ³•: python step4.py <step3è¾“å‡ºæ–‡ä»¶æˆ–å®éªŒæ–‡ä»¶å¤¹>")
        print("ç¤ºä¾‹:")
        print("  python step4.py experiments/20241204_143052_attention_paper/step3_retrieval.txt")
        print("  python step4.py experiments/20241204_143052_attention_paper/")
        print("\næ–°åŠŸèƒ½:")
        print("âœ… è‡ªåŠ¨è¯†åˆ«å®éªŒæ–‡ä»¶å¤¹")
        print("âœ… æ™ºèƒ½æ¦‚å¿µåˆå¹¶å’Œç›¸ä¼¼åº¦è®¡ç®—")
        print("âœ… æ¦‚å¿µå±‚æ¬¡ç»“æ„ä¼˜åŒ–")
        print("âœ… ç»Ÿä¸€çš„å®éªŒç®¡ç†")
        sys.exit(1)
    
    input_path = sys.argv[1]
    
    print(f"ğŸš€ æ­¥éª¤4: æ¦‚å¿µåˆå¹¶ä¸ä¼˜åŒ– (å¢å¼ºç‰ˆ)")
    print(f"ğŸ“„ è¾“å…¥: {input_path}")
    print("="*60)
    
    try:
        # 1. åŠ è½½æ­¥éª¤3ç»“æœå’Œå®éªŒç®¡ç†å™¨
        print("ğŸ“‚ åŠ è½½æ­¥éª¤3ç»“æœ...")
        step3_result, experiment_manager = load_step3_result(input_path)
        
        if not step3_result.get("success"):
            print("âŒ æ­¥éª¤3æœªæˆåŠŸå®Œæˆï¼Œæ— æ³•ç»§ç»­")
            sys.exit(1)
        
        print(f"âœ… å·²åŠ è½½å®éªŒ: {experiment_manager.experiment_name}")
        print(f"ğŸ“ å®éªŒç›®å½•: {experiment_manager.experiment_dir}")
        print()
        
        # 2. åŠ è½½ä¹‹å‰æ­¥éª¤çš„æ•°æ®
        print("ğŸ“‚ åŠ è½½ä¹‹å‰æ­¥éª¤çš„æ•°æ®...")
        previous_data = load_previous_steps_data(experiment_manager)
        
        # 3. æ‰§è¡Œæ­¥éª¤4å¤„ç†
        print("ğŸ”„ å¼€å§‹æ­¥éª¤4å¤„ç†...")
        result = process_step4_concept_merging(step3_result, previous_data)
        
        # 4. ä¿å­˜ç»“æœåˆ°å®éªŒæ–‡ä»¶å¤¹
        print("ğŸ’¾ ä¿å­˜å®éªŒç»“æœ...")
        saved_files = experiment_manager.save_step_result(
            step_num=4,
            result=result,
            save_formats=['txt', 'json']
        )
        
        if result.get("success"):
            print(f"\nâœ… æ­¥éª¤4å®Œæˆ!")
            print(f"ğŸ“ å®éªŒç›®å½•: {experiment_manager.experiment_dir}")
            print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶:")
            for format_type, file_path in saved_files.items():
                print(f"   - {format_type.upper()}: {file_path}")
            
            # æ˜¾ç¤ºå¤„ç†ç»“æœæ‘˜è¦
            stats = result.get("statistics", {})
            input_stats = result.get("input_statistics", {})
            
            print(f"\nğŸ“Š æ¦‚å¿µåˆå¹¶ç»“æœæ‘˜è¦:")
            print(f"   - åŸå§‹æ¦‚å¿µæ•°: {input_stats.get('original_concept_count', 0)}")
            print(f"   - åˆå¹¶åæ¦‚å¿µæ•°: {stats.get('merged_concept_count', 0)}")
            print(f"   - å‹ç¼©æ¯”: {input_stats.get('compression_ratio', 0):.2f}")
            print(f"   - æ¦‚å¿µç»„æ•°: {stats.get('concept_groups_count', 0)}")
            print(f"   - å¹³å‡ç½®ä¿¡åº¦: {stats.get('avg_confidence', 0):.3f}")
            print(f"   - å¹³å‡æ¦‚å¿µé•¿åº¦: {stats.get('avg_concept_length', 0):.1f} å­—ç¬¦")
            print(f"   - æ¶‰åŠåˆ†å—æ•°: {stats.get('total_source_chunks', 0)}")
            print(f"   - å¤„ç†æ—¶é—´: {result.get('processing_time', 0):.2f} ç§’")
            
            # æ˜¾ç¤ºæ¦‚å¿µè´¨é‡åˆ†å¸ƒ
            print(f"\nğŸ“ˆ æ¦‚å¿µè´¨é‡åˆ†å¸ƒ:")
            print(f"   - é«˜ç½®ä¿¡åº¦ (>0.7): {stats.get('high_confidence_count', 0)} ä¸ª")
            print(f"   - ä¸­ç­‰ç½®ä¿¡åº¦ (0.4-0.7): {stats.get('medium_confidence_count', 0)} ä¸ª")
            print(f"   - ä½ç½®ä¿¡åº¦ (<0.4): {stats.get('low_confidence_count', 0)} ä¸ª")
            
            # æ˜¾ç¤ºé¡¶çº§åˆå¹¶æ¦‚å¿µ
            concept_nodes = result.get("concept_nodes", [])
            if concept_nodes:
                print(f"\nğŸŒŸ é¡¶çº§åˆå¹¶æ¦‚å¿µ (å‰10ä¸ª):")
                for i, concept in enumerate(concept_nodes[:10], 1):
                    print(f"   {i:2d}. {concept['concept_text']}")
                    print(f"       ç½®ä¿¡åº¦: {concept['confidence_score']:.3f}, "
                          f"é¢‘ç‡: {concept['frequency']}, "
                          f"åˆå¹¶æ•°: {concept['merge_group_size']}")
                
                if len(concept_nodes) > 10:
                    print(f"   ... è¿˜æœ‰ {len(concept_nodes) - 10} ä¸ªåˆå¹¶æ¦‚å¿µ")
            
            # æ˜¾ç¤ºå®éªŒçŠ¶æ€
            summary = experiment_manager.get_experiment_summary()
            print(f"\nğŸ§ª å®éªŒçŠ¶æ€:")
            print(f"   - å®éªŒID: {summary['experiment_id']}")
            print(f"   - å·²å®Œæˆæ­¥éª¤: {summary['steps_completed']}/{summary['total_steps']}")
            print(f"   - å½“å‰çŠ¶æ€: {summary['status']}")
            
            # æç¤ºåç»­æ­¥éª¤
            print(f"\nğŸ“‹ åç»­æ­¥éª¤:")
            print(f"   è¿è¡Œä¸‹ä¸€æ­¥: python step5.py {saved_files['txt']}")
            print(f"   æŸ¥çœ‹ç»“æœ: cat {saved_files['txt']}")
            print(f"   æŸ¥çœ‹æ¦‚å¿µ: grep -A 20 'é¡¶çº§åˆå¹¶æ¦‚å¿µ' {saved_files['txt']}")
                
        else:
            print(f"âŒ æ­¥éª¤4å¤±è´¥: {result.get('error')}")
            
            # å³ä½¿å¤±è´¥ä¹Ÿä¿å­˜é”™è¯¯ä¿¡æ¯
            experiment_manager.save_step_result(
                step_num=4,
                result=result,
                save_formats=['txt']
            )
            sys.exit(1)
            
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        # å¦‚æœæœ‰å®éªŒç®¡ç†å™¨ï¼Œä¿å­˜é”™è¯¯ä¿¡æ¯
        if 'experiment_manager' in locals():
            error_result = {
                "step": 4,
                "step_name": "æ¦‚å¿µåˆå¹¶ä¸ä¼˜åŒ–",
                "success": False,
                "error": str(e),
                "traceback": traceback.format_exc(),
                "processing_time": 0.0,
                "timestamp": datetime.now().isoformat()
            }
            
            try:
                experiment_manager.save_step_result(4, error_result, ['txt'])
                print(f"ğŸ“„ é”™è¯¯ä¿¡æ¯å·²ä¿å­˜åˆ°å®éªŒç›®å½•: {experiment_manager.experiment_dir}")
            except:
                pass
        
        sys.exit(1)

if __name__ == "__main__":
    main()