#!/usr/bin/env python3
"""
æ­¥éª¤4: æ¦‚å¿µåˆå¹¶ - åŸºäºpipeline0604.py
================

ç”¨æ³•: python step4.py step3Out.txt
"""

import sys
import json
from pathlib import Path
from datetime import datetime

# å¯¼å…¥ç°æœ‰çš„å¤„ç†å™¨ç±»
sys.path.append(str(Path(__file__).parent))
from pipeline0604 import StepByStepPipelineProcessor

def save_result_as_txt(result, output_file):
    """ä¿å­˜ç»“æœä¸ºtxtæ ¼å¼"""
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("æ­¥éª¤4: æ¦‚å¿µåˆå¹¶ - å¤„ç†ç»“æœ\n")
        f.write("="*80 + "\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"å¤„ç†çŠ¶æ€: {'âœ… æˆåŠŸ' if result.get('success') else 'âŒ å¤±è´¥'}\n")
        
        if result.get("success"):
            stats = result.get("statistics", {})
            input_stats = result.get("input_statistics", {})
            
            f.write(f"\nğŸ“Š æ¦‚å¿µåˆå¹¶ç»Ÿè®¡:\n")
            f.write(f"- åˆå¹¶åæ¦‚å¿µæ•°: {stats.get('merged_concept_count', 0)}\n")
            f.write(f"- åŸå§‹æ¦‚å¿µæ•°: {input_stats.get('original_concept_count', 0)}\n")
            f.write(f"- å‹ç¼©æ¯”: {input_stats.get('compression_ratio', 0):.2f}\n")
            f.write(f"- å¹³å‡æ¦‚å¿µé•¿åº¦: {stats.get('avg_concept_length', 0):.1f} å­—ç¬¦\n")
            f.write(f"- æ€»æ¥æºåˆ†å—æ•°: {stats.get('total_source_chunks', 0)}\n")
            f.write(f"- å¹³å‡ç½®ä¿¡åº¦: {stats.get('avg_confidence', 0):.3f}\n")
            f.write(f"- å¤„ç†æ—¶é—´: {result.get('processing_time', 0):.2f} ç§’\n")
            
            # æ˜¾ç¤ºåˆå¹¶åçš„æ¦‚å¿µ
            concept_nodes = result.get("concept_nodes", [])
            f.write(f"\nğŸ”— åˆå¹¶åçš„æ¦‚å¿µåˆ—è¡¨ ({len(concept_nodes)} ä¸ª):\n")
            f.write("-" * 80 + "\n")
            
            for i, concept in enumerate(concept_nodes, 1):
                f.write(f"\næ¦‚å¿µ {i}:\n")
                f.write(f"  ID: {concept.get('concept_id', 'æœªçŸ¥')}\n")
                f.write(f"  æ–‡æœ¬: {concept.get('concept_text', 'æœªçŸ¥')}\n")
                f.write(f"  é•¿åº¦: {concept.get('concept_length', 0)} å­—ç¬¦\n")
                f.write(f"  æ¥æºåˆ†å—: {concept.get('source_chunks', [])}\n")
                f.write(f"  ç½®ä¿¡åº¦: {concept.get('confidence_score', 0):.3f}\n")
                f.write("-" * 60 + "\n")
                
        else:
            f.write(f"\nâŒ é”™è¯¯ä¿¡æ¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}\n")
        
        # åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ æœºå™¨å¯è¯»çš„æ•°æ®
        f.write(f"\n" + "="*80 + "\n")
        f.write("# æœºå™¨å¯è¯»æ•°æ® (è¯·å‹¿æ‰‹åŠ¨ä¿®æ”¹)\n")
        f.write("# JSON_DATA_START\n")
        f.write(json.dumps(result, ensure_ascii=False, indent=2))
        f.write("\n# JSON_DATA_END\n")

def load_result_from_txt(input_file):
    """ä»txtæ–‡ä»¶ä¸­åŠ è½½ç»“æœæ•°æ®"""
    with open(input_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    start_marker = "# JSON_DATA_START\n"
    end_marker = "\n# JSON_DATA_END"
    
    start_idx = content.find(start_marker)
    end_idx = content.find(end_marker)
    
    if start_idx == -1 or end_idx == -1:
        raise ValueError("æ— æ³•ä»txtæ–‡ä»¶ä¸­è§£ææ•°æ®")
    
    json_str = content[start_idx + len(start_marker):end_idx]
    return json.loads(json_str)

def main():
    if len(sys.argv) != 2:
        print("ç”¨æ³•: python step4.py <step3è¾“å‡ºæ–‡ä»¶>")
        sys.exit(1)
    
    input_file = sys.argv[1]
    output_file = "step4Out.txt"
    
    print(f"ğŸ”„ æ­¥éª¤4: æ¦‚å¿µåˆå¹¶")
    print(f"ğŸ“„ è¾“å…¥: {input_file}")
    print(f"ğŸ“„ è¾“å‡º: {output_file}")
    
    try:
        # è¯»å–step3çš„ç»“æœ
        step3_result = load_result_from_txt(input_file)
        
        if not step3_result.get("success"):
            print("âŒ è¾“å…¥æ–‡ä»¶æ˜¾ç¤ºstep3å¤±è´¥")
            sys.exit(1)
        
        # è¯»å–ä¹‹å‰æ­¥éª¤çš„ç»“æœ
        step_files = [
            (1, input_file.replace("step3Out.txt", "step1Out.txt")),
            (2, input_file.replace("step3Out.txt", "step2Out.txt"))
        ]
        
        previous_results = {}
        for step_num, file_path in step_files:
            if Path(file_path).exists():
                previous_results[f"step{step_num}"] = load_result_from_txt(file_path)
            else:
                print(f"âš ï¸ æ‰¾ä¸åˆ°step{step_num}è¾“å‡ºæ–‡ä»¶")
                previous_results[f"step{step_num}"] = {"success": True}
        
        # ä½¿ç”¨ç°æœ‰çš„å¤„ç†å™¨
        processor = StepByStepPipelineProcessor(
            input_file="dummy",
            output_dir="temp_step4"
        )
        
        # æ‰‹åŠ¨è®¾ç½®ä¹‹å‰æ­¥éª¤çš„ç»“æœ
        for step_key, result in previous_results.items():
            processor.step_results[step_key] = result
        processor.step_results["step3"] = step3_result
        
        # è°ƒç”¨ç°æœ‰çš„æ–¹æ³•
        result = processor.step4_concept_merging()
        
        # ä¿å­˜ç»“æœä¸ºtxt
        save_result_as_txt(result, output_file)
        
        if result.get("success"):
            print(f"âœ… æ­¥éª¤4å®Œæˆ: {output_file}")
        else:
            print(f"âŒ æ­¥éª¤4å¤±è´¥: {result.get('error')}")
            sys.exit(1)
            
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        sys.exit(1)

if __name__ == "__main__":
    main()
