"""
åˆ†æ­¥éª¤Pipelineå¤„ç†è„šæœ¬
========================================

å°†æ•´ä¸ªå¤„ç†æµç¨‹æ‹†åˆ†ä¸ºç‹¬ç«‹çš„æ­¥éª¤ï¼Œæ¯æ­¥éƒ½ç”Ÿæˆè¯¦ç»†çš„txtæŠ¥å‘Š
"""

import os
import json
import time
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
from pathlib import Path
import traceback
import uuid

# å¯¼å…¥pipelineç›¸å…³æ¨¡å—
from pipeline_new import ImprovedConceptBasedPipeline
from llama_index.core import Document
from data_generate_0526 import SimpleDataGenerator, FileProcessor

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('step_by_step_pipeline.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class StepByStepPipelineProcessor:
    """åˆ†æ­¥éª¤Pipelineå¤„ç†å™¨ - æ¯æ­¥å•ç‹¬ä¿å­˜txtç»“æœ"""
    
    def __init__(self, 
                 input_file: str,
                 output_dir: str = "step_by_step_results",
                 enable_qa_generation: bool = True,
                 qa_model_name: str = "gpt-4o-mini",
                 questions_per_type: Dict[str, int] = None):
        """
        åˆå§‹åŒ–åˆ†æ­¥éª¤å¤„ç†å™¨
        
        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            enable_qa_generation: æ˜¯å¦å¯ç”¨é—®ç­”ç”Ÿæˆ
            qa_model_name: QAç”Ÿæˆæ¨¡å‹
            questions_per_type: æ¯ç§ç±»å‹é—®é¢˜æ•°é‡
        """
        self.input_file = Path(input_file)
        self.output_dir = Path(output_dir)
        self.enable_qa_generation = enable_qa_generation
        
        # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
        self.setup_output_directories()
        
        # åˆå§‹åŒ–pipelineå’ŒQAç”Ÿæˆå™¨
        self.pipeline = ImprovedConceptBasedPipeline()
        
        if self.enable_qa_generation:
            self.questions_per_type = questions_per_type or {
                "remember": 2, "understand": 2, "apply": 1,
                "analyze": 1, "evaluate": 1, "create": 1
            }
            self.qa_generator = SimpleDataGenerator(
                model_name=qa_model_name,
                questions_per_type=self.questions_per_type
            )
        
        # å­˜å‚¨å„æ­¥éª¤ç»“æœ
        self.step_results = {}
        self.step_timings = {}
        
        logger.info(f"ğŸ“„ åˆå§‹åŒ–åˆ†æ­¥éª¤å¤„ç†å™¨")
        logger.info(f"   è¾“å…¥æ–‡ä»¶: {self.input_file}")
        logger.info(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def setup_output_directories(self):
        """è®¾ç½®è¾“å‡ºç›®å½•ç»“æ„"""
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå„æ­¥éª¤çš„å­ç›®å½•
        self.step_dirs = {
            "step1": self.output_dir / "01_æ–‡ä»¶åŠ è½½",
            "step2": self.output_dir / "02_æ–‡æ¡£åˆ†å—", 
            "step3": self.output_dir / "03_æ¦‚å¿µæå–",
            "step4": self.output_dir / "04_æ¦‚å¿µåˆå¹¶",
            "step5": self.output_dir / "05_æ¦‚å¿µæ£€ç´¢",
            "step6": self.output_dir / "06_è¯æ®æå–",
            "step7": self.output_dir / "07_é—®ç­”ç”Ÿæˆ",
            "step8": self.output_dir / "08_æœ€ç»ˆæ±‡æ€»"
        }
        
        for step_dir in self.step_dirs.values():
            step_dir.mkdir(exist_ok=True)
    
    def save_step_txt_report(self, step_name: str, step_num: str, content: str):
        """ä¿å­˜æ­¥éª¤çš„txtæŠ¥å‘Š"""
        step_dir = self.step_dirs[f"step{step_num}"]
        report_file = step_dir / f"{step_name}_è¯¦ç»†æŠ¥å‘Š.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"=" * 80 + "\n")
            f.write(f"{step_name} - è¯¦ç»†æŠ¥å‘Š\n")
            f.write(f"=" * 80 + "\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"å¤„ç†æ–‡ä»¶: {self.input_file.name}\n")
            f.write(f"=" * 80 + "\n\n")
            f.write(content)
        
        logger.info(f"ğŸ’¾ {step_name}æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    def step1_load_file(self) -> Dict[str, Any]:
        """æ­¥éª¤1: æ–‡ä»¶åŠ è½½å’Œé¢„å¤„ç†"""
        print("\n" + "="*60)
        print("ğŸ”„ æ­¥éª¤1: æ–‡ä»¶åŠ è½½å’Œé¢„å¤„ç†")
        print("="*60)
        
        start_time = time.time()
        
        try:
            # åŠ è½½æ–‡ä»¶
            logger.info(f"ğŸ“„ æ­£åœ¨åŠ è½½æ–‡ä»¶: {self.input_file}")
            text_content = FileProcessor.extract_text(str(self.input_file))
            
            # åˆ›å»ºDocumentå¯¹è±¡
            document = Document(
                text=text_content,
                metadata={
                    "file_name": self.input_file.name,
                    "file_path": str(self.input_file),
                    "file_type": self.input_file.suffix.lower(),
                    "file_size": self.input_file.stat().st_size,
                    "text_length": len(text_content),
                    "processing_timestamp": datetime.now().isoformat()
                }
            )
            
            processing_time = time.time() - start_time
            
            # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
            report_content = f"""
æ–‡ä»¶åŸºæœ¬ä¿¡æ¯:
- æ–‡ä»¶å: {self.input_file.name}
- æ–‡ä»¶ç±»å‹: {self.input_file.suffix.lower()}
- æ–‡ä»¶å¤§å°: {self.input_file.stat().st_size / 1024:.2f} KB
- æ–‡æœ¬é•¿åº¦: {len(text_content):,} å­—ç¬¦
- å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’

æ–‡æœ¬å†…å®¹ç»Ÿè®¡:
- æ€»å­—ç¬¦æ•°: {len(text_content):,}
- æ€»è¡Œæ•°: {text_content.count(chr(10)) + 1:,}
- æ®µè½æ•°ä¼°è®¡: {len([p for p in text_content.split('\n\n') if p.strip()]):,}

æ–‡æœ¬å†…å®¹é¢„è§ˆ (å‰500å­—ç¬¦):
{'-'*50}
{text_content[:500]}
{'-'*50}

æ–‡æœ¬å†…å®¹é¢„è§ˆ (å500å­—ç¬¦):
{'-'*50}
{text_content[-500:]}
{'-'*50}

å…ƒæ•°æ®ä¿¡æ¯:
{json.dumps(document.metadata, ensure_ascii=False, indent=2)}
"""
            
            # ä¿å­˜txtæŠ¥å‘Š
            self.save_step_txt_report("æ­¥éª¤1_æ–‡ä»¶åŠ è½½", "1", report_content)
            
            # ä¿å­˜åŸå§‹æ–‡æœ¬
            raw_text_file = self.step_dirs["step1"] / "åŸå§‹æ–‡æœ¬å†…å®¹.txt"
            with open(raw_text_file, 'w', encoding='utf-8') as f:
                f.write(text_content)
            
            result = {
                "success": True,
                "document": document,
                "text_length": len(text_content),
                "processing_time": processing_time,
                "metadata": document.metadata
            }
            
            self.step_results["step1"] = result
            self.step_timings["step1"] = processing_time
            
            print(f"âœ… æ­¥éª¤1å®Œæˆ - æ–‡ä»¶åŠ è½½æˆåŠŸ")
            print(f"   æ–‡ä»¶å¤§å°: {self.input_file.stat().st_size / 1024:.1f} KB")
            print(f"   æ–‡æœ¬é•¿åº¦: {len(text_content):,} å­—ç¬¦")
            print(f"   å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
            
            return result
            
        except Exception as e:
            error_msg = f"æ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            
            error_report = f"""
é”™è¯¯ä¿¡æ¯:
{error_msg}

é”™è¯¯è¯¦æƒ…:
{traceback.format_exc()}
"""
            self.save_step_txt_report("æ­¥éª¤1_æ–‡ä»¶åŠ è½½_é”™è¯¯", "1", error_report)
            
            result = {
                "success": False,
                "error": error_msg,
                "processing_time": time.time() - start_time
            }
            
            self.step_results["step1"] = result
            return result
    
    def step2_document_chunking(self) -> Dict[str, Any]:
        """æ­¥éª¤2: æ–‡æ¡£åˆ†å—"""
        print("\n" + "="*60)
        print("ğŸ”„ æ­¥éª¤2: æ–‡æ¡£åˆ†å—")
        print("="*60)
        
        if not self.step_results.get("step1", {}).get("success"):
            print("âŒ æ­¥éª¤1æœªæˆåŠŸå®Œæˆï¼Œè·³è¿‡æ­¥éª¤2")
            return {"success": False, "error": "ä¾èµ–çš„æ­¥éª¤1æœªå®Œæˆ"}
        
        start_time = time.time()
        
        try:
            document = self.step_results["step1"]["document"]
            
            # æ‰§è¡Œæ–‡æ¡£åˆ†å—
            logger.info("âœ‚ï¸ å¼€å§‹æ–‡æ¡£åˆ†å—...")
            chunk_nodes = self.pipeline.chunker.chunk_and_extract_concepts([document])
            
            processing_time = time.time() - start_time
            
            # åˆ†æchunkç»Ÿè®¡ä¿¡æ¯
            chunk_lengths = [len(chunk.text) for chunk in chunk_nodes]
            total_chunks = len(chunk_nodes)
            avg_chunk_length = sum(chunk_lengths) / total_chunks if total_chunks > 0 else 0
            
            # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
            report_content = f"""
åˆ†å—ç»Ÿè®¡ä¿¡æ¯:
- æ€»åˆ†å—æ•°: {total_chunks}
- å¹³å‡åˆ†å—é•¿åº¦: {avg_chunk_length:.1f} å­—ç¬¦
- æœ€çŸ­åˆ†å—é•¿åº¦: {min(chunk_lengths) if chunk_lengths else 0} å­—ç¬¦
- æœ€é•¿åˆ†å—é•¿åº¦: {max(chunk_lengths) if chunk_lengths else 0} å­—ç¬¦
- å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’

åˆ†å—è¯¦ç»†ä¿¡æ¯:
{'-'*60}
"""
            
            # ä¸ºæ¯ä¸ªchunkç”Ÿæˆè¯¦ç»†ä¿¡æ¯
            for i, chunk in enumerate(chunk_nodes, 1):
                concepts = chunk.metadata.get("concepts", [])
                report_content += f"""
åˆ†å— {i}:
- ID: {chunk.node_id}
- é•¿åº¦: {len(chunk.text)} å­—ç¬¦
- æ¦‚å¿µæ•°é‡: {len(concepts)}
- æ¦‚å¿µåˆ—è¡¨: {concepts}
- å†…å®¹é¢„è§ˆ: {chunk.text[:100]}...
{'-'*40}
"""
            
            # ä¿å­˜txtæŠ¥å‘Š
            self.save_step_txt_report("æ­¥éª¤2_æ–‡æ¡£åˆ†å—", "2", report_content)
            
            # ä¿å­˜æ¯ä¸ªchunkçš„å®Œæ•´å†…å®¹
            chunks_detail_file = self.step_dirs["step2"] / "åˆ†å—è¯¦ç»†å†…å®¹.txt"
            with open(chunks_detail_file, 'w', encoding='utf-8') as f:
                f.write("æ–‡æ¡£åˆ†å—è¯¦ç»†å†…å®¹\n")
                f.write("=" * 80 + "\n\n")
                
                for i, chunk in enumerate(chunk_nodes, 1):
                    f.write(f"åˆ†å— {i} (ID: {chunk.node_id})\n")
                    f.write("-" * 50 + "\n")
                    f.write(f"é•¿åº¦: {len(chunk.text)} å­—ç¬¦\n")
                    f.write(f"æ¦‚å¿µ: {chunk.metadata.get('concepts', [])}\n")
                    f.write("å†…å®¹:\n")
                    f.write(chunk.text)
                    f.write("\n\n" + "=" * 80 + "\n\n")
            
            result = {
                "success": True,
                "chunk_nodes": chunk_nodes,
                "chunk_count": total_chunks,
                "avg_chunk_length": avg_chunk_length,
                "processing_time": processing_time
            }
            
            self.step_results["step2"] = result
            self.step_timings["step2"] = processing_time
            
            print(f"âœ… æ­¥éª¤2å®Œæˆ - æ–‡æ¡£åˆ†å—æˆåŠŸ")
            print(f"   åˆ†å—æ•°é‡: {total_chunks}")
            print(f"   å¹³å‡é•¿åº¦: {avg_chunk_length:.1f} å­—ç¬¦")
            print(f"   å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
            
            return result
            
        except Exception as e:
            error_msg = f"æ–‡æ¡£åˆ†å—å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            
            error_report = f"""
é”™è¯¯ä¿¡æ¯:
{error_msg}

é”™è¯¯è¯¦æƒ…:
{traceback.format_exc()}
"""
            self.save_step_txt_report("æ­¥éª¤2_æ–‡æ¡£åˆ†å—_é”™è¯¯", "2", error_report)
            
            result = {
                "success": False,
                "error": error_msg,
                "processing_time": time.time() - start_time
            }
            
            self.step_results["step2"] = result
            return result
    
    def step3_concept_extraction(self) -> Dict[str, Any]:
        """æ­¥éª¤3: æ¦‚å¿µæå–"""
        print("\n" + "="*60)
        print("ğŸ”„ æ­¥éª¤3: æ¦‚å¿µæå–")
        print("="*60)
        
        if not self.step_results.get("step2", {}).get("success"):
            print("âŒ æ­¥éª¤2æœªæˆåŠŸå®Œæˆï¼Œè·³è¿‡æ­¥éª¤3")
            return {"success": False, "error": "ä¾èµ–çš„æ­¥éª¤2æœªå®Œæˆ"}
        
        start_time = time.time()
        
        try:
            chunk_nodes = self.step_results["step2"]["chunk_nodes"]
            
            # ä»chunkä¸­æå–æ¦‚å¿µ
            logger.info("ğŸ§  å¼€å§‹æ¦‚å¿µæå–...")
            all_concepts = []
            
            for chunk in chunk_nodes:
                concepts = chunk.metadata.get("concepts", [])
                all_concepts.extend(concepts)
            
            # ç»Ÿè®¡æ¦‚å¿µä¿¡æ¯
            unique_concepts = list(set(all_concepts))
            concept_frequency = {concept: all_concepts.count(concept) for concept in unique_concepts}
            
            processing_time = time.time() - start_time
            
            # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
            report_content = f"""
æ¦‚å¿µæå–ç»Ÿè®¡:
- æ€»æ¦‚å¿µæ•° (å«é‡å¤): {len(all_concepts)}
- å”¯ä¸€æ¦‚å¿µæ•°: {len(unique_concepts)}
- å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’

æ¦‚å¿µé¢‘ç‡åˆ†æ (æŒ‰é¢‘ç‡é™åº):
{'-'*60}
"""
            
            # æŒ‰é¢‘ç‡æ’åºæ¦‚å¿µ
            sorted_concepts = sorted(concept_frequency.items(), key=lambda x: x[1], reverse=True)
            
            for concept, freq in sorted_concepts:
                report_content += f"- {concept}: {freq} æ¬¡\n"
            
            report_content += f"\n{'-'*60}\n\næ¦‚å¿µè¯¦ç»†åˆ†å¸ƒ:\n"
            
            # ä¸ºæ¯ä¸ªchunkæ˜¾ç¤ºå…¶æ¦‚å¿µ
            for i, chunk in enumerate(chunk_nodes, 1):
                concepts = chunk.metadata.get("concepts", [])
                report_content += f"""
åˆ†å— {i} çš„æ¦‚å¿µ:
- æ¦‚å¿µæ•°é‡: {len(concepts)}
- æ¦‚å¿µåˆ—è¡¨: {concepts}
- æ–‡æœ¬é¢„è§ˆ: {chunk.text[:80]}...
{'-'*40}
"""
            
            # ä¿å­˜txtæŠ¥å‘Š
            self.save_step_txt_report("æ­¥éª¤3_æ¦‚å¿µæå–", "3", report_content)
            
            # ä¿å­˜æ¦‚å¿µåˆ—è¡¨
            concepts_file = self.step_dirs["step3"] / "æ¦‚å¿µåˆ—è¡¨.txt"
            with open(concepts_file, 'w', encoding='utf-8') as f:
                f.write("æå–çš„æ¦‚å¿µåˆ—è¡¨\n")
                f.write("=" * 50 + "\n\n")
                f.write("æŒ‰é¢‘ç‡æ’åºçš„æ¦‚å¿µ:\n")
                f.write("-" * 30 + "\n")
                
                for concept, freq in sorted_concepts:
                    f.write(f"{freq:3d} | {concept}\n")
            
            result = {
                "success": True,
                "all_concepts": all_concepts,
                "unique_concepts": unique_concepts,
                "concept_frequency": concept_frequency,
                "processing_time": processing_time
            }
            
            self.step_results["step3"] = result
            self.step_timings["step3"] = processing_time
            
            print(f"âœ… æ­¥éª¤3å®Œæˆ - æ¦‚å¿µæå–æˆåŠŸ")
            print(f"   æ€»æ¦‚å¿µæ•°: {len(all_concepts)}")
            print(f"   å”¯ä¸€æ¦‚å¿µæ•°: {len(unique_concepts)}")
            print(f"   å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
            
            return result
            
        except Exception as e:
            error_msg = f"æ¦‚å¿µæå–å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            
            error_report = f"""
é”™è¯¯ä¿¡æ¯:
{error_msg}

é”™è¯¯è¯¦æƒ…:
{traceback.format_exc()}
"""
            self.save_step_txt_report("æ­¥éª¤3_æ¦‚å¿µæå–_é”™è¯¯", "3", error_report)
            
            result = {
                "success": False,
                "error": error_msg,
                "processing_time": time.time() - start_time
            }
            
            self.step_results["step3"] = result
            return result
    
    def step4_concept_merging(self) -> Dict[str, Any]:
        """æ­¥éª¤4: æ¦‚å¿µåˆå¹¶"""
        print("\n" + "="*60)
        print("ğŸ”„ æ­¥éª¤4: æ¦‚å¿µåˆå¹¶")
        print("="*60)
        
        if not self.step_results.get("step2", {}).get("success"):
            print("âŒ æ­¥éª¤2æœªæˆåŠŸå®Œæˆï¼Œè·³è¿‡æ­¥éª¤4")
            return {"success": False, "error": "ä¾èµ–çš„æ­¥éª¤2æœªå®Œæˆ"}
        
        start_time = time.time()
        
        try:
            chunk_nodes = self.step_results["step2"]["chunk_nodes"]
            
            # æ‰§è¡Œæ¦‚å¿µåˆå¹¶
            logger.info("ğŸ”— å¼€å§‹æ¦‚å¿µåˆå¹¶...")
            concept_nodes = self.pipeline.concept_merger.merge_document_concepts(chunk_nodes)
            
            processing_time = time.time() - start_time
            
            # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
            report_content = f"""
æ¦‚å¿µåˆå¹¶ç»Ÿè®¡:
- åˆå¹¶åæ¦‚å¿µæ•°: {len(concept_nodes)}
- å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’

åˆå¹¶åçš„æ¦‚å¿µåˆ—è¡¨:
{'-'*60}
"""
            
            # ä¸ºæ¯ä¸ªåˆå¹¶åçš„æ¦‚å¿µç”Ÿæˆè¯¦ç»†ä¿¡æ¯
            for i, concept_node in enumerate(concept_nodes, 1):
                source_chunks = concept_node.metadata.get("source_chunks", [])
                report_content += f"""
æ¦‚å¿µ {i}:
- ID: {concept_node.node_id}
- æ¦‚å¿µæ–‡æœ¬: {concept_node.text}
- æ¥æºåˆ†å—æ•°: {len(source_chunks)}
- æ¥æºåˆ†å—ID: {source_chunks}
{'-'*40}
"""
            
            # ä¿å­˜txtæŠ¥å‘Š
            self.save_step_txt_report("æ­¥éª¤4_æ¦‚å¿µåˆå¹¶", "4", report_content)
            
            # ä¿å­˜åˆå¹¶åæ¦‚å¿µçš„è¯¦ç»†ä¿¡æ¯
            merged_concepts_file = self.step_dirs["step4"] / "åˆå¹¶åæ¦‚å¿µè¯¦ç»†ä¿¡æ¯.txt"
            with open(merged_concepts_file, 'w', encoding='utf-8') as f:
                f.write("åˆå¹¶åçš„æ¦‚å¿µè¯¦ç»†ä¿¡æ¯\n")
                f.write("=" * 80 + "\n\n")
                
                for i, concept_node in enumerate(concept_nodes, 1):
                    f.write(f"æ¦‚å¿µ {i}\n")
                    f.write("-" * 50 + "\n")
                    f.write(f"ID: {concept_node.node_id}\n")
                    f.write(f"æ¦‚å¿µæ–‡æœ¬: {concept_node.text}\n")
                    f.write(f"å…ƒæ•°æ®: {json.dumps(concept_node.metadata, ensure_ascii=False, indent=2)}\n")
                    f.write("\n" + "=" * 80 + "\n\n")
            
            result = {
                "success": True,
                "concept_nodes": concept_nodes,
                "concept_count": len(concept_nodes),
                "processing_time": processing_time
            }
            
            self.step_results["step4"] = result
            self.step_timings["step4"] = processing_time
            
            print(f"âœ… æ­¥éª¤4å®Œæˆ - æ¦‚å¿µåˆå¹¶æˆåŠŸ")
            print(f"   åˆå¹¶åæ¦‚å¿µæ•°: {len(concept_nodes)}")
            print(f"   å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
            
            return result
            
        except Exception as e:
            error_msg = f"æ¦‚å¿µåˆå¹¶å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            
            error_report = f"""
é”™è¯¯ä¿¡æ¯:
{error_msg}

é”™è¯¯è¯¦æƒ…:
{traceback.format_exc()}
"""
            self.save_step_txt_report("æ­¥éª¤4_æ¦‚å¿µåˆå¹¶_é”™è¯¯", "4", error_report)
            
            result = {
                "success": False,
                "error": error_msg,
                "processing_time": time.time() - start_time
            }
            
            self.step_results["step4"] = result
            return result
    
    def step5_concept_retrieval(self) -> Dict[str, Any]:
        """æ­¥éª¤5: æ¦‚å¿µæ£€ç´¢"""
        print("\n" + "="*60)
        print("ğŸ”„ æ­¥éª¤5: æ¦‚å¿µæ£€ç´¢")
        print("="*60)
        
        if not (self.step_results.get("step2", {}).get("success") and 
                self.step_results.get("step4", {}).get("success")):
            print("âŒ ä¾èµ–çš„æ­¥éª¤æœªæˆåŠŸå®Œæˆï¼Œè·³è¿‡æ­¥éª¤5")
            return {"success": False, "error": "ä¾èµ–çš„æ­¥éª¤æœªå®Œæˆ"}
        
        start_time = time.time()
        
        try:
            chunk_nodes = self.step_results["step2"]["chunk_nodes"]
            concept_nodes = self.step_results["step4"]["concept_nodes"]
            
            # åˆ›å»ºchunkç´¢å¼•
            logger.info("ğŸ” å¼€å§‹æ¦‚å¿µæ£€ç´¢...")
            from llama_index.core import VectorStoreIndex
            chunk_index = VectorStoreIndex(chunk_nodes)
            
            # æ‰§è¡Œæ¦‚å¿µæ£€ç´¢
            concept_to_chunks = self.pipeline.retriever.retrieve_chunks_for_concepts(
                concept_nodes, chunk_index
            )
            
            processing_time = time.time() - start_time
            
            # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
            report_content = f"""
æ¦‚å¿µæ£€ç´¢ç»Ÿè®¡:
- å‚ä¸æ£€ç´¢çš„æ¦‚å¿µæ•°: {len(concept_nodes)}
- æ£€ç´¢ç»“æœæ˜ å°„æ•°: {len(concept_to_chunks)}
- å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’

æ£€ç´¢ç»“æœè¯¦æƒ…:
{'-'*60}
"""
            
            # ä¸ºæ¯ä¸ªæ¦‚å¿µçš„æ£€ç´¢ç»“æœç”Ÿæˆè¯¦ç»†ä¿¡æ¯
            for concept_node in concept_nodes:
                concept_id = concept_node.node_id
                chunks = concept_to_chunks.get(concept_id, [])
                
                report_content += f"""
æ¦‚å¿µ: {concept_node.text}
- æ¦‚å¿µID: {concept_id}
- æ£€ç´¢åˆ°çš„ç›¸å…³åˆ†å—æ•°: {len(chunks)}
"""
                
                for j, chunk_with_score in enumerate(chunks, 1):
                    chunk = chunk_with_score.node
                    score = chunk_with_score.score
                    report_content += f"  ç›¸å…³åˆ†å— {j}: ID={chunk.node_id}, ç›¸ä¼¼åº¦={score:.3f}, å†…å®¹={chunk.text[:50]}...\n"
                
                report_content += "-" * 40 + "\n"
            
            # ä¿å­˜txtæŠ¥å‘Š
            self.save_step_txt_report("æ­¥éª¤5_æ¦‚å¿µæ£€ç´¢", "5", report_content)
            
            # ä¿å­˜æ£€ç´¢ç»“æœçš„è¯¦ç»†æ˜ å°„
            retrieval_details_file = self.step_dirs["step5"] / "æ£€ç´¢ç»“æœè¯¦ç»†æ˜ å°„.txt"
            with open(retrieval_details_file, 'w', encoding='utf-8') as f:
                f.write("æ¦‚å¿µæ£€ç´¢è¯¦ç»†ç»“æœ\n")
                f.write("=" * 80 + "\n\n")
                
                for concept_node in concept_nodes:
                    concept_id = concept_node.node_id
                    chunks = concept_to_chunks.get(concept_id, [])
                    
                    f.write(f"æ¦‚å¿µ: {concept_node.text}\n")
                    f.write(f"æ¦‚å¿µID: {concept_id}\n")
                    f.write(f"æ£€ç´¢åˆ° {len(chunks)} ä¸ªç›¸å…³åˆ†å—:\n")
                    f.write("-" * 60 + "\n")
                    
                    for j, chunk_with_score in enumerate(chunks, 1):
                        chunk = chunk_with_score.node
                        score = chunk_with_score.score
                        f.write(f"\nç›¸å…³åˆ†å— {j}:\n")
                        f.write(f"  - åˆ†å—ID: {chunk.node_id}\n")
                        f.write(f"  - ç›¸ä¼¼åº¦åˆ†æ•°: {score:.4f}\n")
                        f.write(f"  - åˆ†å—å†…å®¹:\n{chunk.text}\n")
                        f.write("-" * 30 + "\n")
                    
                    f.write("\n" + "=" * 80 + "\n\n")
            
            result = {
                "success": True,
                "concept_to_chunks": concept_to_chunks,
                "retrieval_count": len(concept_to_chunks),
                "processing_time": processing_time
            }
            
            self.step_results["step5"] = result
            self.step_timings["step5"] = processing_time
            
            print(f"âœ… æ­¥éª¤5å®Œæˆ - æ¦‚å¿µæ£€ç´¢æˆåŠŸ")
            print(f"   æ£€ç´¢æ˜ å°„æ•°: {len(concept_to_chunks)}")
            print(f"   å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
            
            return result
            
        except Exception as e:
            error_msg = f"æ¦‚å¿µæ£€ç´¢å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            
            error_report = f"""
é”™è¯¯ä¿¡æ¯:
{error_msg}

é”™è¯¯è¯¦æƒ…:
{traceback.format_exc()}
"""
            self.save_step_txt_report("æ­¥éª¤5_æ¦‚å¿µæ£€ç´¢_é”™è¯¯", "5", error_report)
            
            result = {
                "success": False,
                "error": error_msg,
                "processing_time": time.time() - start_time
            }
            
            self.step_results["step5"] = result
            return result
    
    def step6_evidence_extraction(self) -> Dict[str, Any]:
        """æ­¥éª¤6: è¯æ®æå–"""
        print("\n" + "="*60)
        print("ğŸ”„ æ­¥éª¤6: è¯æ®æå–")
        print("="*60)
        
        if not (self.step_results.get("step4", {}).get("success") and 
                self.step_results.get("step5", {}).get("success")):
            print("âŒ ä¾èµ–çš„æ­¥éª¤æœªæˆåŠŸå®Œæˆï¼Œè·³è¿‡æ­¥éª¤6")
            return {"success": False, "error": "ä¾èµ–çš„æ­¥éª¤æœªå®Œæˆ"}
        
        start_time = time.time()
        
        try:
            concept_nodes = self.step_results["step4"]["concept_nodes"]
            concept_to_chunks = self.step_results["step5"]["concept_to_chunks"]
            
            # æ‰§è¡Œè¯æ®æå–
            logger.info("ğŸ” å¼€å§‹è¯æ®æå–...")
            evidence_nodes = self.pipeline.evidence_extractor.extract_evidence_for_concepts(
                concept_nodes, concept_to_chunks
            )
            
            processing_time = time.time() - start_time
            
            # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
            report_content = f"""
è¯æ®æå–ç»Ÿè®¡:
- è¾“å…¥æ¦‚å¿µæ•°: {len(concept_nodes)}
- æå–çš„è¯æ®æ•°: {len(evidence_nodes)}
- å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’

è¯æ®æå–è¯¦æƒ…:
{'-'*60}
"""
            
            # ä¸ºæ¯ä¸ªè¯æ®ç”Ÿæˆè¯¦ç»†ä¿¡æ¯
            for i, evidence_node in enumerate(evidence_nodes, 1):
                concept_text = evidence_node.metadata.get("concept_text", "æœªçŸ¥")
                relevance_score = evidence_node.metadata.get("relevance_score", 0.0)
                
                report_content += f"""
è¯æ® {i}:
- è¯æ®ID: {evidence_node.node_id}
- å…³è”æ¦‚å¿µ: {concept_text}
- ç›¸å…³æ€§åˆ†æ•°: {relevance_score:.3f}
- è¯æ®é•¿åº¦: {len(evidence_node.text)} å­—ç¬¦
- è¯æ®å†…å®¹: {evidence_node.text[:100]}...
{'-'*40}
"""
            
            # ä¿å­˜txtæŠ¥å‘Š
            self.save_step_txt_report("æ­¥éª¤6_è¯æ®æå–", "6", report_content)
            
            # ä¿å­˜è¯æ®çš„å®Œæ•´å†…å®¹
            evidence_details_file = self.step_dirs["step6"] / "è¯æ®è¯¦ç»†å†…å®¹.txt"
            with open(evidence_details_file, 'w', encoding='utf-8') as f:
                f.write("è¯æ®æå–è¯¦ç»†ç»“æœ\n")
                f.write("=" * 80 + "\n\n")
                
                for i, evidence_node in enumerate(evidence_nodes, 1):
                    concept_text = evidence_node.metadata.get("concept_text", "æœªçŸ¥")
                    relevance_score = evidence_node.metadata.get("relevance_score", 0.0)
                    source_chunks = evidence_node.metadata.get("source_chunks", [])
                    
                    f.write(f"è¯æ® {i}\n")
                    f.write("-" * 50 + "\n")
                    f.write(f"è¯æ®ID: {evidence_node.node_id}\n")
                    f.write(f"å…³è”æ¦‚å¿µ: {concept_text}\n")
                    f.write(f"ç›¸å…³æ€§åˆ†æ•°: {relevance_score:.4f}\n")
                    f.write(f"æ¥æºåˆ†å—: {source_chunks}\n")
                    f.write(f"è¯æ®é•¿åº¦: {len(evidence_node.text)} å­—ç¬¦\n")
                    f.write("è¯æ®å®Œæ•´å†…å®¹:\n")
                    f.write(evidence_node.text)
                    f.write("\n\n" + "=" * 80 + "\n\n")
            
            result = {
                "success": True,
                "evidence_nodes": evidence_nodes,
                "evidence_count": len(evidence_nodes),
                "processing_time": processing_time
            }
            
            self.step_results["step6"] = result
            self.step_timings["step6"] = processing_time
            
            print(f"âœ… æ­¥éª¤6å®Œæˆ - è¯æ®æå–æˆåŠŸ")
            print(f"   æå–è¯æ®æ•°: {len(evidence_nodes)}")
            print(f"   å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
            
            return result
            
        except Exception as e:
            error_msg = f"è¯æ®æå–å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            
            error_report = f"""
é”™è¯¯ä¿¡æ¯:
{error_msg}

é”™è¯¯è¯¦æƒ…:
{traceback.format_exc()}
"""
            self.save_step_txt_report("æ­¥éª¤6_è¯æ®æå–_é”™è¯¯", "6", error_report)
            
            result = {
                "success": False,
                "error": error_msg,
                "processing_time": time.time() - start_time
            }
            
            self.step_results["step6"] = result
            return result
    
    def step7_qa_generation(self) -> Dict[str, Any]:
        """æ­¥éª¤7: é—®ç­”ç”Ÿæˆ"""
        print("\n" + "="*60)
        print("ğŸ”„ æ­¥éª¤7: é—®ç­”ç”Ÿæˆ")
        print("="*60)
        
        if not self.enable_qa_generation:
            print("â„¹ï¸  é—®ç­”ç”Ÿæˆå·²ç¦ç”¨ï¼Œè·³è¿‡æ­¥éª¤7")
            return {"success": True, "skipped": True, "reason": "QA generation disabled"}
        
        if not self.step_results.get("step6", {}).get("success"):
            print("âŒ æ­¥éª¤6æœªæˆåŠŸå®Œæˆï¼Œè·³è¿‡æ­¥éª¤7")
            return {"success": False, "error": "ä¾èµ–çš„æ­¥éª¤6æœªå®Œæˆ"}
        
        start_time = time.time()
        
        try:
            evidence_nodes = self.step_results["step6"]["evidence_nodes"]
            
            if not evidence_nodes:
                print("âš ï¸  æ²¡æœ‰è¯æ®å¯ç”¨äºé—®ç­”ç”Ÿæˆï¼Œè·³è¿‡æ­¥éª¤7")
                return {"success": True, "skipped": True, "reason": "No evidence available"}
            
            # æ‰§è¡Œé—®ç­”ç”Ÿæˆ
            logger.info("â“ å¼€å§‹é—®ç­”ç”Ÿæˆ...")
            
            all_qa_pairs = []
            
            for i, evidence_node in enumerate(evidence_nodes):
                try:
                    # åŸºäºæ¯ä¸ªè¯æ®ç”Ÿæˆé—®ç­”å¯¹
                    qa_pairs = self.qa_generator.generate_qa_pairs_from_text(evidence_node.text)
                    
                    # æ·»åŠ æ¥æºä¿¡æ¯
                    for qa_pair in qa_pairs:
                        qa_pair["evidence_source"] = evidence_node.node_id
                        qa_pair["evidence_concept"] = evidence_node.metadata.get("concept_text", "æœªçŸ¥")
                        qa_pair["generation_timestamp"] = datetime.now().isoformat()
                    
                    all_qa_pairs.extend(qa_pairs)
                    
                except Exception as e:
                    logger.warning(f"ä»è¯æ® {i+1} ç”Ÿæˆé—®ç­”å¯¹å¤±è´¥: {e}")
                    continue
            
            processing_time = time.time() - start_time
            
            # ç»Ÿè®¡é—®ç­”å¯¹ç±»å‹
            qa_types = {}
            for qa_pair in all_qa_pairs:
                qa_type = qa_pair.get("type", "unknown")
                qa_types[qa_type] = qa_types.get(qa_type, 0) + 1
            
            # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
            report_content = f"""
é—®ç­”ç”Ÿæˆç»Ÿè®¡:
- è¾“å…¥è¯æ®æ•°: {len(evidence_nodes)}
- ç”Ÿæˆé—®ç­”å¯¹æ•°: {len(all_qa_pairs)}
- å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’

é—®ç­”ç±»å‹åˆ†å¸ƒ:
{'-'*60}
"""
            
            for qa_type, count in qa_types.items():
                report_content += f"- {qa_type}: {count} ä¸ª\n"
            
            report_content += f"\n{'-'*60}\né—®ç­”å¯¹è¯¦æƒ…:\n"
            
            # ä¸ºæ¯ä¸ªé—®ç­”å¯¹ç”Ÿæˆè¯¦ç»†ä¿¡æ¯
            for i, qa_pair in enumerate(all_qa_pairs, 1):
                report_content += f"""
é—®ç­”å¯¹ {i}:
- ç±»å‹: {qa_pair.get('type', 'æœªçŸ¥')}
- éš¾åº¦: {qa_pair.get('difficulty', 'æœªçŸ¥')}
- æ¥æºæ¦‚å¿µ: {qa_pair.get('evidence_concept', 'æœªçŸ¥')}
- é—®é¢˜: {qa_pair.get('question', 'æœªçŸ¥')}
- ç­”æ¡ˆ: {qa_pair.get('answer', 'æœªçŸ¥')[:100]}...
{'-'*40}
"""
            
            # ä¿å­˜txtæŠ¥å‘Š
            self.save_step_txt_report("æ­¥éª¤7_é—®ç­”ç”Ÿæˆ", "7", report_content)
            
            # ä¿å­˜é—®ç­”å¯¹çš„å®Œæ•´å†…å®¹
            qa_details_file = self.step_dirs["step7"] / "é—®ç­”å¯¹è¯¦ç»†å†…å®¹.txt"
            with open(qa_details_file, 'w', encoding='utf-8') as f:
                f.write("é—®ç­”ç”Ÿæˆè¯¦ç»†ç»“æœ\n")
                f.write("=" * 80 + "\n\n")
                
                for i, qa_pair in enumerate(all_qa_pairs, 1):
                    f.write(f"é—®ç­”å¯¹ {i}\n")
                    f.write("-" * 50 + "\n")
                    f.write(f"ç±»å‹: {qa_pair.get('type', 'æœªçŸ¥')}\n")
                    f.write(f"éš¾åº¦: {qa_pair.get('difficulty', 'æœªçŸ¥')}\n")
                    f.write(f"æ¥æºæ¦‚å¿µ: {qa_pair.get('evidence_concept', 'æœªçŸ¥')}\n")
                    f.write(f"æ¥æºè¯æ®ID: {qa_pair.get('evidence_source', 'æœªçŸ¥')}\n")
                    f.write(f"ç”Ÿæˆæ—¶é—´: {qa_pair.get('generation_timestamp', 'æœªçŸ¥')}\n")
                    f.write("é—®é¢˜:\n")
                    f.write(qa_pair.get('question', 'æœªçŸ¥'))
                    f.write("\n\nç­”æ¡ˆ:\n")
                    f.write(qa_pair.get('answer', 'æœªçŸ¥'))
                    f.write("\n\n" + "=" * 80 + "\n\n")
            
            # ç”Ÿæˆæ ‡å‡†æ ¼å¼çš„è®­ç»ƒæ•°æ®
            training_data = []
            for i, qa_pair in enumerate(all_qa_pairs):
                training_item = {
                    "_id": uuid.uuid4().hex,
                    "Question": qa_pair.get("question", ""),
                    "Answer": qa_pair.get("answer", ""),
                    "Type": qa_pair.get("type", "unknown"),
                    "Difficulty": qa_pair.get("difficulty", "medium"),
                    "Domain": getattr(self.qa_generator, 'domain', 'general'),
                    "Rationale": qa_pair.get("rationale", ""),
                    "Context": []  # è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦æ„å»ºContext
                }
                training_data.append(training_item)
            
            # ä¿å­˜è®­ç»ƒæ•°æ®
            training_data_file = self.step_dirs["step7"] / "è®­ç»ƒæ•°æ®.jsonl"
            with open(training_data_file, 'w', encoding='utf-8') as f:
                for item in training_data:
                    f.write(json.dumps(item, ensure_ascii=False) + "\n")
            
            result = {
                "success": True,
                "qa_pairs": all_qa_pairs,
                "training_data": training_data,
                "qa_count": len(all_qa_pairs),
                "qa_types": qa_types,
                "processing_time": processing_time
            }
            
            self.step_results["step7"] = result
            self.step_timings["step7"] = processing_time
            
            print(f"âœ… æ­¥éª¤7å®Œæˆ - é—®ç­”ç”ŸæˆæˆåŠŸ")
            print(f"   ç”Ÿæˆé—®ç­”å¯¹: {len(all_qa_pairs)} ä¸ª")
            print(f"   é—®é¢˜ç±»å‹: {list(qa_types.keys())}")
            print(f"   å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
            
            return result
            
        except Exception as e:
            error_msg = f"é—®ç­”ç”Ÿæˆå¤±è´¥: {str(e)}"
            logger.error(error_msg)
            
            error_report = f"""
é”™è¯¯ä¿¡æ¯:
{error_msg}

é”™è¯¯è¯¦æƒ…:
{traceback.format_exc()}
"""
            self.save_step_txt_report("æ­¥éª¤7_é—®ç­”ç”Ÿæˆ_é”™è¯¯", "7", error_report)
            
            result = {
                "success": False,
                "error": error_msg,
                "processing_time": time.time() - start_time
            }
            
            self.step_results["step7"] = result
            return result
    
    def step8_final_summary(self) -> Dict[str, Any]:
        """æ­¥éª¤8: æœ€ç»ˆæ±‡æ€»"""
        print("\n" + "="*60)
        print("ğŸ”„ æ­¥éª¤8: æœ€ç»ˆæ±‡æ€»")
        print("="*60)
        
        start_time = time.time()
        
        try:
            # æ±‡æ€»æ‰€æœ‰æ­¥éª¤çš„ç»“æœ
            total_processing_time = sum(self.step_timings.values())
            
            # ğŸ”§ æ·»åŠ è¿™ä¸€è¡Œï¼šè®¡ç®—å½“å‰æ­¥éª¤çš„å¤„ç†æ—¶é—´
            processing_time = time.time() - start_time
            
            # ç”Ÿæˆæœ€ç»ˆæ±‡æ€»æŠ¥å‘Š
            report_content = f"""
Pipelineå¤„ç†å®Œæ•´æ±‡æ€»æŠ¥å‘Š
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
å¤„ç†æ–‡ä»¶: {self.input_file}

æ€»ä½“ç»Ÿè®¡:
- æ€»å¤„ç†æ—¶é—´: {total_processing_time:.2f} ç§’
- æˆåŠŸæ­¥éª¤æ•°: {sum(1 for step in self.step_results.values() if step.get('success', False))}
- å¤±è´¥æ­¥éª¤æ•°: {sum(1 for step in self.step_results.values() if not step.get('success', False))}

å„æ­¥éª¤è€—æ—¶:
{'-'*60}
"""
            
            step_names = [
                "æ­¥éª¤1: æ–‡ä»¶åŠ è½½",
                "æ­¥éª¤2: æ–‡æ¡£åˆ†å—", 
                "æ­¥éª¤3: æ¦‚å¿µæå–",
                "æ­¥éª¤4: æ¦‚å¿µåˆå¹¶",
                "æ­¥éª¤5: æ¦‚å¿µæ£€ç´¢", 
                "æ­¥éª¤6: è¯æ®æå–",
                "æ­¥éª¤7: é—®ç­”ç”Ÿæˆ"
            ]
            
            for i, step_name in enumerate(step_names, 1):
                step_key = f"step{i}"
                if step_key in self.step_timings:
                    timing = self.step_timings[step_key]
                    status = "âœ… æˆåŠŸ" if self.step_results[step_key]["success"] else "âŒ å¤±è´¥"
                    report_content += f"- {step_name}: {status}, è€—æ—¶: {timing:.2f} ç§’\n"
            
            # ä¿å­˜txtæŠ¥å‘Š
            self.save_step_txt_report("æ­¥éª¤8_æœ€ç»ˆæ±‡æ€»", "8", report_content)
            
            result = {
                "success": True,
                "total_processing_time": total_processing_time,
                "step_results": self.step_results,
                "step_timings": self.step_timings
            }
            
            self.step_results["step8"] = result
            self.step_timings["step8"] = processing_time
            
            print(f"âœ… æ­¥éª¤8å®Œæˆ - æœ€ç»ˆæ±‡æ€»æˆåŠŸ")
            print(f"   æ€»å¤„ç†æ—¶é—´: {total_processing_time:.2f} ç§’")
            print(f"   æˆåŠŸæ­¥éª¤æ•°: {sum(1 for step in self.step_results.values() if step.get('success', False))} ä¸ª")
            print(f"   å¤±è´¥æ­¥éª¤æ•°: {sum(1 for step in self.step_results.values() if not step.get('success', False))} ä¸ª")
            
            return result
            
        except Exception as e:
            error_msg = f"æœ€ç»ˆæ±‡æ€»å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            
            error_report = f"""
é”™è¯¯ä¿¡æ¯:
{error_msg}

é”™è¯¯è¯¦æƒ…:
{traceback.format_exc()}
"""
            self.save_step_txt_report("æ­¥éª¤8_æœ€ç»ˆæ±‡æ€»_é”™è¯¯", "8", error_report)
            
            result = {
                "success": False,
                "error": error_msg,
                "processing_time": time.time() - start_time
            }
            
            self.step_results["step8"] = result
            return result

def main():
    """ä¸»å‡½æ•° - æ‰§è¡Œåˆ†æ­¥éª¤å¤„ç†"""
    print("ğŸ”„ åˆ†æ­¥éª¤Pipelineå¤„ç†å·¥å…·")
    print("=" * 50)
    
    # ğŸ”§ ä¿®æ”¹ï¼šè®¾ç½®é»˜è®¤è¾“å…¥æ–‡ä»¶
    default_file = "attention is all you need.pdf"
    input_file = input(f"è¯·è¾“å…¥æ–‡ä»¶è·¯å¾„ (é»˜è®¤: {default_file}): ").strip()
    if not input_file:
        input_file = default_file
        print(f"âœ… ä½¿ç”¨é»˜è®¤æ–‡ä»¶: {default_file}")
    
    input_path = Path(input_file)
    if not input_path.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        return
    
    # é…ç½®è¾“å‡ºç›®å½•
    output_dir = input("è¾“å‡ºç›®å½• (é»˜è®¤: step_by_step_results_en): ").strip()
    if not output_dir:
        output_dir = "step_by_step_results_en"
    
    # è¯¢é—®æ˜¯å¦å¯ç”¨QAç”Ÿæˆ
    enable_qa = input("æ˜¯å¦å¯ç”¨é—®ç­”ç”Ÿæˆ? (y/N): ").strip().lower() == 'y'
    
    qa_config = {}
    qa_model = "gpt-4o-mini"
    
    if enable_qa:
        print("\né…ç½®é—®ç­”ç”Ÿæˆå‚æ•°:")
        print("1. ä½¿ç”¨é»˜è®¤é…ç½®")
        print("2. è‡ªå®šä¹‰é…ç½®")
        
        config_choice = input("è¯·é€‰æ‹© (1-2): ").strip()
        
        if config_choice == "2":
            print("\nè¯·è¾“å…¥æ¯ç§ç±»å‹çš„é—®é¢˜æ•°é‡:")
            qa_config = {
                "remember": int(input("Rememberç±»å‹ (é»˜è®¤2): ") or "2"),
                "understand": int(input("Understandç±»å‹ (é»˜è®¤2): ") or "2"),
                "apply": int(input("Applyç±»å‹ (é»˜è®¤1): ") or "1"),
                "analyze": int(input("Analyzeç±»å‹ (é»˜è®¤1): ") or "1"),
                "evaluate": int(input("Evaluateç±»å‹ (é»˜è®¤1): ") or "1"),
                "create": int(input("Createç±»å‹ (é»˜è®¤1): ") or "1")
            }
        
        qa_model = input("QAç”Ÿæˆæ¨¡å‹ (é»˜è®¤gpt-4o-mini): ").strip() or "gpt-4o-mini"
    
    print(f"\nğŸ“„ å¼€å§‹å¤„ç†æ–‡ä»¶: {input_path.name}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"â“ é—®ç­”ç”Ÿæˆ: {'å¯ç”¨' if enable_qa else 'ç¦ç”¨'}")
    
    try:
        # åˆå§‹åŒ–å¤„ç†å™¨
        processor = StepByStepPipelineProcessor(
            input_file=str(input_path),
            output_dir=output_dir,
            enable_qa_generation=enable_qa,
            qa_model_name=qa_model,
            questions_per_type=qa_config if qa_config else None
        )
        
        # æ‰§è¡Œæ‰€æœ‰æ­¥éª¤
        print("\nğŸš€ å¼€å§‹æ‰§è¡Œåˆ†æ­¥éª¤å¤„ç†...")
        
        # æ­¥éª¤1: æ–‡ä»¶åŠ è½½
        step1_result = processor.step1_load_file()
        if not step1_result["success"]:
            print("âŒ æ­¥éª¤1å¤±è´¥ï¼Œç»ˆæ­¢å¤„ç†")
            return
        
        # æ­¥éª¤2: æ–‡æ¡£åˆ†å—
        step2_result = processor.step2_document_chunking()
        if not step2_result["success"]:
            print("âŒ æ­¥éª¤2å¤±è´¥ï¼Œç»ˆæ­¢å¤„ç†")
            return
        
        # æ­¥éª¤3: æ¦‚å¿µæå–
        step3_result = processor.step3_concept_extraction()
        if not step3_result["success"]:
            print("âŒ æ­¥éª¤3å¤±è´¥ï¼Œç»ˆæ­¢å¤„ç†")
            return
        
        # æ­¥éª¤4: æ¦‚å¿µåˆå¹¶
        step4_result = processor.step4_concept_merging()
        if not step4_result["success"]:
            print("âŒ æ­¥éª¤4å¤±è´¥ï¼Œç»ˆæ­¢å¤„ç†")
            return
        
        # æ­¥éª¤5: æ¦‚å¿µæ£€ç´¢
        step5_result = processor.step5_concept_retrieval()
        if not step5_result["success"]:
            print("âŒ æ­¥éª¤5å¤±è´¥ï¼Œç»ˆæ­¢å¤„ç†")
            return
        
        # æ­¥éª¤6: è¯æ®æå–
        step6_result = processor.step6_evidence_extraction()
        if not step6_result["success"]:
            print("âŒ æ­¥éª¤6å¤±è´¥ï¼Œç»ˆæ­¢å¤„ç†")
            return
        
        # æ­¥éª¤7: é—®ç­”ç”Ÿæˆ
        step7_result = processor.step7_qa_generation()
        if not step7_result["success"] and not step7_result.get("skipped"):
            print("âŒ æ­¥éª¤7å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œæ±‡æ€»")
        
        # æ­¥éª¤8: æœ€ç»ˆæ±‡æ€»
        step8_result = processor.step8_final_summary()
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        print("\n" + "="*60)
        print("ğŸ‰ åˆ†æ­¥éª¤å¤„ç†å®Œæˆ!")
        print("="*60)
        
        total_time = sum(processor.step_timings.values())
        success_count = sum(1 for step in processor.step_results.values() if step.get('success', False))
        
        print(f"ğŸ“Š å¤„ç†ç»“æœ:")
        print(f"   - æ€»å¤„ç†æ—¶é—´: {total_time:.2f} ç§’")
        print(f"   - æˆåŠŸæ­¥éª¤æ•°: {success_count}/8")
        print(f"   - è¾“å‡ºç›®å½•: {processor.output_dir}")
        
        # æ˜¾ç¤ºå„æ­¥éª¤ç»“æœ
        step_names = [
            "æ–‡ä»¶åŠ è½½", "æ–‡æ¡£åˆ†å—", "æ¦‚å¿µæå–", "æ¦‚å¿µåˆå¹¶",
            "æ¦‚å¿µæ£€ç´¢", "è¯æ®æå–", "é—®ç­”ç”Ÿæˆ", "æœ€ç»ˆæ±‡æ€»"
        ]
        
        print(f"\nğŸ“‹ å„æ­¥éª¤çŠ¶æ€:")
        for i, step_name in enumerate(step_names, 1):
            step_key = f"step{i}"
            if step_key in processor.step_results:
                result = processor.step_results[step_key]
                status = "âœ…" if result.get("success") else "âš ï¸" if result.get("skipped") else "âŒ"
                timing = processor.step_timings.get(step_key, 0)
                print(f"   {status} {step_name}: {timing:.2f}ç§’")
        
        print(f"\nğŸ“ æŸ¥çœ‹è¯¦ç»†ç»“æœ:")
        print(f"   æ–‡ä»¶å¤¹: {processor.output_dir}/")
        print(f"   æ¯ä¸ªæ­¥éª¤éƒ½æœ‰ç‹¬ç«‹çš„txtæŠ¥å‘Š")
        
        if enable_qa and step7_result.get("success"):
            qa_count = step7_result.get("qa_count", 0)
            print(f"\nâ“ é—®ç­”ç”Ÿæˆç»“æœ:")
            print(f"   - ç”Ÿæˆé—®ç­”å¯¹: {qa_count} ä¸ª")
            print(f"   - è®­ç»ƒæ•°æ®: {processor.output_dir}/07_é—®ç­”ç”Ÿæˆ/è®­ç»ƒæ•°æ®.jsonl")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ å¤„ç†è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        logger.error(f"ä¸»ç¨‹åºé”™è¯¯: {traceback.format_exc()}")

if __name__ == "__main__":
    main()
