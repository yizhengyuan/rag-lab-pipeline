"""
æ•´åˆç‰ˆ Pipeline - å°†æ¦‚å¿µæå–å’Œé—®ç­”å¯¹ç”Ÿæˆåˆå¹¶åœ¨ä¸€ä¸ªè„šæœ¬ä¸­
==============================================================

åŠŸèƒ½ï¼š
1. è¾“å…¥ PDF æˆ–å…¶ä»–æ ¼å¼æ–‡æ¡£
2. ä½¿ç”¨ pipeline.py çš„ ImprovedConceptBasedPipeline è¿›è¡Œæ¦‚å¿µæå–
3. ä½¿ç”¨ data_generate_0526.py çš„ SimpleDataGenerator ç”Ÿæˆé—®ç­”å¯¹
4. è¾“å‡ºå®Œæ•´çš„è®­ç»ƒæ•°æ®
"""

import os
import json
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
import traceback

# é»˜è®¤é…ç½®
DEFAULT_CONFIG = {
    "llm_model": "gpt-4o-mini",
    "OPENAI_API_KEY": "sk-zk2884399e3bbb43b998bd31be7b517f82f67bb0e95df2a1",
    "BASE_URL": "https://api.zhizengzeng.com/v1/"
}

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["OPENAI_API_KEY"] = DEFAULT_CONFIG["OPENAI_API_KEY"]
if DEFAULT_CONFIG["BASE_URL"]:
    os.environ["BASE_URL"] = DEFAULT_CONFIG["BASE_URL"]

# å¯¼å…¥ LlamaIndex ç»„ä»¶
from llama_index.core import SimpleDirectoryReader, Document, Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

# å¯¼å…¥ pipeline.py çš„ç±»å’Œå‡½æ•°
from pipeline import ImprovedConceptBasedPipeline

# å¯¼å…¥ data_generate_0526.py çš„ç±»å’Œå‡½æ•°
from data_generate_0526 import (
    SimpleDataGenerator, 
    FileProcessor, 
    QUESTIONS_PER_TYPE
)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class IntegratedPipeline:
    """æ•´åˆç‰ˆ Pipeline - æ¦‚å¿µæå– + é—®ç­”å¯¹ç”Ÿæˆ"""
    
    def __init__(self, 
                 openai_api_key: str = None,
                 base_url: str = None,
                 model_name: str = None,
                 embedding_model: str = "text-embedding-ada-002",
                 output_dir: str = "./integrated_output",
                 questions_per_type: Dict[str, int] = None):
        """
        åˆå§‹åŒ–æ•´åˆ Pipeline
        
        Args:
            openai_api_key: OpenAI API å¯†é’¥ï¼ˆå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
            base_url: API åŸºç¡€ URLï¼ˆå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
            model_name: ç”¨äºé—®ç­”ç”Ÿæˆçš„æ¨¡å‹åç§°ï¼ˆå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
            embedding_model: ç”¨äºæ¦‚å¿µæå–çš„åµŒå…¥æ¨¡å‹
            output_dir: è¾“å‡ºç›®å½•
            questions_per_type: æ¯ç§è®¤çŸ¥å±‚æ¬¡çš„é—®é¢˜æ•°é‡é…ç½®
        """
        # ä½¿ç”¨æä¾›çš„å‚æ•°æˆ–é»˜è®¤é…ç½®
        self.openai_api_key = openai_api_key or DEFAULT_CONFIG["OPENAI_API_KEY"]
        self.base_url = base_url or DEFAULT_CONFIG["BASE_URL"]
        self.model_name = model_name or DEFAULT_CONFIG["llm_model"]
        self.embedding_model = embedding_model
        self.output_dir = output_dir
        self.questions_per_type = questions_per_type or QUESTIONS_PER_TYPE
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ["OPENAI_API_KEY"] = self.openai_api_key
        if self.base_url:
            os.environ["BASE_URL"] = self.base_url
        
        logger.info(f"ğŸ”§ é…ç½®ä¿¡æ¯:")
        logger.info(f"   - æ¨¡å‹: {self.model_name}")
        logger.info(f"   - API Key: {self.openai_api_key[:10]}...{self.openai_api_key[-4:]}")
        logger.info(f"   - Base URL: {self.base_url}")
        logger.info(f"   - åµŒå…¥æ¨¡å‹: {self.embedding_model}")
        logger.info(f"   - è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # é…ç½® LlamaIndex å…¨å±€è®¾ç½®ï¼Œç¡®ä¿ä½¿ç”¨è‡ªå®šä¹‰ base_url
        self._setup_llamaindex_settings()
        
        # åˆå§‹åŒ–ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶
        self.concept_pipeline = ImprovedConceptBasedPipeline(
            openai_api_key=self.openai_api_key,
            model_name=self.model_name,
            embedding_model=self.embedding_model
        )
        
        self.qa_generator = SimpleDataGenerator(
            model_name=self.model_name,
            questions_per_type=self.questions_per_type
        )
        
        self.file_processor = FileProcessor()
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.ensure_output_dirs()
    
    def _setup_llamaindex_settings(self):
        """é…ç½® LlamaIndex å…¨å±€è®¾ç½®ï¼Œç¡®ä¿ä½¿ç”¨è‡ªå®šä¹‰ base_url"""
        try:
            # åˆ›å»ºå¸¦æœ‰è‡ªå®šä¹‰ base_url çš„ LLM å’ŒåµŒå…¥æ¨¡å‹
            if self.base_url:
                # é…ç½® LLM
                Settings.llm = OpenAI(
                    model=self.model_name,
                    api_key=self.openai_api_key,
                    api_base=self.base_url,
                    temperature=0.1
                )
                
                # é…ç½®åµŒå…¥æ¨¡å‹
                Settings.embed_model = OpenAIEmbedding(
                    model=self.embedding_model,
                    api_key=self.openai_api_key,
                    api_base=self.base_url
                )
                
                logger.info(f"   âœ… LlamaIndex é…ç½®å®Œæˆï¼Œä½¿ç”¨è‡ªå®šä¹‰ API: {self.base_url}")
            else:
                # ä½¿ç”¨é»˜è®¤é…ç½®
                Settings.llm = OpenAI(
                    model=self.model_name,
                    api_key=self.openai_api_key,
                    temperature=0.1
                )
                
                Settings.embed_model = OpenAIEmbedding(
                    model=self.embedding_model,
                    api_key=self.openai_api_key
                )
                
                logger.info(f"   âœ… LlamaIndex é…ç½®å®Œæˆï¼Œä½¿ç”¨é»˜è®¤ OpenAI API")
                
        except Exception as e:
            logger.warning(f"   âš ï¸ LlamaIndex é…ç½®è­¦å‘Š: {e}")
            logger.info(f"   ğŸ”„ å°†ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®")
    
    def ensure_output_dirs(self):
        """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
    
    def process_single_file(self, file_path: str, save_intermediate: bool = True) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶çš„å®Œæ•´æµç¨‹
        
        Args:
            file_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
            save_intermediate: æ˜¯å¦ä¿å­˜ä¸­é—´ç»“æœ
            
        Returns:
            åŒ…å«æ‰€æœ‰ç»“æœçš„å­—å…¸
        """
        try:
            file_name = os.path.basename(file_path)
            base_name = os.path.splitext(file_name)[0]
            
            logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path}")
            logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
            logger.info("=" * 80)
            
            # æ­¥éª¤1: åŠ è½½æ–‡æ¡£
            logger.info("ğŸ“„ æ­¥éª¤1: åŠ è½½æ–‡æ¡£...")
            documents = self._load_document(file_path)
            
            # æå–å®Œæ•´æ–‡æœ¬ç”¨äºé—®ç­”ç”Ÿæˆ
            full_text = "\n\n".join([doc.text for doc in documents])
            logger.info(f"   âœ… æ–‡æ¡£åŠ è½½å®Œæˆï¼Œæ€»å­—ç¬¦æ•°: {len(full_text)}")
            
            # æ­¥éª¤2: æ¦‚å¿µæå– Pipeline
            logger.info("ğŸ”¬ æ­¥éª¤2: æ‰§è¡Œæ¦‚å¿µæå– Pipeline...")
            try:
                concept_results = self.concept_pipeline.run_pipeline(documents)
                self.concept_pipeline.save_results(concept_results, f"{base_name}_concepts.json")
                
                logger.info(f"   âœ… æ¦‚å¿µæå–å®Œæˆ:")
                logger.info(f"      - Chunks: {len(concept_results['chunk_nodes'])}")
                logger.info(f"      - Concepts: {len(concept_results['concept_nodes'])}")
                logger.info(f"      - Evidence: {len(concept_results['evidence_nodes'])}")
            except Exception as e:
                logger.error(f"   âŒ æ¦‚å¿µæå–å¤±è´¥: {e}")
                logger.info(f"   ğŸ”„ è·³è¿‡æ¦‚å¿µæå–ï¼Œç»§ç»­é—®ç­”ç”Ÿæˆ...")
                concept_results = {
                    "chunk_nodes": [],
                    "concept_nodes": [],
                    "concept_to_chunks": {},
                    "evidence_nodes": [],
                    "indexes": {}
                }
            
            # æ­¥éª¤3: é—®ç­”å¯¹ç”Ÿæˆ
            logger.info("â“ æ­¥éª¤3: ç”Ÿæˆé—®ç­”å¯¹...")
            qa_pairs = self.qa_generator.generate_qa_pairs_from_text(full_text)
            logger.info(f"   âœ… é—®ç­”å¯¹ç”Ÿæˆå®Œæˆï¼Œå…± {len(qa_pairs)} ä¸ª")
            
            # æ­¥éª¤4: åˆ›å»ºè®­ç»ƒæ•°æ®
            logger.info("ğŸ“Š æ­¥éª¤4: åˆ›å»ºè®­ç»ƒæ•°æ®...")
            training_data = []
            for qa_pair in qa_pairs:
                training_item = self.qa_generator.create_training_data(qa_pair, full_text, file_name)
                if training_item:
                    training_data.append(training_item)
            logger.info(f"   âœ… è®­ç»ƒæ•°æ®åˆ›å»ºå®Œæˆï¼Œå…± {len(training_data)} æ¡")
            
            # æ­¥éª¤5: ä¿å­˜ç»“æœ
            logger.info("ğŸ’¾ æ­¥éª¤5: ä¿å­˜ç»“æœ...")
            results = {
                "file_info": {
                    "file_path": file_path,
                    "file_name": file_name,
                    "base_name": base_name,
                    "text_length": len(full_text)
                },
                "config_info": {
                    "model_name": self.model_name,
                    "embedding_model": self.embedding_model,
                    "base_url": self.base_url,
                    "questions_per_type": self.questions_per_type
                },
                "concept_results": concept_results,
                "qa_pairs": qa_pairs,
                "training_data": training_data,
                "processing_time": datetime.now().isoformat()
            }
            
            if save_intermediate:
                self._save_results(results, base_name)
            
            logger.info(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {file_path}")
            logger.info(f"ğŸ“ˆ ç”Ÿæˆç»Ÿè®¡:")
            logger.info(f"   - æ¦‚å¿µæ•°é‡: {len(concept_results['concept_nodes'])}")
            logger.info(f"   - è¯æ®æ•°é‡: {len(concept_results['evidence_nodes'])}")
            logger.info(f"   - é—®ç­”å¯¹æ•°é‡: {len(qa_pairs)}")
            logger.info(f"   - è®­ç»ƒæ•°æ®æ¡æ•°: {len(training_data)}")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            return {}
    
    def process_directory(self, dir_path: str, file_extensions: List[str] = None) -> Dict[str, Any]:
        """
        å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        
        Args:
            dir_path: ç›®å½•è·¯å¾„
            file_extensions: æ”¯æŒçš„æ–‡ä»¶æ‰©å±•ååˆ—è¡¨
            
        Returns:
            åŒ…å«æ‰€æœ‰ç»“æœçš„å­—å…¸
        """
        if file_extensions is None:
            file_extensions = ['.pdf', '.txt', '.md', '.json', '.jsonl']
        
        logger.info(f"ğŸš€ å¼€å§‹å¤„ç†ç›®å½•: {dir_path}")
        logger.info(f"ğŸ“ æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_extensions}")
        logger.info("=" * 80)
        
        all_results = []
        all_training_data = []
        
        # éå†ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        for root, _, files in os.walk(dir_path):
            for file in files:
                if any(file.endswith(ext) for ext in file_extensions):
                    file_path = os.path.join(root, file)
                    try:
                        result = self.process_single_file(file_path, save_intermediate=True)
                        if result:
                            all_results.append(result)
                            all_training_data.extend(result.get("training_data", []))
                    except Exception as e:
                        logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                        continue
        
        # ä¿å­˜åˆå¹¶çš„ç»“æœ
        combined_results = {
            "directory_info": {
                "directory_path": dir_path,
                "total_files": len(all_results),
                "total_training_data": len(all_training_data)
            },
            "config_info": {
                "model_name": self.model_name,
                "embedding_model": self.embedding_model,
                "base_url": self.base_url,
                "questions_per_type": self.questions_per_type
            },
            "individual_results": all_results,
            "combined_training_data": all_training_data,
            "processing_time": datetime.now().isoformat()
        }
        
        self._save_combined_results(combined_results)
        
        logger.info(f"âœ… ç›®å½•å¤„ç†å®Œæˆ: {dir_path}")
        logger.info(f"ğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
        logger.info(f"   - å¤„ç†æ–‡ä»¶æ•°: {len(all_results)}")
        logger.info(f"   - æ€»è®­ç»ƒæ•°æ®æ¡æ•°: {len(all_training_data)}")
        
        return combined_results
    
    def _load_document(self, file_path: str) -> List[Document]:
        """åŠ è½½æ–‡æ¡£"""
        ext = os.path.splitext(file_path)[1].lower()
        
        if ext == '.pdf':
            # å¯¹äº PDFï¼Œä½¿ç”¨ LlamaIndex çš„ SimpleDirectoryReader
            reader = SimpleDirectoryReader(input_files=[file_path])
            return reader.load_data()
        else:
            # å¯¹äºå…¶ä»–æ ¼å¼ï¼Œä½¿ç”¨ FileProcessor æå–æ–‡æœ¬ï¼Œç„¶ååˆ›å»º Document
            text = self.file_processor.extract_text(file_path)
            return [Document(text=text, metadata={"file_path": file_path})]
    
    def _save_results(self, results: Dict[str, Any], base_name: str):
        """ä¿å­˜å•ä¸ªæ–‡ä»¶çš„ç»“æœ"""
        # ä¿å­˜æ¦‚å¿µæå–ç»“æœï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if results["concept_results"]["concept_nodes"]:
            concept_output_path = os.path.join(self.output_dir, f"{base_name}_concepts.json")
            self.concept_pipeline.save_results(results["concept_results"], concept_output_path)
        
        # ä¿å­˜é—®ç­”å¯¹ï¼ˆJSONæ ¼å¼ï¼‰
        qa_output_path = os.path.join(self.output_dir, f"{base_name}_qa_pairs.json")
        with open(qa_output_path, 'w', encoding='utf-8') as f:
            json.dump(results["qa_pairs"], f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜è®­ç»ƒæ•°æ®ï¼ˆJSONLæ ¼å¼ï¼‰
        training_output_path = os.path.join(self.output_dir, f"{base_name}_training.jsonl")
        with open(training_output_path, 'w', encoding='utf-8') as f:
            for data in results["training_data"]:
                f.write(json.dumps(data, ensure_ascii=False) + "\n")
        
        # ä¿å­˜å®Œæ•´ç»“æœæ‘˜è¦
        summary_output_path = os.path.join(self.output_dir, f"{base_name}_summary.json")
        summary = {
            "file_info": results["file_info"],
            "config_info": results["config_info"],
            "statistics": {
                "concepts_count": len(results["concept_results"]["concept_nodes"]),
                "evidence_count": len(results["concept_results"]["evidence_nodes"]),
                "qa_pairs_count": len(results["qa_pairs"]),
                "training_data_count": len(results["training_data"])
            },
            "processing_time": results["processing_time"]
        }
        with open(summary_output_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"   âœ… ç»“æœå·²ä¿å­˜:")
        if results["concept_results"]["concept_nodes"]:
            logger.info(f"      - æ¦‚å¿µæå–: {concept_output_path}")
        logger.info(f"      - é—®ç­”å¯¹: {qa_output_path}")
        logger.info(f"      - è®­ç»ƒæ•°æ®: {training_output_path}")
        logger.info(f"      - æ‘˜è¦: {summary_output_path}")
    
    def _save_combined_results(self, combined_results: Dict[str, Any]):
        """ä¿å­˜åˆå¹¶çš„ç»“æœ"""
        # ä¿å­˜åˆå¹¶çš„è®­ç»ƒæ•°æ®
        combined_training_path = os.path.join(self.output_dir, "combined_training_data.jsonl")
        with open(combined_training_path, 'w', encoding='utf-8') as f:
            for data in combined_results["combined_training_data"]:
                f.write(json.dumps(data, ensure_ascii=False) + "\n")
        
        # ä¿å­˜å®Œæ•´çš„åˆå¹¶ç»“æœ
        combined_summary_path = os.path.join(self.output_dir, "combined_summary.json")
        with open(combined_summary_path, 'w', encoding='utf-8') as f:
            json.dump(combined_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"   âœ… åˆå¹¶ç»“æœå·²ä¿å­˜:")
        logger.info(f"      - åˆå¹¶è®­ç»ƒæ•°æ®: {combined_training_path}")
        logger.info(f"      - åˆå¹¶æ‘˜è¦: {combined_summary_path}")

def add_cli():
    """æ·»åŠ å‘½ä»¤è¡Œæ¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ•´åˆç‰ˆ Pipeline - æ¦‚å¿µæå– + é—®ç­”å¯¹ç”Ÿæˆ')
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--input', '-i', required=True, help='è¾“å…¥æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„')
    parser.add_argument('--output', '-o', default='./integrated_output', help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('--api-key', '-k', default=DEFAULT_CONFIG["OPENAI_API_KEY"], help=f'OpenAI API å¯†é’¥ï¼ˆé»˜è®¤: {DEFAULT_CONFIG["OPENAI_API_KEY"][:10]}...ï¼‰')
    parser.add_argument('--base-url', '-b', default=DEFAULT_CONFIG["BASE_URL"], help=f'API åŸºç¡€ URLï¼ˆé»˜è®¤: {DEFAULT_CONFIG["BASE_URL"]}ï¼‰')
    parser.add_argument('--model', '-m', default=DEFAULT_CONFIG["llm_model"], help=f'ç”¨äºé—®ç­”ç”Ÿæˆçš„æ¨¡å‹åç§°ï¼ˆé»˜è®¤: {DEFAULT_CONFIG["llm_model"]}ï¼‰')
    parser.add_argument('--embedding-model', '-e', default='text-embedding-ada-002', help='ç”¨äºæ¦‚å¿µæå–çš„åµŒå…¥æ¨¡å‹')
    
    # é—®é¢˜æ•°é‡é…ç½®
    parser.add_argument('--remember', type=int, default=2, help='Rememberç±»å‹é—®é¢˜æ•°é‡')
    parser.add_argument('--understand', type=int, default=2, help='Understandç±»å‹é—®é¢˜æ•°é‡')
    parser.add_argument('--apply', type=int, default=1, help='Applyç±»å‹é—®é¢˜æ•°é‡')
    parser.add_argument('--analyze', type=int, default=1, help='Analyzeç±»å‹é—®é¢˜æ•°é‡')
    parser.add_argument('--evaluate', type=int, default=1, help='Evaluateç±»å‹é—®é¢˜æ•°é‡')
    parser.add_argument('--create', type=int, default=1, help='Createç±»å‹é—®é¢˜æ•°é‡')
    
    # æ–°å¢ï¼šä»…é—®ç­”ç”Ÿæˆæ¨¡å¼
    parser.add_argument('--qa-only', action='store_true', help='ä»…æ‰§è¡Œé—®ç­”ç”Ÿæˆï¼Œè·³è¿‡æ¦‚å¿µæå–')
    
    args = parser.parse_args()
    
    # æ„å»ºé—®é¢˜æ•°é‡é…ç½®
    questions_per_type = {
        "remember": args.remember,
        "understand": args.understand,
        "apply": args.apply,
        "analyze": args.analyze,
        "evaluate": args.evaluate,
        "create": args.create
    }
    
    # åˆ›å»ºæ•´åˆ Pipeline
    pipeline = IntegratedPipeline(
        openai_api_key=args.api_key,
        base_url=args.base_url,
        model_name=args.model,
        embedding_model=args.embedding_model,
        output_dir=args.output,
        questions_per_type=questions_per_type
    )
    
    logger.info(f"é…ç½®çš„é—®é¢˜æ•°é‡: {questions_per_type}")
    if args.qa_only:
        logger.info("âš ï¸ ä»…é—®ç­”ç”Ÿæˆæ¨¡å¼ï¼šå°†è·³è¿‡æ¦‚å¿µæå–æ­¥éª¤")
    
    # å¤„ç†è¾“å…¥
    if os.path.isdir(args.input):
        pipeline.process_directory(args.input)
    else:
        pipeline.process_single_file(args.input)

# ä½¿ç”¨ç¤ºä¾‹
def example_usage():
    """ä½¿ç”¨ç¤ºä¾‹"""
    # ä½¿ç”¨é»˜è®¤é…ç½®åˆ›å»º Pipelineï¼ˆæ— éœ€æä¾› API å¯†é’¥ï¼‰
    pipeline = IntegratedPipeline(
        output_dir="./example_output"
    )
    
    # å¤„ç†å•ä¸ªæ–‡ä»¶
    # results = pipeline.process_single_file("example.pdf")
    
    # æˆ–å¤„ç†æ•´ä¸ªç›®å½•
    # results = pipeline.process_directory("./documents/")

def quick_start():
    """å¿«é€Ÿå¼€å§‹ç¤ºä¾‹"""
    logger.info("ğŸš€ å¿«é€Ÿå¼€å§‹ç¤ºä¾‹")
    logger.info("=" * 50)
    
    # ä½¿ç”¨é»˜è®¤é…ç½®
    pipeline = IntegratedPipeline()
    
    logger.info("âœ… Pipeline åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨é»˜è®¤é…ç½®:")
    logger.info(f"   - æ¨¡å‹: {DEFAULT_CONFIG['llm_model']}")
    logger.info(f"   - API URL: {DEFAULT_CONFIG['BASE_URL']}")
    logger.info("ç°åœ¨å¯ä»¥å¼€å§‹å¤„ç†æ–‡æ¡£äº†ï¼")

if __name__ == "__main__":
    add_cli()