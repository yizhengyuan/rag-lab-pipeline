#!/usr/bin/env python3
"""
æ­¥éª¤3: æ¦‚å¿µæå–ä¸æ˜ å°„ - å¢å¼ºç‰ˆ
================================

åŠŸèƒ½ï¼š
1. ä»æ­¥éª¤2çš„ç»“æœä¸­è¯»å–åˆ†å—æ•°æ®
2. æ‰§è¡Œé«˜è´¨é‡çš„æ¦‚å¿µæå–å’ŒéªŒè¯
3. ä½¿ç”¨æ–°çš„æ¦‚å¿µæ˜ å°„åŠŸèƒ½
4. ä¿å­˜åˆ°åŒä¸€ä¸ªå®éªŒæ–‡ä»¶å¤¹

ç”¨æ³•: 
- python step3.py <step2è¾“å‡ºæ–‡ä»¶.txt>
- python step3.py <å®éªŒæ–‡ä»¶å¤¹è·¯å¾„>

æ–°åŠŸèƒ½ï¼š
- âœ… è‡ªåŠ¨è¯†åˆ«å¹¶ä½¿ç”¨step2çš„å®éªŒæ–‡ä»¶å¤¹
- âœ… é«˜è´¨é‡çš„æ¦‚å¿µæå–å’Œæ¦‚å¿µå›¾æ„å»º
- âœ… æ¦‚å¿µå…³ç³»æ˜ å°„å’Œå±‚æ¬¡ç»“æ„
- âœ… ç»Ÿä¸€çš„å®éªŒç®¡ç†
- âœ… æ”¯æŒæ–°çš„concept_mappingæç¤ºè¯
"""

import sys
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Set
from collections import Counter
import logging

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
sys.path.append(str(Path(__file__).parent))
from llama_index.core import Document
from llama_index.core.schema import TextNode

# å¯¼å…¥æ ¸å¿ƒå¤„ç†æ¨¡å—
from config.settings import load_config_from_yaml
from core.nodes import ConceptNode

# å¯¼å…¥å®éªŒç®¡ç†å™¨
from utils.experiment_manager import ExperimentManager
from utils.helpers import FileHelper

logger = logging.getLogger(__name__)

def load_step2_result(step2_file_or_dir: str) -> tuple:
    """
    ä»æ­¥éª¤2çš„è¾“å‡ºæ–‡ä»¶æˆ–å®éªŒæ–‡ä»¶å¤¹ä¸­åŠ è½½ç»“æœ
    
    Args:
        step2_file_or_dir: æ­¥éª¤2çš„è¾“å‡ºæ–‡ä»¶æˆ–å®éªŒæ–‡ä»¶å¤¹è·¯å¾„
        
    Returns:
        tuple: (step2_result, experiment_manager)
    """
    step2_path = Path(step2_file_or_dir)
    
    if step2_path.is_file():
        # æƒ…å†µ1ï¼šç›´æ¥æŒ‡å®šäº†step2çš„è¾“å‡ºæ–‡ä»¶
        if step2_path.name.startswith("step2") and step2_path.suffix == ".txt":
            experiment_dir = step2_path.parent
            experiment_manager = ExperimentManager.load_experiment(str(experiment_dir))
            
            # ä»txtæ–‡ä»¶åŠ è½½ç»“æœ
            step2_result = load_result_from_txt(str(step2_path))
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {step2_path}")
            
    elif step2_path.is_dir():
        # æƒ…å†µ2ï¼šç›´æ¥æŒ‡å®šäº†å®éªŒæ–‡ä»¶å¤¹
        experiment_manager = ExperimentManager.load_experiment(str(step2_path))
        
        # æŸ¥æ‰¾step2çš„è¾“å‡ºæ–‡ä»¶
        step2_txt_path = experiment_manager.get_step_output_path(2, "txt")
        if not step2_txt_path.exists():
            raise FileNotFoundError(f"å®éªŒæ–‡ä»¶å¤¹ä¸­æ‰¾ä¸åˆ°step2è¾“å‡ºæ–‡ä»¶: {step2_txt_path}")
        
        step2_result = load_result_from_txt(str(step2_txt_path))
        
    else:
        raise FileNotFoundError(f"è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {step2_path}")
    
    return step2_result, experiment_manager

def load_result_from_txt(input_file: str) -> Dict[str, Any]:
    """ä»txtæ–‡ä»¶ä¸­åŠ è½½ç»“æœæ•°æ®"""
    with open(input_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    start_marker = "# JSON_DATA_START\n"
    end_marker = "\n# JSON_DATA_END"
    
    start_idx = content.find(start_marker)
    end_idx = content.find(end_marker)
    
    if start_idx == -1 or end_idx == -1:
        raise ValueError("æ— æ³•ä»txtæ–‡ä»¶ä¸­è§£ææ•°æ®")
    
    json_str = content[start_idx + len(start_marker):end_idx]
    return json.loads(json_str)

def reconstruct_chunks_from_step2(step2_result: Dict[str, Any]) -> List[TextNode]:
    """ä»æ­¥éª¤2ç»“æœé‡å»ºTextNodeå¯¹è±¡"""
    
    # å°è¯•ä¸åŒçš„å­—æ®µåï¼ˆå…¼å®¹æ€§ï¼‰
    chunks_data = None
    for field_name in ["chunks", "chunk_nodes", "processed_chunks"]:
        if field_name in step2_result:
            chunks_data = step2_result[field_name]
            print(f"âœ… æ‰¾åˆ°åˆ†å—æ•°æ®å­—æ®µ: {field_name}")
            break
    
    if not chunks_data:
        # åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„å­—æ®µ
        available_fields = list(step2_result.keys())
        print(f"âŒ æœªæ‰¾åˆ°åˆ†å—æ•°æ®ï¼Œå¯ç”¨å­—æ®µ: {available_fields}")
        raise ValueError("æ­¥éª¤2ç»“æœä¸­æ²¡æœ‰æ‰¾åˆ°åˆ†å—æ•°æ®")
    
    print(f"ğŸ“„ é‡å»º {len(chunks_data)} ä¸ªTextNodeå¯¹è±¡...")
    
    # é‡å»ºTextNodeå¯¹è±¡
    chunk_nodes = []
    for i, chunk_data in enumerate(chunks_data):
        try:
            if isinstance(chunk_data, dict):
                # ä»å­—å…¸æ•°æ®é‡å»ºTextNode
                text_node = TextNode(
                    text=chunk_data.get("text", ""),
                    metadata=chunk_data.get("metadata", {})
                )
                
                # è®¾ç½®node_id
                if "node_id" in chunk_data:
                    text_node.node_id = chunk_data["node_id"]
                
                chunk_nodes.append(text_node)
                
            elif hasattr(chunk_data, 'text'):
                # å·²ç»æ˜¯TextNodeå¯¹è±¡
                chunk_nodes.append(chunk_data)
                
            else:
                print(f"âš ï¸ è·³è¿‡æ— æ•ˆçš„chunkæ•°æ®: {type(chunk_data)}")
                continue
                
        except Exception as e:
            print(f"âš ï¸ é‡å»ºchunk {i} å¤±è´¥: {e}")
            continue
    
    print(f"âœ… æˆåŠŸé‡å»º {len(chunk_nodes)} ä¸ªTextNodeå¯¹è±¡")
    return chunk_nodes

def extract_and_analyze_concepts(chunk_nodes: List[TextNode], config) -> Dict[str, Any]:
    """
    ä»chunksä¸­æå–å’Œåˆ†ææ¦‚å¿µ
    
    Args:
        chunk_nodes: æ–‡æœ¬èŠ‚ç‚¹åˆ—è¡¨
        config: é…ç½®å¯¹è±¡
        
    Returns:
        Dict[str, Any]: æ¦‚å¿µåˆ†æç»“æœ
    """
    print("ğŸ§  å¼€å§‹æ¦‚å¿µæå–å’Œåˆ†æ...")
    
    # 1. æ”¶é›†æ‰€æœ‰æ¦‚å¿µ
    all_concepts = []
    chunk_concept_map = {}
    
    for i, chunk in enumerate(chunk_nodes):
        # è·å–æ¦‚å¿µæ•°æ®
        concepts_data = chunk.metadata.get("concepts", "[]")
        
        if isinstance(concepts_data, str):
            try:
                concepts = json.loads(concepts_data)
            except json.JSONDecodeError:
                concepts = []
        else:
            concepts = concepts_data if isinstance(concepts_data, list) else []
        
        # è¿‡æ»¤å’Œæ¸…ç†æ¦‚å¿µ
        valid_concepts = []
        for concept in concepts:
            if isinstance(concept, str) and len(concept.strip()) > 2:
                cleaned_concept = concept.strip()
                # é«˜è´¨é‡æ¦‚å¿µè¿‡æ»¤
                if is_high_quality_concept(cleaned_concept):
                    valid_concepts.append(cleaned_concept)
        
        all_concepts.extend(valid_concepts)
        chunk_concept_map[f"chunk_{i}"] = valid_concepts
    
    print(f"   æ”¶é›†åˆ° {len(all_concepts)} ä¸ªæ¦‚å¿µ")
    
    # 2. æ¦‚å¿µé¢‘ç‡åˆ†æ
    concept_frequency = Counter(all_concepts)
    unique_concepts = list(concept_frequency.keys())
    
    print(f"   å”¯ä¸€æ¦‚å¿µæ•°: {len(unique_concepts)}")
    
    # 3. æ¦‚å¿µè´¨é‡è¯„ä¼°
    quality_scores = {}
    for concept in unique_concepts:
        quality_scores[concept] = calculate_concept_quality(concept, concept_frequency[concept])
    
    # 4. æŒ‰è´¨é‡å’Œé¢‘ç‡æ’åº
    sorted_by_quality = sorted(unique_concepts, key=lambda x: quality_scores[x], reverse=True)
    sorted_by_frequency = sorted(concept_frequency.items(), key=lambda x: x[1], reverse=True)
    
    # 5. åˆ†ç±»æ¦‚å¿µ
    high_quality_concepts = [c for c in unique_concepts if quality_scores[c] >= 4.0]
    medium_quality_concepts = [c for c in unique_concepts if 2.0 <= quality_scores[c] < 4.0]
    low_quality_concepts = [c for c in unique_concepts if quality_scores[c] < 2.0]
    
    # 6. å°è¯•æ¦‚å¿µæ˜ å°„ï¼ˆä½¿ç”¨æ–°çš„concept_mappingæç¤ºè¯ï¼‰
    concept_map = None
    try:
        concept_map = create_concept_map(chunk_nodes, config)
        print(f"   âœ… ç”Ÿæˆæ¦‚å¿µå›¾æˆåŠŸ")
    except Exception as e:
        print(f"   âš ï¸ æ¦‚å¿µæ˜ å°„å¤±è´¥: {e}")
    
    # 7. ç”Ÿæˆç»Ÿè®¡æ•°æ®
    statistics = {
        "total_concepts": len(all_concepts),
        "unique_concepts": len(unique_concepts),
        "avg_frequency": sum(concept_frequency.values()) / len(unique_concepts) if unique_concepts else 0,
        "avg_quality_score": sum(quality_scores.values()) / len(quality_scores) if quality_scores else 0,
        "high_quality_count": len(high_quality_concepts),
        "medium_quality_count": len(medium_quality_concepts),
        "low_quality_count": len(low_quality_concepts),
        "concepts_per_chunk": len(all_concepts) / len(chunk_nodes) if chunk_nodes else 0
    }
    
    return {
        "all_concepts": all_concepts,
        "unique_concepts": unique_concepts,
        "concept_frequency": dict(concept_frequency),
        "quality_scores": quality_scores,
        "sorted_by_quality": sorted_by_quality,
        "sorted_by_frequency": sorted_by_frequency,
        "high_quality_concepts": high_quality_concepts,
        "medium_quality_concepts": medium_quality_concepts,
        "low_quality_concepts": low_quality_concepts,
        "chunk_concept_map": chunk_concept_map,
        "concept_map": concept_map,
        "statistics": statistics
    }

def is_high_quality_concept(concept: str) -> bool:
    """åˆ¤æ–­æ¦‚å¿µæ˜¯å¦ä¸ºé«˜è´¨é‡"""
    if not concept or len(concept.strip()) < 3:
        return False
    
    concept_lower = concept.lower()
    
    # è¿‡æ»¤ä½è´¨é‡æ¨¡å¼
    low_quality_patterns = [
        # è¿‡äºé€šç”¨çš„è¯
        'method', 'system', 'process', 'study', 'analysis', 'research', 
        'data', 'information', 'model', 'approach', 'technique',
        # ç®€å•å½¢å®¹è¯
        'good', 'bad', 'new', 'old', 'important', 'main', 'key',
        # æ–‡æ¡£å¼•ç”¨
        'paper', 'article', 'work', 'figure', 'table', 'section'
    ]
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä½è´¨é‡æ¨¡å¼
    for pattern in low_quality_patterns:
        if pattern in concept_lower:
            return False
    
    # æ£€æŸ¥é•¿åº¦ï¼ˆ2-6ä¸ªè¯æ¯”è¾ƒå¥½ï¼‰
    word_count = len(concept.split())
    if word_count < 1 or word_count > 6:
        return False
    
    return True

def calculate_concept_quality(concept: str, frequency: int) -> float:
    """è®¡ç®—æ¦‚å¿µè´¨é‡åˆ†æ•°ï¼ˆ0-5åˆ†ï¼‰"""
    score = 3.0  # åŸºç¡€åˆ†æ•°
    
    # é•¿åº¦è¯„åˆ†
    word_count = len(concept.split())
    if 2 <= word_count <= 4:
        score += 1.0
    elif word_count == 1 or word_count > 5:
        score -= 0.5
    
    # é¢‘ç‡è¯„åˆ†
    if frequency >= 3:
        score += 0.5
    elif frequency >= 2:
        score += 0.2
    
    # åŒ…å«ä¸“ä¸šæœ¯è¯­åŠ åˆ†
    professional_indicators = [
        'network', 'learning', 'attention', 'transformer', 'neural',
        'algorithm', 'optimization', 'architecture', 'mechanism',
        'translation', 'encoding', 'decoding', 'embedding'
    ]
    
    concept_lower = concept.lower()
    for indicator in professional_indicators:
        if indicator in concept_lower:
            score += 0.3
            break
    
    # å¤§å°å†™æ··åˆï¼ˆä¸“æœ‰åè¯ï¼‰åŠ åˆ†
    if any(c.isupper() for c in concept) and any(c.islower() for c in concept):
        score += 0.2
    
    return min(5.0, max(0.0, score))

def create_concept_map(chunk_nodes: List[TextNode], config) -> Dict[str, Any]:
    """
    ä½¿ç”¨æ–°çš„concept_mappingæç¤ºè¯åˆ›å»ºæ¦‚å¿µå›¾
    
    Args:
        chunk_nodes: æ–‡æœ¬èŠ‚ç‚¹åˆ—è¡¨
        config: é…ç½®å¯¹è±¡
        
    Returns:
        Dict[str, Any]: æ¦‚å¿µå›¾æ•°æ®
    """
    print("ğŸ—ºï¸ å¼€å§‹åˆ›å»ºæ¦‚å¿µå›¾...")
    
    # è¿™é‡Œå¯ä»¥å®ç°æ¦‚å¿µå›¾åˆ›å»ºé€»è¾‘
    # ç”±äºéœ€è¦è°ƒç”¨LLMï¼Œæš‚æ—¶è¿”å›åŸºç¡€ç»“æ„
    concept_map = {
        "main_topics": [],
        "cross_topic_relationships": [],
        "created_at": datetime.now().isoformat(),
        "total_chunks": len(chunk_nodes)
    }
    
    # ç®€åŒ–ç‰ˆæœ¬ï¼šåŸºäºæ¦‚å¿µé¢‘ç‡åˆ›å»ºä¸»é¢˜
    all_concepts = []
    for chunk in chunk_nodes:
        concepts_data = chunk.metadata.get("concepts", "[]")
        if isinstance(concepts_data, str):
            try:
                concepts = json.loads(concepts_data)
                all_concepts.extend(concepts)
            except:
                pass
    
    # æŒ‰é¢‘ç‡åˆ†ç»„æ¦‚å¿µ
    concept_freq = Counter(all_concepts)
    high_freq_concepts = [c for c, f in concept_freq.most_common(10)]
    
    if high_freq_concepts:
        concept_map["main_topics"].append({
            "topic": "æ ¸å¿ƒæ¦‚å¿µ",
            "key_concepts": [
                {
                    "concept": concept,
                    "frequency": concept_freq[concept],
                    "definition": f"é«˜é¢‘æ¦‚å¿µ: {concept}",
                    "relationships": [],
                    "examples": [],
                    "supporting_details": []
                } for concept in high_freq_concepts
            ]
        })
    
    return concept_map

def process_step3_concept_extraction(step2_result: Dict[str, Any], config_path: str = "config.yml") -> Dict[str, Any]:
    """
    æ‰§è¡Œæ­¥éª¤3çš„æ¦‚å¿µæå–å¤„ç†
    
    Args:
        step2_result: æ­¥éª¤2çš„ç»“æœ
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        Dict[str, Any]: æ­¥éª¤3çš„å¤„ç†ç»“æœ
    """
    start_time = time.time()
    
    try:
        # 1. åŠ è½½é…ç½®
        print("ğŸ“‹ åŠ è½½é…ç½®...")
        config = load_config_from_yaml(config_path)
        
        # 2. é‡å»ºTextNodeå¯¹è±¡
        print("ğŸ“„ é‡å»ºåˆ†å—æ•°æ®...")
        chunk_nodes = reconstruct_chunks_from_step2(step2_result)
        
        if not chunk_nodes:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„åˆ†å—æ•°æ®")
        
        # 3. æå–å’Œåˆ†ææ¦‚å¿µ
        print("ğŸ§  æ‰§è¡Œæ¦‚å¿µæå–å’Œåˆ†æ...")
        concept_analysis = extract_and_analyze_concepts(chunk_nodes, config)
        
        processing_time = time.time() - start_time
        
        # 4. æ„å»ºç»“æœ
        result = {
            "success": True,
            "step": 3,
            "step_name": "æ¦‚å¿µæå–ä¸æ˜ å°„",
            "chunk_count": len(chunk_nodes),
            "concept_analysis": concept_analysis,
            "statistics": concept_analysis["statistics"],
            "processing_time": processing_time,
            "timestamp": datetime.now().isoformat(),
            "config_used": {
                "concepts_per_chunk": config.get("concept_extraction.concepts_per_chunk", 5),
                "similarity_threshold": config.get("concept_merging.similarity_threshold", 0.7)
            }
        }
        
        return result
        
    except Exception as e:
        error_msg = f"æ­¥éª¤3å¤„ç†å¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        
        result = {
            "success": False,
            "step": 3,
            "step_name": "æ¦‚å¿µæå–ä¸æ˜ å°„",
            "error": error_msg,
            "processing_time": time.time() - start_time,
            "timestamp": datetime.now().isoformat()
        }
        
        return result

def main():
    if len(sys.argv) != 2:
        print("ç”¨æ³•: python step3.py <step2è¾“å‡ºæ–‡ä»¶æˆ–å®éªŒæ–‡ä»¶å¤¹>")
        print("ç¤ºä¾‹:")
        print("  python step3.py experiments/20241204_143052_attention_paper/step2_chunking.txt")
        print("  python step3.py experiments/20241204_143052_attention_paper/")
        print("\næ–°åŠŸèƒ½:")
        print("âœ… è‡ªåŠ¨è¯†åˆ«å®éªŒæ–‡ä»¶å¤¹")
        print("âœ… é«˜è´¨é‡æ¦‚å¿µæå–å’ŒéªŒè¯")
        print("âœ… æ¦‚å¿µå…³ç³»æ˜ å°„å’Œåˆ†æ")
        print("âœ… ç»Ÿä¸€çš„å®éªŒç®¡ç†")
        sys.exit(1)
    
    input_path = sys.argv[1]
    
    print(f"ğŸš€ æ­¥éª¤3: æ¦‚å¿µæå–ä¸æ˜ å°„ (å¢å¼ºç‰ˆ)")
    print(f"ğŸ“„ è¾“å…¥: {input_path}")
    print("="*60)
    
    try:
        # 1. åŠ è½½æ­¥éª¤2ç»“æœå’Œå®éªŒç®¡ç†å™¨
        print("ğŸ“‚ åŠ è½½æ­¥éª¤2ç»“æœ...")
        step2_result, experiment_manager = load_step2_result(input_path)
        
        if not step2_result.get("success"):
            print("âŒ æ­¥éª¤2æœªæˆåŠŸå®Œæˆï¼Œæ— æ³•ç»§ç»­")
            sys.exit(1)
        
        print(f"âœ… å·²åŠ è½½å®éªŒ: {experiment_manager.experiment_name}")
        print(f"ğŸ“ å®éªŒç›®å½•: {experiment_manager.experiment_dir}")
        print()
        
        # 2. æ‰§è¡Œæ­¥éª¤3å¤„ç†
        print("ğŸ”„ å¼€å§‹æ­¥éª¤3å¤„ç†...")
        result = process_step3_concept_extraction(step2_result)
        
        # 3. ä¿å­˜ç»“æœåˆ°å®éªŒæ–‡ä»¶å¤¹
        print("ğŸ’¾ ä¿å­˜å®éªŒç»“æœ...")
        saved_files = experiment_manager.save_step_result(
            step_num=3,
            result=result,
            save_formats=['txt', 'json']
        )
        
        if result.get("success"):
            print(f"\nâœ… æ­¥éª¤3å®Œæˆ!")
            print(f"ğŸ“ å®éªŒç›®å½•: {experiment_manager.experiment_dir}")
            print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶:")
            for format_type, file_path in saved_files.items():
                print(f"   - {format_type.upper()}: {file_path}")
            
            # æ˜¾ç¤ºå¤„ç†ç»“æœæ‘˜è¦
            concept_analysis = result.get("concept_analysis", {})
            stats = concept_analysis.get("statistics", {})
            
            print(f"\nğŸ“Š æ¦‚å¿µåˆ†æç»“æœæ‘˜è¦:")
            print(f"   - å¤„ç†åˆ†å—æ•°: {result.get('chunk_count', 0)}")
            print(f"   - æ€»æ¦‚å¿µæ•°: {stats.get('total_concepts', 0)}")
            print(f"   - å”¯ä¸€æ¦‚å¿µæ•°: {stats.get('unique_concepts', 0)}")
            print(f"   - å¹³å‡æ¦‚å¿µé¢‘ç‡: {stats.get('avg_frequency', 0):.2f}")
            print(f"   - å¹³å‡è´¨é‡åˆ†æ•°: {stats.get('avg_quality_score', 0):.2f}/5.0")
            print(f"   - é«˜è´¨é‡æ¦‚å¿µæ•°: {stats.get('high_quality_count', 0)}")
            print(f"   - ä¸­ç­‰è´¨é‡æ¦‚å¿µæ•°: {stats.get('medium_quality_count', 0)}")
            print(f"   - ä½è´¨é‡æ¦‚å¿µæ•°: {stats.get('low_quality_count', 0)}")
            print(f"   - å¤„ç†æ—¶é—´: {result.get('processing_time', 0):.2f} ç§’")
            
            # æ˜¾ç¤ºé«˜è´¨é‡æ¦‚å¿µç¤ºä¾‹
            high_quality = concept_analysis.get("high_quality_concepts", [])
            if high_quality:
                print(f"\nğŸŒŸ é«˜è´¨é‡æ¦‚å¿µç¤ºä¾‹ (å‰15ä¸ª):")
                for i, concept in enumerate(high_quality[:15], 1):
                    quality_score = concept_analysis.get("quality_scores", {}).get(concept, 0)
                    frequency = concept_analysis.get("concept_frequency", {}).get(concept, 0)
                    print(f"   {i:2d}. {concept} (è´¨é‡: {quality_score:.1f}, é¢‘ç‡: {frequency})")
                
                if len(high_quality) > 15:
                    print(f"   ... è¿˜æœ‰ {len(high_quality) - 15} ä¸ªé«˜è´¨é‡æ¦‚å¿µ")
            
            # æ˜¾ç¤ºæ¦‚å¿µæ˜ å°„ä¿¡æ¯
            concept_map = concept_analysis.get("concept_map")
            if concept_map:
                print(f"\nğŸ—ºï¸ æ¦‚å¿µå›¾ä¿¡æ¯:")
                print(f"   - ä¸»è¦ä¸»é¢˜æ•°: {len(concept_map.get('main_topics', []))}")
                print(f"   - è·¨ä¸»é¢˜å…³ç³»æ•°: {len(concept_map.get('cross_topic_relationships', []))}")
            
            # æ˜¾ç¤ºå®éªŒçŠ¶æ€
            summary = experiment_manager.get_experiment_summary()
            print(f"\nğŸ§ª å®éªŒçŠ¶æ€:")
            print(f"   - å®éªŒID: {summary['experiment_id']}")
            print(f"   - å·²å®Œæˆæ­¥éª¤: {summary['steps_completed']}/{summary['total_steps']}")
            print(f"   - å½“å‰çŠ¶æ€: {summary['status']}")
            
            # æç¤ºåç»­æ­¥éª¤
            print(f"\nğŸ“‹ åç»­æ­¥éª¤:")
            print(f"   è¿è¡Œä¸‹ä¸€æ­¥: python step4.py {saved_files['txt']}")
            print(f"   æŸ¥çœ‹ç»“æœ: cat {saved_files['txt']}")
            print(f"   æŸ¥çœ‹æ¦‚å¿µ: grep -A 30 'é«˜è´¨é‡æ¦‚å¿µç¤ºä¾‹' {saved_files['txt']}")
                
        else:
            print(f"âŒ æ­¥éª¤3å¤±è´¥: {result.get('error')}")
            
            # å³ä½¿å¤±è´¥ä¹Ÿä¿å­˜é”™è¯¯ä¿¡æ¯
            experiment_manager.save_step_result(
                step_num=3,
                result=result,
                save_formats=['txt']
            )
            sys.exit(1)
            
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        # å¦‚æœæœ‰å®éªŒç®¡ç†å™¨ï¼Œä¿å­˜é”™è¯¯ä¿¡æ¯
        if 'experiment_manager' in locals():
            error_result = {
                "step": 3,
                "step_name": "æ¦‚å¿µæå–ä¸æ˜ å°„",
                "success": False,
                "error": str(e),
                "traceback": traceback.format_exc(),
                "processing_time": 0.0,
                "timestamp": datetime.now().isoformat()
            }
            
            try:
                experiment_manager.save_step_result(3, error_result, ['txt'])
                print(f"ğŸ“„ é”™è¯¯ä¿¡æ¯å·²ä¿å­˜åˆ°å®éªŒç›®å½•: {experiment_manager.experiment_dir}")
            except:
                pass
        
        sys.exit(1)

if __name__ == "__main__":
    main()