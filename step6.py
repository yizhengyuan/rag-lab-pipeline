#!/usr/bin/env python3
"""
æ­¥éª¤6: è¯æ®æå–ä¸è´¨é‡è¯„ä¼° - å¢å¼ºç‰ˆ
=====================================

åŠŸèƒ½ï¼š
1. ä»æ­¥éª¤5çš„æ£€ç´¢ç»“æœä¸­è·å–æ¦‚å¿µ-åˆ†å—æ˜ å°„
2. æ™ºèƒ½æå–æ¯ä¸ªæ¦‚å¿µçš„æ”¯æŒè¯æ®
3. å¤šç»´åº¦è¯æ®è´¨é‡è¯„ä¼°å’Œåˆ†ç±»
4. ä¿å­˜åˆ°åŒä¸€ä¸ªå®éªŒæ–‡ä»¶å¤¹

ç”¨æ³•: 
- python step6.py <step5è¾“å‡ºæ–‡ä»¶.txt>
- python step6.py <å®éªŒæ–‡ä»¶å¤¹è·¯å¾„>

æ–°åŠŸèƒ½ï¼š
- âœ… è‡ªåŠ¨è¯†åˆ«å¹¶ä½¿ç”¨step5çš„å®éªŒæ–‡ä»¶å¤¹
- âœ… æ™ºèƒ½è¯æ®æå–å’Œç›¸å…³æ€§è®¡ç®—
- âœ… å¤šç±»å‹è¯æ®è¯†åˆ«ï¼ˆå®šä¹‰ã€ä¾‹å­ã€è§£é‡Šç­‰ï¼‰
- âœ… è¯æ®è´¨é‡è¯„åˆ†å’Œè¿‡æ»¤
- âœ… ç»Ÿä¸€çš„å®éªŒç®¡ç†
"""

import sys
import json
import time
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Set, Tuple
from collections import defaultdict
import logging

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
sys.path.append(str(Path(__file__).parent))
from llama_index.core import Document
from llama_index.core.schema import TextNode

# å¯¼å…¥æ ¸å¿ƒå¤„ç†æ¨¡å—
from config.settings import load_config_from_yaml
from core.nodes import EvidenceNode

# å¯¼å…¥å®éªŒç®¡ç†å™¨
from utils.experiment_manager import ExperimentManager
from utils.helpers import FileHelper

logger = logging.getLogger(__name__)

def load_step5_result(step5_file_or_dir: str) -> tuple:
    """
    ä»æ­¥éª¤5çš„è¾“å‡ºæ–‡ä»¶æˆ–å®éªŒæ–‡ä»¶å¤¹ä¸­åŠ è½½ç»“æœ
    
    Args:
        step5_file_or_dir: æ­¥éª¤5çš„è¾“å‡ºæ–‡ä»¶æˆ–å®éªŒæ–‡ä»¶å¤¹è·¯å¾„
        
    Returns:
        tuple: (step5_result, experiment_manager)
    """
    step5_path = Path(step5_file_or_dir)
    
    if step5_path.is_file():
        # æƒ…å†µ1ï¼šç›´æ¥æŒ‡å®šäº†step5çš„è¾“å‡ºæ–‡ä»¶
        if step5_path.name.startswith("step5") and step5_path.suffix == ".txt":
            experiment_dir = step5_path.parent
            experiment_manager = ExperimentManager.load_experiment(str(experiment_dir))
            
            # ä»txtæ–‡ä»¶åŠ è½½ç»“æœ
            step5_result = load_result_from_txt(str(step5_path))
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {step5_path}")
            
    elif step5_path.is_dir():
        # æƒ…å†µ2ï¼šç›´æ¥æŒ‡å®šäº†å®éªŒæ–‡ä»¶å¤¹
        experiment_manager = ExperimentManager.load_experiment(str(step5_path))
        
        # æŸ¥æ‰¾step5çš„è¾“å‡ºæ–‡ä»¶
        step5_txt_path = experiment_manager.get_step_output_path(5, "txt")
        if not step5_txt_path.exists():
            raise FileNotFoundError(f"å®éªŒæ–‡ä»¶å¤¹ä¸­æ‰¾ä¸åˆ°step5è¾“å‡ºæ–‡ä»¶: {step5_txt_path}")
        
        step5_result = load_result_from_txt(str(step5_txt_path))
        
    else:
        raise FileNotFoundError(f"è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {step5_path}")
    
    return step5_result, experiment_manager

def load_result_from_txt(input_file: str) -> Dict[str, Any]:
    """ä»txtæ–‡ä»¶ä¸­åŠ è½½ç»“æœæ•°æ®"""
    with open(input_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    start_marker = "# JSON_DATA_START\n"
    end_marker = "\n# JSON_DATA_END"
    
    start_idx = content.find(start_marker)
    end_idx = content.find(end_marker)
    
    if start_idx == -1 or end_idx == -1:
        raise ValueError("æ— æ³•ä»txtæ–‡ä»¶ä¸­è§£ææ•°æ®")
    
    json_str = content[start_idx + len(start_marker):end_idx]
    return json.loads(json_str)

def load_previous_steps_data(experiment_manager: ExperimentManager) -> Dict[str, Any]:
    """
    åŠ è½½ä¹‹å‰æ­¥éª¤çš„æ•°æ®
    
    Args:
        experiment_manager: å®éªŒç®¡ç†å™¨
        
    Returns:
        Dict[str, Any]: åŒ…å«ä¹‹å‰æ­¥éª¤æ•°æ®çš„å­—å…¸
    """
    previous_data = {}
    
    # åŠ è½½æ­¥éª¤2çš„æ•°æ®ï¼ˆéœ€è¦åˆ†å—æ–‡æœ¬ï¼‰
    step2_path = experiment_manager.get_step_output_path(2, "txt")
    if step2_path.exists():
        try:
            step2_result = load_result_from_txt(str(step2_path))
            previous_data["step2"] = step2_result
            print(f"âœ… åŠ è½½æ­¥éª¤2æ•°æ®: {step2_path}")
        except Exception as e:
            print(f"âš ï¸ åŠ è½½æ­¥éª¤2æ•°æ®å¤±è´¥: {e}")
    
    return previous_data

def extract_retrieval_results_from_step5(step5_result: Dict[str, Any]) -> Dict[str, Any]:
    """
    ä»æ­¥éª¤5ç»“æœä¸­æå–æ£€ç´¢æ•°æ®
    
    Args:
        step5_result: æ­¥éª¤5çš„ç»“æœ
        
    Returns:
        Dict[str, Any]: æ£€ç´¢ç»“æœæ•°æ®
    """
    print("ğŸ“Š ä»æ­¥éª¤5ç»“æœä¸­æå–æ£€ç´¢æ•°æ®...")
    
    retrieval_results = step5_result.get("retrieval_results", {})
    
    if not retrieval_results:
        raise ValueError("æ­¥éª¤5ç»“æœä¸­æ²¡æœ‰æ‰¾åˆ°æ£€ç´¢æ•°æ®")
    
    print(f"   - æ£€ç´¢ç»“æœæ•°: {len(retrieval_results)}")
    
    # ç»Ÿè®¡æ£€ç´¢è¦†ç›–æƒ…å†µ
    concepts_with_chunks = 0
    total_chunk_mappings = 0
    
    for concept_id, result in retrieval_results.items():
        retrieved_chunks = result.get("retrieved_chunks", [])
        if retrieved_chunks:
            concepts_with_chunks += 1
            total_chunk_mappings += len(retrieved_chunks)
    
    print(f"   - æœ‰æ£€ç´¢ç»“æœçš„æ¦‚å¿µ: {concepts_with_chunks}")
    print(f"   - æ€»æ¦‚å¿µ-åˆ†å—æ˜ å°„æ•°: {total_chunk_mappings}")
    
    return retrieval_results

def get_chunk_text_mapping(step2_result: Dict[str, Any]) -> Dict[str, str]:
    """
    åˆ›å»ºåˆ†å—IDåˆ°æ–‡æœ¬çš„æ˜ å°„
    
    Args:
        step2_result: æ­¥éª¤2çš„ç»“æœ
        
    Returns:
        Dict[str, str]: åˆ†å—IDåˆ°æ–‡æœ¬çš„æ˜ å°„
    """
    print("ğŸ—‚ï¸ åˆ›å»ºåˆ†å—æ–‡æœ¬æ˜ å°„...")
    
    # å°è¯•ä¸åŒçš„å­—æ®µå
    chunks_data = None
    for field_name in ["chunks", "chunk_nodes", "processed_chunks"]:
        if field_name in step2_result:
            chunks_data = step2_result[field_name]
            break
    
    if not chunks_data:
        raise ValueError("æ­¥éª¤2ç»“æœä¸­æ²¡æœ‰æ‰¾åˆ°åˆ†å—æ•°æ®")
    
    chunk_text_mapping = {}
    
    for chunk_data in chunks_data:
        if isinstance(chunk_data, dict):
            chunk_id = chunk_data.get("metadata", {}).get("chunk_id") or chunk_data.get("node_id", "unknown")
            chunk_text = chunk_data.get("text", "")
            chunk_text_mapping[chunk_id] = chunk_text
        else:
            print(f"âš ï¸ è·³è¿‡æ— æ•ˆçš„chunkæ•°æ®: {type(chunk_data)}")
    
    print(f"   âœ… åˆ›å»ºäº† {len(chunk_text_mapping)} ä¸ªåˆ†å—æ–‡æœ¬æ˜ å°„")
    return chunk_text_mapping

def classify_evidence_type(evidence_text: str, concept_text: str) -> str:
    """
    åˆ†ç±»è¯æ®ç±»å‹
    
    Args:
        evidence_text: è¯æ®æ–‡æœ¬
        concept_text: æ¦‚å¿µæ–‡æœ¬
        
    Returns:
        str: è¯æ®ç±»å‹
    """
    evidence_lower = evidence_text.lower()
    concept_lower = concept_text.lower()
    
    # å®šä¹‰ç±»å‹æ¨¡å¼
    definition_patterns = [
        r'\bis\s+(?:a|an|the)?\s*\w+',
        r'\bdefine[ds]?\s+as',
        r'\brefer[s]?\s+to',
        r'\bmean[s]?\s+that',
        r'\bconsist[s]?\s+of',
        r'\btype\s+of',
        r'\bkind\s+of'
    ]
    
    example_patterns = [
        r'\bfor\s+example',
        r'\bsuch\s+as',
        r'\bincluding',
        r'\be\.g\.',
        r'\bnamely',
        r'\bspecifically',
        r'\bin\s+particular'
    ]
    
    explanation_patterns = [
        r'\bbecause',
        r'\bsince',
        r'\btherefore',
        r'\bthus',
        r'\bhence',
        r'\bas\s+a\s+result',
        r'\bdue\s+to',
        r'\bowing\s+to'
    ]
    
    procedure_patterns = [
        r'\bstep[s]?',
        r'\bprocess',
        r'\bmethod',
        r'\bprocedure',
        r'\balgorithm',
        r'\btechnique',
        r'\bapproach'
    ]
    
    # æ£€æŸ¥æ¨¡å¼åŒ¹é…
    for pattern in definition_patterns:
        if re.search(pattern, evidence_lower):
            return "definition"
    
    for pattern in example_patterns:
        if re.search(pattern, evidence_lower):
            return "example"
    
    for pattern in explanation_patterns:
        if re.search(pattern, evidence_lower):
            return "explanation"
    
    for pattern in procedure_patterns:
        if re.search(pattern, evidence_lower):
            return "procedure"
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¦‚å¿µåç§°
    if concept_lower in evidence_lower:
        return "reference"
    
    return "general"

def calculate_evidence_relevance(evidence_text: str, 
                               concept_text: str, 
                               similarity_score: float) -> float:
    """
    è®¡ç®—è¯æ®ç›¸å…³æ€§åˆ†æ•°
    
    Args:
        evidence_text: è¯æ®æ–‡æœ¬
        concept_text: æ¦‚å¿µæ–‡æœ¬
        similarity_score: æ£€ç´¢ç›¸ä¼¼åº¦åˆ†æ•°
        
    Returns:
        float: ç›¸å…³æ€§åˆ†æ•° (0-1)
    """
    from difflib import SequenceMatcher
    
    # åŸºç¡€åˆ†æ•°æ¥è‡ªæ£€ç´¢ç›¸ä¼¼åº¦
    base_score = similarity_score
    
    # 1. æ¦‚å¿µè¯æ±‡åŒ¹é…åº¦
    concept_words = set(concept_text.lower().split())
    evidence_words = set(evidence_text.lower().split())
    
    if concept_words and evidence_words:
        word_overlap = len(concept_words.intersection(evidence_words)) / len(concept_words)
    else:
        word_overlap = 0.0
    
    # 2. æ–‡æœ¬é•¿åº¦é€‚ä¸­æ€§ï¼ˆå¤ªçŸ­æˆ–å¤ªé•¿éƒ½å‡åˆ†ï¼‰
    evidence_length = len(evidence_text)
    if 50 <= evidence_length <= 300:
        length_factor = 1.0
    elif 20 <= evidence_length < 50 or 300 < evidence_length <= 500:
        length_factor = 0.8
    elif evidence_length > 500:
        length_factor = 0.6
    else:
        length_factor = 0.4
    
    # 3. ç›´æ¥åŒ…å«æ¦‚å¿µåç§°åŠ åˆ†
    concept_containment = 0.0
    if concept_text.lower() in evidence_text.lower():
        concept_containment = 0.3
    
    # 4. å®Œæ•´å¥å­åŠ åˆ†
    sentence_completeness = 0.0
    if evidence_text.strip().endswith(('.', '!', '?', ':')):
        sentence_completeness = 0.1
    
    # ç»¼åˆç›¸å…³æ€§åˆ†æ•°
    relevance_score = (
        base_score * 0.4 +
        word_overlap * 0.3 +
        concept_containment * 0.2 +
        length_factor * 0.1 +
        sentence_completeness
    )
    
    return min(1.0, max(0.0, relevance_score))

def extract_evidence_from_text(chunk_text: str, 
                             concept_text: str, 
                             similarity_score: float,
                             min_length: int = 20,
                             max_length: int = 400) -> List[Dict[str, Any]]:
    """
    ä»åˆ†å—æ–‡æœ¬ä¸­æå–è¯æ®
    
    Args:
        chunk_text: åˆ†å—æ–‡æœ¬
        concept_text: æ¦‚å¿µæ–‡æœ¬
        similarity_score: ç›¸ä¼¼åº¦åˆ†æ•°
        min_length: æœ€å°è¯æ®é•¿åº¦
        max_length: æœ€å¤§è¯æ®é•¿åº¦
        
    Returns:
        List[Dict[str, Any]]: æå–çš„è¯æ®åˆ—è¡¨
    """
    # å°†æ–‡æœ¬åˆ†è§£ä¸ºå¥å­
    sentences = re.split(r'[.!?]+', chunk_text)
    sentences = [s.strip() for s in sentences if s.strip()]
    
    evidences = []
    concept_lower = concept_text.lower()
    
    # 1. å¯»æ‰¾åŒ…å«æ¦‚å¿µçš„å¥å­
    for i, sentence in enumerate(sentences):
        if len(sentence) < min_length or len(sentence) > max_length:
            continue
        
        sentence_lower = sentence.lower()
        
        # æ£€æŸ¥æ˜¯å¦ä¸æ¦‚å¿µç›¸å…³
        if any(word in sentence_lower for word in concept_lower.split()):
            # å°è¯•æ‰©å±•ä¸Šä¸‹æ–‡
            extended_evidence = sentence
            
            # å‘å‰æ‰©å±•ä¸€å¥ï¼ˆå¦‚æœåˆé€‚ï¼‰
            if i > 0 and len(extended_evidence + " " + sentences[i-1]) <= max_length:
                extended_evidence = sentences[i-1] + " " + extended_evidence
            
            # å‘åæ‰©å±•ä¸€å¥ï¼ˆå¦‚æœåˆé€‚ï¼‰
            if i < len(sentences) - 1 and len(extended_evidence + " " + sentences[i+1]) <= max_length:
                extended_evidence = extended_evidence + " " + sentences[i+1]
            
            # è®¡ç®—ç›¸å…³æ€§
            relevance_score = calculate_evidence_relevance(
                extended_evidence, concept_text, similarity_score
            )
            
            # åˆ†ç±»è¯æ®ç±»å‹
            evidence_type = classify_evidence_type(extended_evidence, concept_text)
            
            evidences.append({
                "evidence_text": extended_evidence,
                "evidence_length": len(extended_evidence),
                "relevance_score": relevance_score,
                "evidence_type": evidence_type,
                "sentence_index": i,
                "is_extended": len(extended_evidence) > len(sentence)
            })
    
    # 2. å¯»æ‰¾å®šä¹‰æ€§æ®µè½ï¼ˆè¿ç»­2-3å¥è¯ï¼‰
    for i in range(len(sentences) - 1):
        if i + 2 < len(sentences):
            paragraph = " ".join(sentences[i:i+3])
        else:
            paragraph = " ".join(sentences[i:i+2])
        
        if min_length <= len(paragraph) <= max_length:
            paragraph_lower = paragraph.lower()
            
            # æ£€æŸ¥æ˜¯å¦åƒå®šä¹‰æˆ–è§£é‡Š
            if any(pattern in paragraph_lower for pattern in [
                concept_lower, "define", "refer to", "means", "is a", "consist of"
            ]):
                relevance_score = calculate_evidence_relevance(
                    paragraph, concept_text, similarity_score
                )
                
                if relevance_score >= 0.3:  # åªä¿ç•™è¾ƒé«˜ç›¸å…³æ€§çš„æ®µè½
                    evidence_type = classify_evidence_type(paragraph, concept_text)
                    
                    evidences.append({
                        "evidence_text": paragraph,
                        "evidence_length": len(paragraph),
                        "relevance_score": relevance_score,
                        "evidence_type": evidence_type,
                        "sentence_index": i,
                        "is_extended": True
                    })
    
    # å»é‡å¹¶æŒ‰ç›¸å…³æ€§æ’åº
    unique_evidences = []
    seen_texts = set()
    
    for evidence in evidences:
        text_key = evidence["evidence_text"][:100]  # ä½¿ç”¨å‰100å­—ç¬¦ä½œä¸ºå»é‡é”®
        if text_key not in seen_texts:
            seen_texts.add(text_key)
            unique_evidences.append(evidence)
    
    # æŒ‰ç›¸å…³æ€§æ’åº
    unique_evidences.sort(key=lambda x: x["relevance_score"], reverse=True)
    
    return unique_evidences[:5]  # æœ€å¤šè¿”å›5ä¸ªè¯æ®

def perform_evidence_extraction(retrieval_results: Dict[str, Any],
                              chunk_text_mapping: Dict[str, str],
                              config) -> Dict[str, Any]:
    """
    æ‰§è¡Œè¯æ®æå–
    
    Args:
        retrieval_results: æ£€ç´¢ç»“æœ
        chunk_text_mapping: åˆ†å—æ–‡æœ¬æ˜ å°„
        config: é…ç½®å¯¹è±¡
        
    Returns:
        Dict[str, Any]: è¯æ®æå–ç»“æœ
    """
    print("ğŸ” å¼€å§‹æ‰§è¡Œè¯æ®æå–...")
    
    min_length = config.get("evidence_extraction.min_length", 20)
    max_length = config.get("evidence_extraction.max_length", 400)
    min_relevance = 0.2  # æœ€å°ç›¸å…³æ€§é˜ˆå€¼
    
    evidence_nodes = []
    evidence_by_concept = {}
    evidence_type_counts = defaultdict(int)
    
    total_evidence = 0
    concepts_with_evidence = 0
    
    for concept_id, retrieval_data in retrieval_results.items():
        concept_text = retrieval_data.get("concept_text", "")
        retrieved_chunks = retrieval_data.get("retrieved_chunks", [])
        
        print(f"   æå–æ¦‚å¿µè¯æ®: {concept_text}")
        
        concept_evidences = []
        
        for chunk_data in retrieved_chunks:
            chunk_id = chunk_data.get("chunk_id", "")
            similarity_score = chunk_data.get("similarity_score", 0.0)
            
            # è·å–åˆ†å—æ–‡æœ¬
            chunk_text = chunk_text_mapping.get(chunk_id, "")
            if not chunk_text:
                continue
            
            # æå–è¯æ®
            evidences = extract_evidence_from_text(
                chunk_text, concept_text, similarity_score, min_length, max_length
            )
            
            for evidence in evidences:
                if evidence["relevance_score"] >= min_relevance:
                    evidence_id = f"evidence_{len(evidence_nodes)}"
                    
                    evidence_node = {
                        "evidence_id": evidence_id,
                        "concept_id": concept_id,
                        "concept_text": concept_text,
                        "evidence_text": evidence["evidence_text"],
                        "evidence_length": evidence["evidence_length"],
                        "relevance_score": evidence["relevance_score"],
                        "evidence_type": evidence["evidence_type"],
                        "source_chunk_id": chunk_id,
                        "chunk_similarity": similarity_score,
                        "sentence_index": evidence["sentence_index"],
                        "is_extended": evidence["is_extended"],
                        "created_at": datetime.now().isoformat()
                    }
                    
                    evidence_nodes.append(evidence_node)
                    concept_evidences.append(evidence_node)
                    evidence_type_counts[evidence["evidence_type"]] += 1
                    total_evidence += 1
        
        if concept_evidences:
            concepts_with_evidence += 1
            evidence_by_concept[concept_id] = concept_evidences
        else:
            evidence_by_concept[concept_id] = []
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    if evidence_nodes:
        avg_relevance = sum(e["relevance_score"] for e in evidence_nodes) / len(evidence_nodes)
        avg_length = sum(e["evidence_length"] for e in evidence_nodes) / len(evidence_nodes)
    else:
        avg_relevance = 0.0
        avg_length = 0.0
    
    avg_evidence_per_concept = total_evidence / len(retrieval_results) if retrieval_results else 0
    
    print(f"   âœ… è¯æ®æå–å®Œæˆ:")
    print(f"      - æ€»è¯æ®æ•°: {total_evidence}")
    print(f"      - æœ‰è¯æ®çš„æ¦‚å¿µ: {concepts_with_evidence}/{len(retrieval_results)}")
    print(f"      - å¹³å‡ç›¸å…³æ€§: {avg_relevance:.4f}")
    
    return {
        "evidence_nodes": evidence_nodes,
        "evidence_by_concept": evidence_by_concept,
        "statistics": {
            "total_evidence": total_evidence,
            "concepts_with_evidence": concepts_with_evidence,
            "avg_evidence_per_concept": avg_evidence_per_concept,
            "avg_relevance_score": avg_relevance,
            "avg_evidence_length": avg_length,
            "evidence_type_distribution": dict(evidence_type_counts),
            "min_relevance_threshold": min_relevance,
            "processed_concepts": len(retrieval_results)
        }
    }

def analyze_evidence_quality(evidence_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    åˆ†æè¯æ®è´¨é‡
    
    Args:
        evidence_data: è¯æ®æ•°æ®
        
    Returns:
        Dict[str, Any]: è´¨é‡åˆ†æç»“æœ
    """
    print("ğŸ“ˆ åˆ†æè¯æ®è´¨é‡...")
    
    evidence_nodes = evidence_data["evidence_nodes"]
    
    if not evidence_nodes:
        return {
            "overall_quality_score": 0.0,
            "quality_distribution": {"high": 0, "medium": 0, "low": 0},
            "type_quality_scores": {},
            "top_evidences": []
        }
    
    # è´¨é‡åˆ†ç±»
    high_quality = [e for e in evidence_nodes if e["relevance_score"] >= 0.7]
    medium_quality = [e for e in evidence_nodes if 0.4 <= e["relevance_score"] < 0.7]
    low_quality = [e for e in evidence_nodes if e["relevance_score"] < 0.4]
    
    # æŒ‰ç±»å‹åˆ†æè´¨é‡
    type_scores = defaultdict(list)
    for evidence in evidence_nodes:
        type_scores[evidence["evidence_type"]].append(evidence["relevance_score"])
    
    type_quality_scores = {}
    for evidence_type, scores in type_scores.items():
        type_quality_scores[evidence_type] = {
            "avg_score": sum(scores) / len(scores),
            "count": len(scores),
            "max_score": max(scores),
            "min_score": min(scores)
        }
    
    # æ•´ä½“è´¨é‡è¯„åˆ†
    total_count = len(evidence_nodes)
    overall_quality_score = (
        (len(high_quality) / total_count * 1.0) +
        (len(medium_quality) / total_count * 0.6) +
        (len(low_quality) / total_count * 0.2)
    )
    
    # é¡¶çº§è¯æ®
    top_evidences = sorted(
        evidence_nodes,
        key=lambda x: x["relevance_score"],
        reverse=True
    )[:10]
    
    return {
        "overall_quality_score": overall_quality_score,
        "quality_distribution": {
            "high": len(high_quality),
            "medium": len(medium_quality),
            "low": len(low_quality)
        },
        "type_quality_scores": type_quality_scores,
        "top_evidences": top_evidences,
        "coverage_analysis": {
            "concepts_with_high_quality": len([
                cid for cid, evidences in evidence_data["evidence_by_concept"].items()
                if any(e["relevance_score"] >= 0.7 for e in evidences)
            ]),
            "concepts_without_evidence": len([
                cid for cid, evidences in evidence_data["evidence_by_concept"].items()
                if not evidences
            ])
        }
    }

def process_step6_evidence_extraction(step5_result: Dict[str, Any], 
                                     previous_data: Dict[str, Any],
                                     config_path: str = "config.yml") -> Dict[str, Any]:
    """
    æ‰§è¡Œæ­¥éª¤6çš„è¯æ®æå–å¤„ç†
    
    Args:
        step5_result: æ­¥éª¤5çš„ç»“æœ
        previous_data: ä¹‹å‰æ­¥éª¤çš„æ•°æ®
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        Dict[str, Any]: æ­¥éª¤6çš„å¤„ç†ç»“æœ
    """
    start_time = time.time()
    
    try:
        # 1. åŠ è½½é…ç½®
        print("ğŸ“‹ åŠ è½½é…ç½®...")
        config = load_config_from_yaml(config_path)
        
        # 2. æå–æ£€ç´¢ç»“æœ
        print("ğŸ“Š æå–æ£€ç´¢ç»“æœ...")
        retrieval_results = extract_retrieval_results_from_step5(step5_result)
        
        # 3. åˆ›å»ºåˆ†å—æ–‡æœ¬æ˜ å°„
        print("ğŸ—‚ï¸ åˆ›å»ºåˆ†å—æ–‡æœ¬æ˜ å°„...")
        if "step2" not in previous_data:
            raise ValueError("ç¼ºå°‘æ­¥éª¤2çš„åˆ†å—æ•°æ®")
        
        chunk_text_mapping = get_chunk_text_mapping(previous_data["step2"])
        
        # 4. æ‰§è¡Œè¯æ®æå–
        print("ğŸ” æ‰§è¡Œè¯æ®æå–...")
        evidence_data = perform_evidence_extraction(retrieval_results, chunk_text_mapping, config)
        
        # 5. åˆ†æè¯æ®è´¨é‡
        print("ğŸ“ˆ åˆ†æè¯æ®è´¨é‡...")
        quality_analysis = analyze_evidence_quality(evidence_data)
        
        processing_time = time.time() - start_time
        
        # 6. æ„å»ºç»“æœ
        result = {
            "success": True,
            "step": 6,
            "step_name": "è¯æ®æå–ä¸è´¨é‡è¯„ä¼°",
            "evidence_nodes": evidence_data["evidence_nodes"],
            "evidence_by_concept": evidence_data["evidence_by_concept"],
            "statistics": evidence_data["statistics"],
            "quality_analysis": quality_analysis,
            "processing_time": processing_time,
            "timestamp": datetime.now().isoformat(),
            "config_used": {
                "min_length": config.get("evidence_extraction.min_length", 20),
                "max_length": config.get("evidence_extraction.max_length", 400)
            }
        }
        
        return result
        
    except Exception as e:
        error_msg = f"æ­¥éª¤6å¤„ç†å¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        
        result = {
            "success": False,
            "step": 6,
            "step_name": "è¯æ®æå–ä¸è´¨é‡è¯„ä¼°",
            "error": error_msg,
            "processing_time": time.time() - start_time,
            "timestamp": datetime.now().isoformat()
        }
        
        return result

def main():
    if len(sys.argv) != 2:
        print("ç”¨æ³•: python step6.py <step5è¾“å‡ºæ–‡ä»¶æˆ–å®éªŒæ–‡ä»¶å¤¹>")
        print("ç¤ºä¾‹:")
        print("  python step6.py experiments/20241204_143052_attention_paper/step5_answer_generation.txt")
        print("  python step6.py experiments/20241204_143052_attention_paper/")
        print("\næ–°åŠŸèƒ½:")
        print("âœ… è‡ªåŠ¨è¯†åˆ«å®éªŒæ–‡ä»¶å¤¹")
        print("âœ… æ™ºèƒ½è¯æ®æå–å’Œç›¸å…³æ€§è®¡ç®—")
        print("âœ… å¤šç±»å‹è¯æ®è¯†åˆ«å’Œåˆ†ç±»")
        print("âœ… è¯æ®è´¨é‡è¯„åˆ†å’Œè¿‡æ»¤")
        print("âœ… ç»Ÿä¸€çš„å®éªŒç®¡ç†")
        sys.exit(1)
    
    input_path = sys.argv[1]
    
    print(f"ğŸš€ æ­¥éª¤6: è¯æ®æå–ä¸è´¨é‡è¯„ä¼° (å¢å¼ºç‰ˆ)")
    print(f"ğŸ“„ è¾“å…¥: {input_path}")
    print("="*60)
    
    try:
        # 1. åŠ è½½æ­¥éª¤5ç»“æœå’Œå®éªŒç®¡ç†å™¨
        print("ğŸ“‚ åŠ è½½æ­¥éª¤5ç»“æœ...")
        step5_result, experiment_manager = load_step5_result(input_path)
        
        if not step5_result.get("success"):
            print("âŒ æ­¥éª¤5æœªæˆåŠŸå®Œæˆï¼Œæ— æ³•ç»§ç»­")
            sys.exit(1)
        
        print(f"âœ… å·²åŠ è½½å®éªŒ: {experiment_manager.experiment_name}")
        print(f"ğŸ“ å®éªŒç›®å½•: {experiment_manager.experiment_dir}")
        print()
        
        # 2. åŠ è½½ä¹‹å‰æ­¥éª¤çš„æ•°æ®
        print("ğŸ“‚ åŠ è½½ä¹‹å‰æ­¥éª¤çš„æ•°æ®...")
        previous_data = load_previous_steps_data(experiment_manager)
        
        # 3. æ‰§è¡Œæ­¥éª¤6å¤„ç†
        print("ğŸ”„ å¼€å§‹æ­¥éª¤6å¤„ç†...")
        result = process_step6_evidence_extraction(step5_result, previous_data)
        
        # 4. ä¿å­˜ç»“æœåˆ°å®éªŒæ–‡ä»¶å¤¹
        print("ğŸ’¾ ä¿å­˜å®éªŒç»“æœ...")
        saved_files = experiment_manager.save_step_result(
            step_num=6,
            result=result,
            save_formats=['txt', 'json']
        )
        
        if result.get("success"):
            print(f"\nâœ… æ­¥éª¤6å®Œæˆ!")
            print(f"ğŸ“ å®éªŒç›®å½•: {experiment_manager.experiment_dir}")
            print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶:")
            for format_type, file_path in saved_files.items():
                print(f"   - {format_type.upper()}: {file_path}")
            
            # æ˜¾ç¤ºå¤„ç†ç»“æœæ‘˜è¦
            stats = result.get("statistics", {})
            quality = result.get("quality_analysis", {})
            
            print(f"\nğŸ“Š è¯æ®æå–ç»“æœæ‘˜è¦:")
            print(f"   - æ€»è¯æ®æ•°: {stats.get('total_evidence', 0)}")
            print(f"   - æœ‰è¯æ®çš„æ¦‚å¿µæ•°: {stats.get('concepts_with_evidence', 0)}")
            print(f"   - å¹³å‡æ¯æ¦‚å¿µè¯æ®æ•°: {stats.get('avg_evidence_per_concept', 0):.2f}")
            print(f"   - å¹³å‡ç›¸å…³æ€§åˆ†æ•°: {stats.get('avg_relevance_score', 0):.4f}")
            print(f"   - å¹³å‡è¯æ®é•¿åº¦: {stats.get('avg_evidence_length', 0):.1f} å­—ç¬¦")
            print(f"   - å¤„ç†æ—¶é—´: {result.get('processing_time', 0):.2f} ç§’")
            
            # æ˜¾ç¤ºè¯æ®ç±»å‹åˆ†å¸ƒ
            type_dist = stats.get("evidence_type_distribution", {})
            if type_dist:
                print(f"\nğŸ“ˆ è¯æ®ç±»å‹åˆ†å¸ƒ:")
                for evidence_type, count in type_dist.items():
                    print(f"   - {evidence_type}: {count} ä¸ª")
            
            # æ˜¾ç¤ºè´¨é‡åˆ†æ
            print(f"\nğŸ“ˆ è¯æ®è´¨é‡åˆ†æ:")
            print(f"   - æ•´ä½“è´¨é‡è¯„åˆ†: {quality.get('overall_quality_score', 0):.3f}")
            
            quality_dist = quality.get("quality_distribution", {})
            print(f"   - é«˜è´¨é‡è¯æ®: {quality_dist.get('high', 0)} ä¸ª")
            print(f"   - ä¸­ç­‰è´¨é‡è¯æ®: {quality_dist.get('medium', 0)} ä¸ª")
            print(f"   - ä½è´¨é‡è¯æ®: {quality_dist.get('low', 0)} ä¸ª")
            
            # æ˜¾ç¤ºè¦†ç›–åˆ†æ
            coverage = quality.get("coverage_analysis", {})
            print(f"   - æœ‰é«˜è´¨é‡è¯æ®çš„æ¦‚å¿µ: {coverage.get('concepts_with_high_quality', 0)} ä¸ª")
            print(f"   - æ— è¯æ®çš„æ¦‚å¿µ: {coverage.get('concepts_without_evidence', 0)} ä¸ª")
            
            # æ˜¾ç¤ºé¡¶çº§è¯æ®
            top_evidences = quality.get("top_evidences", [])
            if top_evidences:
                print(f"\nğŸŒŸ é¡¶çº§è¯æ® (å‰5ä¸ª):")
                for i, evidence in enumerate(top_evidences[:5], 1):
                    print(f"   {i}. æ¦‚å¿µ: {evidence['concept_text'][:30]}...")
                    print(f"      ç›¸å…³æ€§: {evidence['relevance_score']:.4f}")
                    print(f"      ç±»å‹: {evidence['evidence_type']}")
                    print(f"      å†…å®¹: {evidence['evidence_text'][:100]}...")
                    print()
            
            # æ˜¾ç¤ºå®éªŒçŠ¶æ€
            summary = experiment_manager.get_experiment_summary()
            print(f"ğŸ§ª å®éªŒçŠ¶æ€:")
            print(f"   - å®éªŒID: {summary['experiment_id']}")
            print(f"   - å·²å®Œæˆæ­¥éª¤: {summary['steps_completed']}/{summary['total_steps']}")
            print(f"   - å½“å‰çŠ¶æ€: {summary['status']}")
            
            # æç¤ºåç»­æ­¥éª¤
            print(f"\nğŸ“‹ åç»­æ­¥éª¤:")
            print(f"   è¿è¡Œä¸‹ä¸€æ­¥: python step7.py {saved_files['txt']}")
            print(f"   æŸ¥çœ‹ç»“æœ: cat {saved_files['txt']}")
            print(f"   æŸ¥çœ‹è¯æ®: grep -A 5 'é¡¶çº§è¯æ®' {saved_files['txt']}")
                
        else:
            print(f"âŒ æ­¥éª¤6å¤±è´¥: {result.get('error')}")
            
            # å³ä½¿å¤±è´¥ä¹Ÿä¿å­˜é”™è¯¯ä¿¡æ¯
            experiment_manager.save_step_result(
                step_num=6,
                result=result,
                save_formats=['txt']
            )
            sys.exit(1)
            
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        # å¦‚æœæœ‰å®éªŒç®¡ç†å™¨ï¼Œä¿å­˜é”™è¯¯ä¿¡æ¯
        if 'experiment_manager' in locals():
            error_result = {
                "step": 6,
                "step_name": "è¯æ®æå–ä¸è´¨é‡è¯„ä¼°",
                "success": False,
                "error": str(e),
                "traceback": traceback.format_exc(),
                "processing_time": 0.0,
                "timestamp": datetime.now().isoformat()
            }
            
            try:
                experiment_manager.save_step_result(6, error_result, ['txt'])
                print(f"ğŸ“„ é”™è¯¯ä¿¡æ¯å·²ä¿å­˜åˆ°å®éªŒç›®å½•: {experiment_manager.experiment_dir}")
            except:
                pass
        
        sys.exit(1)

if __name__ == "__main__":
    main()