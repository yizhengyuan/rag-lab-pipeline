#!/usr/bin/env python3
"""
æ­¥éª¤2: æ–‡æ¡£åˆ†å—ä¸æ¦‚å¿µæå– - å¢å¼ºç‰ˆ
======================================

åŠŸèƒ½ï¼š
1. ä»æ­¥éª¤1çš„å®éªŒç»“æœä¸­è¯»å–æ–‡æ¡£
2. æ‰§è¡Œé«˜è´¨é‡çš„è¯­ä¹‰åˆ†å—
3. æå–å’Œä¼˜åŒ–æ¦‚å¿µ
4. ä¿å­˜åˆ°åŒä¸€ä¸ªå®éªŒæ–‡ä»¶å¤¹

ç”¨æ³•: 
- python step2.py <step1è¾“å‡ºæ–‡ä»¶.txt>
- python step2.py <å®éªŒæ–‡ä»¶å¤¹è·¯å¾„>

æ–°åŠŸèƒ½ï¼š
- âœ… è‡ªåŠ¨è¯†åˆ«å¹¶ä½¿ç”¨step1çš„å®éªŒæ–‡ä»¶å¤¹
- âœ… é«˜è´¨é‡çš„æ¦‚å¿µæå–å’ŒéªŒè¯
- âœ… æ”¹è¿›çš„åˆ†å—ç®—æ³•
- âœ… ç»Ÿä¸€çš„å®éªŒç®¡ç†
"""

import sys
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List
import logging

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
sys.path.append(str(Path(__file__).parent))
from llama_index.core import Document
from llama_index.core.schema import TextNode

# å¯¼å…¥æ ¸å¿ƒå¤„ç†æ¨¡å—
from core.chunking import SemanticChunker
from core.concept_merger import ConceptMerger
from config.settings import load_config_from_yaml

# å¯¼å…¥å®éªŒç®¡ç†å™¨
from utils.experiment_manager import ExperimentManager, load_latest_experiment
from utils.helpers import FileHelper

logger = logging.getLogger(__name__)

def load_step1_result(step1_file_or_dir: str) -> tuple:
    """
    ä»æ­¥éª¤1çš„è¾“å‡ºæ–‡ä»¶æˆ–å®éªŒæ–‡ä»¶å¤¹ä¸­åŠ è½½ç»“æœ
    
    Args:
        step1_file_or_dir: æ­¥éª¤1çš„è¾“å‡ºæ–‡ä»¶æˆ–å®éªŒæ–‡ä»¶å¤¹è·¯å¾„
        
    Returns:
        tuple: (step1_result, experiment_manager)
    """
    step1_path = Path(step1_file_or_dir)
    
    if step1_path.is_file():
        # æƒ…å†µ1ï¼šç›´æ¥æŒ‡å®šäº†step1çš„è¾“å‡ºæ–‡ä»¶
        if step1_path.name.startswith("step1") and step1_path.suffix == ".txt":
            experiment_dir = step1_path.parent
            experiment_manager = ExperimentManager.load_experiment(str(experiment_dir))
            
            # ä»txtæ–‡ä»¶åŠ è½½ç»“æœ
            step1_result = load_result_from_txt(str(step1_path))
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {step1_path}")
            
    elif step1_path.is_dir():
        # æƒ…å†µ2ï¼šç›´æ¥æŒ‡å®šäº†å®éªŒæ–‡ä»¶å¤¹
        experiment_manager = ExperimentManager.load_experiment(str(step1_path))
        
        # æŸ¥æ‰¾step1çš„è¾“å‡ºæ–‡ä»¶
        step1_txt_path = experiment_manager.get_step_output_path(1, "txt")
        if not step1_txt_path.exists():
            raise FileNotFoundError(f"å®éªŒæ–‡ä»¶å¤¹ä¸­æ‰¾ä¸åˆ°step1è¾“å‡ºæ–‡ä»¶: {step1_txt_path}")
        
        step1_result = load_result_from_txt(str(step1_txt_path))
        
    else:
        raise FileNotFoundError(f"è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {step1_path}")
    
    return step1_result, experiment_manager

def load_result_from_txt(input_file: str) -> Dict[str, Any]:
    """ä»txtæ–‡ä»¶ä¸­åŠ è½½ç»“æœæ•°æ®"""
    with open(input_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    start_marker = "# JSON_DATA_START\n"
    end_marker = "\n# JSON_DATA_END"
    
    start_idx = content.find(start_marker)
    end_idx = content.find(end_marker)
    
    if start_idx == -1 or end_idx == -1:
        raise ValueError("æ— æ³•ä»txtæ–‡ä»¶ä¸­è§£ææ•°æ®")
    
    json_str = content[start_idx + len(start_marker):end_idx]
    return json.loads(json_str)

def reconstruct_document_from_step1(step1_result: Dict[str, Any]) -> Document:
    """ä»æ­¥éª¤1ç»“æœé‡å»ºDocumentå¯¹è±¡"""
    doc_data = step1_result.get("document", {})
    
    if not doc_data:
        raise ValueError("æ­¥éª¤1ç»“æœä¸­æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£æ•°æ®")
    
    # é‡å»ºDocumentå¯¹è±¡
    document = Document(
        text=doc_data.get("text", ""),
        metadata=doc_data.get("metadata", {})
    )
    
    # éªŒè¯æ–‡æ¡£å†…å®¹
    if not document.text or len(document.text.strip()) < 100:
        raise ValueError("æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–è¿‡çŸ­")
    
    return document

def extract_high_quality_concepts(chunk_nodes: List[TextNode], config) -> tuple:
    """
    æå–é«˜è´¨é‡æ¦‚å¿µ
    
    Args:
        chunk_nodes: æ–‡æœ¬èŠ‚ç‚¹åˆ—è¡¨
        config: é…ç½®å¯¹è±¡
        
    Returns:
        tuple: (processed_chunks, unique_concepts, concept_stats)
    """
    print("ğŸ§  å¼€å§‹é«˜è´¨é‡æ¦‚å¿µæå–...")
    
    # 1. è§£æç°æœ‰æ¦‚å¿µï¼ˆå¦‚æœå·²ç»å­˜åœ¨ï¼‰
    all_concepts = []
    processed_chunks = []
    
    for i, chunk in enumerate(chunk_nodes):
        # è·å–æ¦‚å¿µæ•°æ®
        concepts_data = chunk.metadata.get("concepts", "[]")
        if isinstance(concepts_data, str):
            try:
                concepts = json.loads(concepts_data)
            except json.JSONDecodeError:
                concepts = []
        else:
            concepts = concepts_data if isinstance(concepts_data, list) else []
        
        # éªŒè¯å’Œæ¸…ç†æ¦‚å¿µ
        valid_concepts = []
        for concept in concepts:
            if isinstance(concept, str) and len(concept.strip()) > 2:
                cleaned_concept = concept.strip()
                # ç®€å•çš„è´¨é‡è¿‡æ»¤
                if not any(pattern in cleaned_concept.lower() for pattern in ['training', 'method', 'system', 'process']):
                    valid_concepts.append(cleaned_concept)
        
        all_concepts.extend(valid_concepts)
        
        # æ›´æ–°chunkçš„æ¦‚å¿µæ•°æ®
        chunk.metadata["concepts"] = json.dumps(valid_concepts)
        chunk.metadata["concept_count"] = len(valid_concepts)
        chunk.metadata["chunk_index"] = i
        
        processed_chunks.append(chunk)
    
    print(f"   æå–åˆ° {len(all_concepts)} ä¸ªåŸå§‹æ¦‚å¿µ")
    
    # 2. æ¦‚å¿µå»é‡å’Œåˆå¹¶
    unique_concepts = list(set(all_concepts))
    print(f"   å»é‡åå‰©ä½™ {len(unique_concepts)} ä¸ªå”¯ä¸€æ¦‚å¿µ")
    
    # 3. å°è¯•ä½¿ç”¨æ¦‚å¿µåˆå¹¶å™¨è¿›ä¸€æ­¥ä¼˜åŒ–
    try:
        concept_merger = ConceptMerger(config)
        
        # è½¬æ¢ä¸ºæ¦‚å¿µèŠ‚ç‚¹æ ¼å¼
        from core.nodes import ConceptNode
        concept_nodes = []
        for concept in unique_concepts:
            concept_node = ConceptNode(
                concept=concept,
                definition=f"ä»æ–‡æ¡£ä¸­æå–çš„æ¦‚å¿µ: {concept}",
                source_chunks=[],
                confidence=0.8
            )
            concept_nodes.append(concept_node)
        
        # æ‰§è¡Œæ¦‚å¿µåˆå¹¶
        merged_concepts = concept_merger.merge_concepts(concept_nodes)
        final_concepts = [node.concept for node in merged_concepts]
        
        print(f"   æ¦‚å¿µåˆå¹¶åå‰©ä½™ {len(final_concepts)} ä¸ªé«˜è´¨é‡æ¦‚å¿µ")
        unique_concepts = final_concepts
        
    except Exception as e:
        logger.warning(f"æ¦‚å¿µåˆå¹¶å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ¦‚å¿µ: {e}")
    
    # 4. ç”Ÿæˆæ¦‚å¿µç»Ÿè®¡
    concept_stats = {
        "total_raw_concepts": len(all_concepts),
        "unique_concepts": len(unique_concepts),
        "concepts_per_chunk": len(all_concepts) / len(processed_chunks) if processed_chunks else 0,
        "concept_quality_score": len(unique_concepts) / max(len(all_concepts), 1) if all_concepts else 0
    }
    
    return processed_chunks, unique_concepts, concept_stats

def improve_chunking_quality(document: Document, config) -> List[TextNode]:
    """
    æ”¹è¿›åˆ†å—è´¨é‡
    
    Args:
        document: æ–‡æ¡£å¯¹è±¡
        config: é…ç½®å¯¹è±¡
        
    Returns:
        List[TextNode]: é«˜è´¨é‡çš„åˆ†å—èŠ‚ç‚¹
    """
    print("ğŸ“„ æ‰§è¡Œé«˜è´¨é‡è¯­ä¹‰åˆ†å—...")
    
    # ä½¿ç”¨æ›´ä¸¥æ ¼çš„åˆ†å—å‚æ•°
    improved_config = config.copy()
    
    # è°ƒæ•´åˆ†å—å‚æ•°ä»¥æé«˜è´¨é‡
    improved_config.set("chunking.max_tokens_per_chunk", 4000)  # å‡å°chunkå¤§å°
    improved_config.set("chunking.min_chunk_length", 50)       # å¢åŠ æœ€å°é•¿åº¦
    improved_config.set("chunking.breakpoint_percentile_threshold", 90)  # æ›´ä¸¥æ ¼çš„åˆ†å‰²é˜ˆå€¼
    
    # åˆå§‹åŒ–æ”¹è¿›çš„åˆ†å—å™¨
    chunker = SemanticChunker(improved_config)
    
    # æ‰§è¡Œåˆ†å—
    chunk_nodes = chunker.chunk_and_extract_concepts([document])
    
    # è¿‡æ»¤è´¨é‡è¿‡ä½çš„åˆ†å—
    quality_chunks = []
    for chunk in chunk_nodes:
        # è´¨é‡æ£€æŸ¥
        if len(chunk.text.strip()) >= 50:  # æœ€å°é•¿åº¦æ£€æŸ¥
            # æ£€æŸ¥æ¦‚å¿µè´¨é‡
            concepts_data = chunk.metadata.get("concepts", "[]")
            if isinstance(concepts_data, str):
                try:
                    concepts = json.loads(concepts_data)
                except json.JSONDecodeError:
                    concepts = []
            else:
                concepts = concepts_data if isinstance(concepts_data, list) else []
            
            # è‡³å°‘è¦æœ‰ä¸€äº›æ¦‚å¿µæˆ–è¶³å¤Ÿé•¿çš„æ–‡æœ¬
            if len(concepts) > 0 or len(chunk.text) > 200:
                quality_chunks.append(chunk)
    
    print(f"   åˆ†å—å®Œæˆ: {len(chunk_nodes)} â†’ {len(quality_chunks)} ä¸ªé«˜è´¨é‡åˆ†å—")
    
    return quality_chunks

def process_step2_chunking(step1_result: Dict[str, Any], config_path: str = "config.yml") -> Dict[str, Any]:
    """
    æ‰§è¡Œæ­¥éª¤2çš„æ–‡æ¡£åˆ†å—å¤„ç†
    
    Args:
        step1_result: æ­¥éª¤1çš„ç»“æœ
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        Dict[str, Any]: æ­¥éª¤2çš„å¤„ç†ç»“æœ
    """
    start_time = time.time()
    
    try:
        # 1. åŠ è½½é…ç½®
        print("ğŸ“‹ åŠ è½½é…ç½®...")
        config = load_config_from_yaml(config_path)
        
        # 2. é‡å»ºæ–‡æ¡£å¯¹è±¡
        print("ğŸ“„ é‡å»ºæ–‡æ¡£å¯¹è±¡...")
        document = reconstruct_document_from_step1(step1_result)
        print(f"âœ… æ–‡æ¡£é‡å»ºæˆåŠŸ: {len(document.text):,} å­—ç¬¦")
        
        # 3. æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åˆ†å—
        existing_chunks = step1_result.get("chunk_nodes", [])
        
        if existing_chunks and len(existing_chunks) > 0:
            print("ğŸ”„ ä½¿ç”¨ç°æœ‰åˆ†å—ï¼Œé‡æ–°æå–æ¦‚å¿µ...")
            
            # é‡å»ºTextNodeå¯¹è±¡
            chunk_nodes = []
            for chunk_data in existing_chunks:
                if isinstance(chunk_data, dict):
                    text_node = TextNode(
                        text=chunk_data.get("text", ""),
                        metadata=chunk_data.get("metadata", {})
                    )
                    if hasattr(chunk_data, 'get') and chunk_data.get("node_id"):
                        text_node.node_id = chunk_data["node_id"]
                    chunk_nodes.append(text_node)
            
        else:
            print("ğŸ“„ æ‰§è¡Œå…¨æ–°çš„é«˜è´¨é‡åˆ†å—...")
            chunk_nodes = improve_chunking_quality(document, config)
        
        # 4. é«˜è´¨é‡æ¦‚å¿µæå–
        processed_chunks, unique_concepts, concept_stats = extract_high_quality_concepts(chunk_nodes, config)
        
        # 5. ç”Ÿæˆåˆ†å—ç»Ÿè®¡
        chunk_lengths = [len(chunk.text) for chunk in processed_chunks]
        
        statistics = {
            "total_chunks": len(processed_chunks),
            "avg_chunk_length": sum(chunk_lengths) / len(chunk_lengths) if chunk_lengths else 0,
            "min_chunk_length": min(chunk_lengths) if chunk_lengths else 0,
            "max_chunk_length": max(chunk_lengths) if chunk_lengths else 0,
            "total_concepts": concept_stats["total_raw_concepts"],
            "unique_concepts": len(unique_concepts),
            "avg_concepts_per_chunk": concept_stats["concepts_per_chunk"],
            "concept_quality_score": concept_stats["concept_quality_score"]
        }
        
        processing_time = time.time() - start_time
        
        # 6. æ„å»ºç»“æœ
        result = {
            "success": True,
            "step": 2,
            "step_name": "æ–‡æ¡£åˆ†å—ä¸æ¦‚å¿µæå–",
            "document": document,
            "chunks": processed_chunks,
            "unique_concepts": unique_concepts,
            "statistics": statistics,
            "concept_stats": concept_stats,
            "processing_time": processing_time,
            "timestamp": datetime.now().isoformat(),
            "config_used": {
                "max_tokens_per_chunk": config.get("chunking.max_tokens_per_chunk", 4000),
                "min_chunk_length": config.get("chunking.min_chunk_length", 50),
                "concepts_per_chunk": config.get("concept_extraction.concepts_per_chunk", 5)
            }
        }
        
        return result
        
    except Exception as e:
        error_msg = f"æ­¥éª¤2å¤„ç†å¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        
        result = {
            "success": False,
            "step": 2,
            "step_name": "æ–‡æ¡£åˆ†å—ä¸æ¦‚å¿µæå–",
            "error": error_msg,
            "processing_time": time.time() - start_time,
            "timestamp": datetime.now().isoformat()
        }
        
        return result

def main():
    if len(sys.argv) != 2:
        print("ç”¨æ³•: python step2.py <step1è¾“å‡ºæ–‡ä»¶æˆ–å®éªŒæ–‡ä»¶å¤¹>")
        print("ç¤ºä¾‹:")
        print("  python step2.py experiments/20241204_143052_attention_paper/step1_vectorization.txt")
        print("  python step2.py experiments/20241204_143052_attention_paper/")
        print("\næ–°åŠŸèƒ½:")
        print("âœ… è‡ªåŠ¨è¯†åˆ«å®éªŒæ–‡ä»¶å¤¹")
        print("âœ… é«˜è´¨é‡æ¦‚å¿µæå–å’ŒéªŒè¯")
        print("âœ… æ”¹è¿›çš„åˆ†å—ç®—æ³•")
        print("âœ… ç»Ÿä¸€çš„å®éªŒç®¡ç†")
        sys.exit(1)
    
    input_path = sys.argv[1]
    
    print(f"ğŸš€ æ­¥éª¤2: æ–‡æ¡£åˆ†å—ä¸æ¦‚å¿µæå– (å¢å¼ºç‰ˆ)")
    print(f"ğŸ“„ è¾“å…¥: {input_path}")
    print("="*60)
    
    try:
        # 1. åŠ è½½æ­¥éª¤1ç»“æœå’Œå®éªŒç®¡ç†å™¨
        print("ğŸ“‚ åŠ è½½æ­¥éª¤1ç»“æœ...")
        step1_result, experiment_manager = load_step1_result(input_path)
        
        if not step1_result.get("success"):
            print("âŒ æ­¥éª¤1æœªæˆåŠŸå®Œæˆï¼Œæ— æ³•ç»§ç»­")
            sys.exit(1)
        
        print(f"âœ… å·²åŠ è½½å®éªŒ: {experiment_manager.experiment_name}")
        print(f"ğŸ“ å®éªŒç›®å½•: {experiment_manager.experiment_dir}")
        print()
        
        # 2. æ‰§è¡Œæ­¥éª¤2å¤„ç†
        print("ğŸ”„ å¼€å§‹æ­¥éª¤2å¤„ç†...")
        result = process_step2_chunking(step1_result)
        
        # 3. ä¿å­˜ç»“æœåˆ°å®éªŒæ–‡ä»¶å¤¹
        print("ğŸ’¾ ä¿å­˜å®éªŒç»“æœ...")
        saved_files = experiment_manager.save_step_result(
            step_num=2,
            result=result,
            save_formats=['txt', 'json']
        )
        
        if result.get("success"):
            print(f"\nâœ… æ­¥éª¤2å®Œæˆ!")
            print(f"ğŸ“ å®éªŒç›®å½•: {experiment_manager.experiment_dir}")
            print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶:")
            for format_type, file_path in saved_files.items():
                print(f"   - {format_type.upper()}: {file_path}")
            
            # æ˜¾ç¤ºå¤„ç†ç»“æœæ‘˜è¦
            stats = result.get("statistics", {})
            concept_stats = result.get("concept_stats", {})
            
            print(f"\nğŸ“Š å¤„ç†ç»“æœæ‘˜è¦:")
            print(f"   - æ€»åˆ†å—æ•°: {stats.get('total_chunks', 0)}")
            print(f"   - å¹³å‡åˆ†å—é•¿åº¦: {stats.get('avg_chunk_length', 0):.1f} å­—ç¬¦")
            print(f"   - åˆ†å—é•¿åº¦èŒƒå›´: {stats.get('min_chunk_length', 0)} - {stats.get('max_chunk_length', 0)} å­—ç¬¦")
            print(f"   - æå–æ¦‚å¿µæ•°: {stats.get('total_concepts', 0)}")
            print(f"   - å”¯ä¸€æ¦‚å¿µæ•°: {stats.get('unique_concepts', 0)}")
            print(f"   - æ¦‚å¿µè´¨é‡åˆ†æ•°: {concept_stats.get('concept_quality_score', 0):.2f}")
            print(f"   - å¹³å‡æ¯åˆ†å—æ¦‚å¿µæ•°: {stats.get('avg_concepts_per_chunk', 0):.1f}")
            print(f"   - å¤„ç†æ—¶é—´: {result.get('processing_time', 0):.2f} ç§’")
            
            # æ˜¾ç¤ºæ¦‚å¿µç¤ºä¾‹
            unique_concepts = result.get("unique_concepts", [])
            if unique_concepts:
                print(f"\nğŸ§  é«˜è´¨é‡æ¦‚å¿µç¤ºä¾‹ (å‰15ä¸ª):")
                for i, concept in enumerate(unique_concepts[:15], 1):
                    print(f"   {i:2d}. {concept}")
                if len(unique_concepts) > 15:
                    print(f"   ... è¿˜æœ‰ {len(unique_concepts) - 15} ä¸ªæ¦‚å¿µ")
            
            # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
            config_used = result.get("config_used", {})
            print(f"\nâš™ï¸ ä½¿ç”¨çš„é…ç½®:")
            for key, value in config_used.items():
                print(f"   - {key}: {value}")
            
            # æ˜¾ç¤ºå®éªŒçŠ¶æ€
            summary = experiment_manager.get_experiment_summary()
            print(f"\nğŸ§ª å®éªŒçŠ¶æ€:")
            print(f"   - å®éªŒID: {summary['experiment_id']}")
            print(f"   - å·²å®Œæˆæ­¥éª¤: {summary['steps_completed']}/{summary['total_steps']}")
            print(f"   - å½“å‰çŠ¶æ€: {summary['status']}")
            
            # æç¤ºåç»­æ­¥éª¤
            print(f"\nğŸ“‹ åç»­æ­¥éª¤:")
            print(f"   è¿è¡Œä¸‹ä¸€æ­¥: python step3.py {saved_files['txt']}")
            print(f"   æŸ¥çœ‹ç»“æœ: cat {saved_files['txt']}")
            print(f"   æŸ¥çœ‹æ¦‚å¿µ: grep -A 20 'é«˜è´¨é‡æ¦‚å¿µç¤ºä¾‹' {saved_files['txt']}")
                
        else:
            print(f"âŒ æ­¥éª¤2å¤±è´¥: {result.get('error')}")
            
            # å³ä½¿å¤±è´¥ä¹Ÿä¿å­˜é”™è¯¯ä¿¡æ¯
            experiment_manager.save_step_result(
                step_num=2,
                result=result,
                save_formats=['txt']
            )
            sys.exit(1)
            
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        # å¦‚æœæœ‰å®éªŒç®¡ç†å™¨ï¼Œä¿å­˜é”™è¯¯ä¿¡æ¯
        if 'experiment_manager' in locals():
            error_result = {
                "step": 2,
                "step_name": "æ–‡æ¡£åˆ†å—ä¸æ¦‚å¿µæå–",
                "success": False,
                "error": str(e),
                "traceback": traceback.format_exc(),
                "processing_time": 0.0,
                "timestamp": datetime.now().isoformat()
            }
            
            try:
                experiment_manager.save_step_result(2, error_result, ['txt'])
                print(f"ğŸ“„ é”™è¯¯ä¿¡æ¯å·²ä¿å­˜åˆ°å®éªŒç›®å½•: {experiment_manager.experiment_dir}")
            except:
                pass
        
        sys.exit(1)

if __name__ == "__main__":
    main()