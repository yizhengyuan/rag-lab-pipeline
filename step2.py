#!/usr/bin/env python3
"""
æ­¥éª¤2: æ–‡æ¡£åˆ†å— - åŸºäºpipeline0604.py (å®Œå…¨ä¿®å¤ç‰ˆ)
================

ç”¨æ³•: python step2.py step1Out.txt
"""

import sys
import json
from pathlib import Path
from datetime import datetime

# å¯¼å…¥ç°æœ‰çš„å¤„ç†å™¨ç±»
sys.path.append(str(Path(__file__).parent))
from pipeline0604 import StepByStepPipelineProcessor
from llama_index.core import Document
from llama_index.core.schema import TextNode

def convert_result_to_serializable(result):
    """å°†ç»“æœè½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼ - é€’å½’å¤„ç†æ‰€æœ‰å¯¹è±¡"""
    
    def convert_object(obj):
        """é€’å½’è½¬æ¢å¯¹è±¡ä¸ºå¯åºåˆ—åŒ–æ ¼å¼"""
        if isinstance(obj, TextNode):
            # è½¬æ¢TextNodeå¯¹è±¡
            return {
                "chunk_id": obj.node_id,
                "text": obj.text,
                "text_length": len(obj.text),
                "concepts": obj.metadata.get("concepts", []),
                "metadata": dict(obj.metadata)
            }
        elif isinstance(obj, Document):
            # è½¬æ¢Documentå¯¹è±¡
            return {
                "text": obj.text,
                "metadata": dict(obj.metadata)
            }
        elif isinstance(obj, dict):
            # é€’å½’å¤„ç†å­—å…¸
            return {k: convert_object(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            # é€’å½’å¤„ç†åˆ—è¡¨
            return [convert_object(item) for item in obj]
        elif isinstance(obj, tuple):
            # è½¬æ¢å…ƒç»„ä¸ºåˆ—è¡¨
            return [convert_object(item) for item in obj]
        elif isinstance(obj, set):
            # è½¬æ¢é›†åˆä¸ºåˆ—è¡¨
            return [convert_object(item) for item in obj]
        elif hasattr(obj, '__dict__'):
            # å¤„ç†å…¶ä»–æœ‰å±æ€§çš„å¯¹è±¡
            return convert_object(obj.__dict__)
        else:
            # åŸºæœ¬ç±»å‹ç›´æ¥è¿”å›
            return obj
    
    return convert_object(result)

def save_result_as_txt(result, output_file):
    """ä¿å­˜ç»“æœä¸ºtxtæ ¼å¼"""
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("æ­¥éª¤2: æ–‡æ¡£åˆ†å— - å¤„ç†ç»“æœ\n")
        f.write("="*80 + "\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"å¤„ç†çŠ¶æ€: {'âœ… æˆåŠŸ' if result.get('success') else 'âŒ å¤±è´¥'}\n")
        
        if result.get("success"):
            stats = result.get("statistics", {})
            
            f.write(f"\nğŸ“Š åˆ†å—ç»Ÿè®¡ä¿¡æ¯:\n")
            f.write(f"- æ€»åˆ†å—æ•°: {stats.get('total_chunks', 0)}\n")
            f.write(f"- å¹³å‡åˆ†å—é•¿åº¦: {stats.get('avg_chunk_length', 0):.1f} å­—ç¬¦\n")
            f.write(f"- æœ€çŸ­åˆ†å—: {stats.get('min_chunk_length', 0)} å­—ç¬¦\n")
            f.write(f"- æœ€é•¿åˆ†å—: {stats.get('max_chunk_length', 0)} å­—ç¬¦\n")
            f.write(f"- æ€»æ¦‚å¿µæ•°: {stats.get('total_concepts', 0)}\n")
            f.write(f"- å”¯ä¸€æ¦‚å¿µæ•°: {stats.get('unique_concepts', 0)}\n")
            f.write(f"- å¹³å‡æ¯åˆ†å—æ¦‚å¿µæ•°: {stats.get('avg_concepts_per_chunk', 0):.1f}\n")
            f.write(f"- å¤„ç†æ—¶é—´: {result.get('processing_time', 0):.2f} ç§’\n")
            
            # æ˜¾ç¤ºåˆ†å—è¯¦æƒ… - å®‰å…¨å¤„ç†chunks
            chunks = result.get("chunks", [])
            f.write(f"\nğŸ“„ åˆ†å—è¯¦ç»†ä¿¡æ¯:\n")
            f.write("-" * 60 + "\n")
            
            for i, chunk in enumerate(chunks[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
                # å®‰å…¨è·å–chunkä¿¡æ¯
                if isinstance(chunk, TextNode):
                    chunk_id = chunk.node_id
                    text_length = len(chunk.text)
                    concepts = chunk.metadata.get("concepts", [])
                    text_preview = chunk.text[:100]
                elif isinstance(chunk, dict):
                    chunk_id = chunk.get('chunk_id', 'æœªçŸ¥')
                    text_length = chunk.get('text_length', 0)
                    concepts = chunk.get('concepts', [])
                    text_preview = chunk.get('text', '')[:100]
                else:
                    chunk_id = f"chunk_{i}"
                    text_length = 0
                    concepts = []
                    text_preview = str(chunk)[:100]
                
                f.write(f"\nåˆ†å— {i}:\n")
                f.write(f"  ID: {chunk_id}\n")
                f.write(f"  é•¿åº¦: {text_length} å­—ç¬¦\n")
                f.write(f"  æ¦‚å¿µæ•°: {len(concepts)}\n")
                f.write(f"  æ¦‚å¿µ: {concepts}\n")
                f.write(f"  å†…å®¹é¢„è§ˆ: {text_preview}...\n")
                f.write("-" * 40 + "\n")
            
            if len(chunks) > 10:
                f.write(f"\n... è¿˜æœ‰ {len(chunks) - 10} ä¸ªåˆ†å— (è¯¦ç»†ä¿¡æ¯è§JSONæ•°æ®)\n")
            
            # æ˜¾ç¤ºæ‰€æœ‰æ¦‚å¿µ
            unique_concepts = result.get("unique_concepts", [])
            f.write(f"\nğŸ§  æå–çš„æ¦‚å¿µåˆ—è¡¨ ({len(unique_concepts)} ä¸ª):\n")
            f.write("-" * 60 + "\n")
            for i, concept in enumerate(unique_concepts, 1):
                f.write(f"{i:3d}. {concept}\n")
                
        else:
            f.write(f"\nâŒ é”™è¯¯ä¿¡æ¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}\n")
        
        # ğŸ”§ ä¿®å¤ï¼šå®Œå…¨è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        try:
            json_result = convert_result_to_serializable(result)
            
            # åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ æœºå™¨å¯è¯»çš„æ•°æ®
            f.write(f"\n" + "="*80 + "\n")
            f.write("# æœºå™¨å¯è¯»æ•°æ® (è¯·å‹¿æ‰‹åŠ¨ä¿®æ”¹)\n")
            f.write("# JSON_DATA_START\n")
            f.write(json.dumps(json_result, ensure_ascii=False, indent=2))
            f.write("\n# JSON_DATA_END\n")
            
        except Exception as json_error:
            f.write(f"\nâš ï¸ JSONåºåˆ—åŒ–å¤±è´¥: {json_error}\n")
            f.write("# ç®€åŒ–çš„æœºå™¨å¯è¯»æ•°æ®\n")
            f.write("# JSON_DATA_START\n")
            simplified_result = {
                "step": result.get("step", 2),
                "success": result.get("success", False),
                "error": result.get("error", ""),
                "processing_time": result.get("processing_time", 0),
                "statistics": result.get("statistics", {}),
                "timestamp": datetime.now().isoformat()
            }
            f.write(json.dumps(simplified_result, ensure_ascii=False, indent=2))
            f.write("\n# JSON_DATA_END\n")

def load_result_from_txt(input_file):
    """ä»txtæ–‡ä»¶ä¸­åŠ è½½ç»“æœæ•°æ®"""
    with open(input_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    start_marker = "# JSON_DATA_START\n"
    end_marker = "\n# JSON_DATA_END"
    
    start_idx = content.find(start_marker)
    end_idx = content.find(end_marker)
    
    if start_idx == -1 or end_idx == -1:
        raise ValueError("æ— æ³•ä»txtæ–‡ä»¶ä¸­è§£ææ•°æ®")
    
    json_str = content[start_idx + len(start_marker):end_idx]
    return json.loads(json_str)

def main():
    if len(sys.argv) != 2:
        print("ç”¨æ³•: python step2.py <step1è¾“å‡ºæ–‡ä»¶>")
        sys.exit(1)
    
    input_file = sys.argv[1]
    output_file = "step2Out.txt"
    
    print(f"ğŸ”„ æ­¥éª¤2: æ–‡æ¡£åˆ†å—")
    print(f"ğŸ“„ è¾“å…¥: {input_file}")
    print(f"ğŸ“„ è¾“å‡º: {output_file}")
    
    try:
        # ä»txtæ–‡ä»¶è¯»å–step1çš„ç»“æœ
        step1_result = load_result_from_txt(input_file)
        
        if not step1_result.get("success"):
            print("âŒ è¾“å…¥æ–‡ä»¶æ˜¾ç¤ºstep1å¤±è´¥")
            sys.exit(1)
        
        # é‡å»ºDocumentå¯¹è±¡
        doc_data = step1_result.get("document", {})
        if isinstance(doc_data, dict):
            document = Document(
                text=doc_data.get("text", ""),
                metadata=doc_data.get("metadata", {})
            )
        else:
            print("âŒ æ— æ³•ä»step1ç»“æœä¸­é‡å»ºDocumentå¯¹è±¡")
            sys.exit(1)
        
        print(f"âœ… æˆåŠŸé‡å»ºDocumentå¯¹è±¡ï¼Œæ–‡æœ¬é•¿åº¦: {len(document.text):,} å­—ç¬¦")
        
        # æ›´æ–°step1_resultä¸­çš„documentä¸ºçœŸæ­£çš„Documentå¯¹è±¡
        step1_result["document"] = document
        
        # ä½¿ç”¨ç°æœ‰çš„å¤„ç†å™¨
        processor = StepByStepPipelineProcessor(
            input_file="dummy",
            output_dir="temp_step2"
        )
        
        # æ‰‹åŠ¨è®¾ç½®step1ç»“æœ
        processor.step_results["step1"] = step1_result
        
        print("ğŸ”§ å¼€å§‹æ‰§è¡Œæ–‡æ¡£åˆ†å—...")
        
        # è°ƒç”¨ç°æœ‰çš„æ–¹æ³•
        result = processor.step2_document_chunking()
        
        # ä¿å­˜ç»“æœä¸ºtxt
        save_result_as_txt(result, output_file)
        
        if result.get("success"):
            print(f"âœ… æ­¥éª¤2å®Œæˆ: {output_file}")
            
            # æ˜¾ç¤ºç®€è¦ç»Ÿè®¡
            stats = result.get("statistics", {})
            print(f"ğŸ“Š åˆ†å—ç»“æœ:")
            print(f"   - æ€»åˆ†å—æ•°: {stats.get('total_chunks', 0)}")
            print(f"   - å¹³å‡åˆ†å—é•¿åº¦: {stats.get('avg_chunk_length', 0):.1f} å­—ç¬¦")
            print(f"   - æå–æ¦‚å¿µæ•°: {stats.get('total_concepts', 0)} (å”¯ä¸€: {stats.get('unique_concepts', 0)})")
            print(f"   - å¤„ç†æ—¶é—´: {result.get('processing_time', 0):.2f} ç§’")
            
            # æ˜¾ç¤ºæ¦‚å¿µç¤ºä¾‹
            unique_concepts = result.get("unique_concepts", [])
            if unique_concepts:
                print(f"ğŸ“ æ¦‚å¿µç¤ºä¾‹ (å‰10ä¸ª):")
                for i, concept in enumerate(unique_concepts[:10], 1):
                    print(f"   {i}. {concept}")
                    
        else:
            print(f"âŒ æ­¥éª¤2å¤±è´¥: {result.get('error')}")
            sys.exit(1)
            
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        # ä¿å­˜é”™è¯¯ä¿¡æ¯
        error_result = {
            "step": 2,
            "step_name": "æ–‡æ¡£åˆ†å—",
            "success": False,
            "error": str(e),
            "processing_time": 0.0,
            "timestamp": datetime.now().isoformat()
        }
        
        try:
            save_result_as_txt(error_result, output_file)
            print(f"ğŸ“„ é”™è¯¯ä¿¡æ¯å·²ä¿å­˜åˆ°: {output_file}")
        except:
            pass
        
        sys.exit(1)

if __name__ == "__main__":
    main()
