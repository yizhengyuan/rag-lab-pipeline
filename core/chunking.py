"""
è¯­ä¹‰åˆ†å—å’Œæ¦‚å¿µæå–æ¨¡å—
"""

import json
import logging
import hashlib
import pickle
import os
import tiktoken  # æ·»åŠ tiktokenç”¨äºtokenè®¡æ•°
from typing import List, Dict, Any
from pathlib import Path
from datetime import datetime, timedelta
from llama_index.core import Document, Settings, VectorStoreIndex
from llama_index.core.node_parser import SemanticSplitterNodeParser, SentenceSplitter
from llama_index.core.schema import TextNode
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

logger = logging.getLogger(__name__)

class EmbeddingCache:
    """Embeddingç¼“å­˜ç®¡ç†å™¨ï¼Œé¿å…é‡å¤è°ƒç”¨API"""
    
    def __init__(self, cache_dir: str, expiry_days: int = 30):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•
            expiry_days: ç¼“å­˜è¿‡æœŸå¤©æ•°
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.expiry_days = expiry_days
        self.cache_file = self.cache_dir / "embedding_cache.pkl"
        self.metadata_file = self.cache_dir / "cache_metadata.json"
        
        # åŠ è½½ç¼“å­˜
        self.cache = self._load_cache()
        self.metadata = self._load_metadata()
    
    def get_text_hash(self, text: str) -> str:
        """è·å–æ–‡æœ¬çš„å“ˆå¸Œå€¼ä½œä¸ºç¼“å­˜é”®"""
        return hashlib.sha256(text.encode('utf-8')).hexdigest()[:16]
    
    def is_cached(self, text: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦å·²ç¼“å­˜ä¸”æœªè¿‡æœŸ"""
        text_hash = self.get_text_hash(text)
        
        if text_hash not in self.cache:
            return False
        
        # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
        cached_time = self.metadata.get(text_hash, {}).get('cached_at')
        if not cached_time:
            return False
        
        cached_date = datetime.fromisoformat(cached_time)
        if datetime.now() - cached_date > timedelta(days=self.expiry_days):
            # è¿‡æœŸï¼Œåˆ é™¤ç¼“å­˜
            del self.cache[text_hash]
            del self.metadata[text_hash]
            return False
        
        return True
    
    def get_embedding(self, text: str) -> List[float]:
        """è·å–ç¼“å­˜çš„embedding"""
        text_hash = self.get_text_hash(text)
        return self.cache.get(text_hash)
    
    def cache_embedding(self, text: str, embedding: List[float]):
        """ç¼“å­˜embedding"""
        text_hash = self.get_text_hash(text)
        self.cache[text_hash] = embedding
        self.metadata[text_hash] = {
            'cached_at': datetime.now().isoformat(),
            'text_length': len(text),
            'text_preview': text[:100]
        }
        
        # å®šæœŸä¿å­˜ï¼ˆæ¯10ä¸ªæ–°ç¼“å­˜ä¿å­˜ä¸€æ¬¡ï¼‰
        if len(self.cache) % 10 == 0:
            self.save_cache()
    
    def save_cache(self):
        """ä¿å­˜ç¼“å­˜åˆ°æ–‡ä»¶"""
        try:
            with open(self.cache_file, 'wb') as f:
                pickle.dump(self.cache, f)
            
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, ensure_ascii=False, indent=2)
            
            logger.debug(f"ğŸ’¾ embeddingç¼“å­˜å·²ä¿å­˜: {len(self.cache)}æ¡è®°å½•")
        except Exception as e:
            logger.warning(f"ä¿å­˜embeddingç¼“å­˜å¤±è´¥: {e}")
    
    def _load_cache(self) -> Dict[str, List[float]]:
        """åŠ è½½ç¼“å­˜"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'rb') as f:
                    cache = pickle.load(f)
                logger.info(f"ğŸ“‚ åŠ è½½embeddingç¼“å­˜: {len(cache)}æ¡è®°å½•")
                return cache
            except Exception as e:
                logger.warning(f"åŠ è½½embeddingç¼“å­˜å¤±è´¥: {e}")
        
        return {}
    
    def _load_metadata(self) -> Dict[str, Dict]:
        """åŠ è½½å…ƒæ•°æ®"""
        if self.metadata_file.exists():
            try:
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"åŠ è½½ç¼“å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
        
        return {}
    
    def clear_expired(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        expired_keys = []
        now = datetime.now()
        
        for text_hash, meta in self.metadata.items():
            cached_time = meta.get('cached_at')
            if cached_time:
                cached_date = datetime.fromisoformat(cached_time)
                if now - cached_date > timedelta(days=self.expiry_days):
                    expired_keys.append(text_hash)
        
        for key in expired_keys:
            self.cache.pop(key, None)
            self.metadata.pop(key, None)
        
        if expired_keys:
            logger.info(f"ğŸ—‘ï¸ æ¸…ç†è¿‡æœŸembeddingç¼“å­˜: {len(expired_keys)}æ¡")
            self.save_cache()
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_size = sum(len(emb) * 8 for emb in self.cache.values())  # ä¼°ç®—å­—èŠ‚æ•°
        
        return {
            "total_entries": len(self.cache),
            "estimated_size_mb": total_size / (1024 * 1024),
            "cache_directory": str(self.cache_dir),
            "expiry_days": self.expiry_days
        }

class SemanticChunker:
    """è¯­ä¹‰åˆ†å—å™¨ - ä½¿ç”¨ LlamaIndex çš„ SemanticSplitterNodeParser"""
    
    def __init__(self, config):
        """
        åˆå§‹åŒ–è¯­ä¹‰åˆ†å—å™¨
        
        Args:
            config: é…ç½®å¯¹è±¡
        """
        self.config = config
        
        # ğŸ”‘ æ–°å¢ï¼štokené™åˆ¶é…ç½® - æ›´æ–°é…ç½®è¯»å–è·¯å¾„
        self.max_tokens_per_chunk = config.get("chunking.max_tokens_per_chunk", 6000)  # ä¿ç•™ä¸€äº›ä½™é‡
        self.max_char_per_chunk = config.get("chunking.max_char_per_chunk", 24000)
        self.min_chunk_length = config.get("chunking.min_chunk_length", 10)
        self.skip_oversized = config.get("chunking.skip_oversized_chunks", False)
        self.enable_validation = config.get("chunking.enable_token_validation", True)
        
        self.embedding_model_name = config.get("api.embedding_model", "text-embedding-ada-002")
        
        # åˆå§‹åŒ–tokenç¼–ç å™¨
        try:
            self.tokenizer = tiktoken.encoding_for_model("gpt-4")  # ä½¿ç”¨é€šç”¨çš„ç¼–ç å™¨
            logger.info(f"âœ… Tokenç¼–ç å™¨åˆå§‹åŒ–æˆåŠŸï¼Œæœ€å¤§tokens: {self.max_tokens_per_chunk}")
        except Exception as e:
            logger.warning(f"Tokenç¼–ç å™¨åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨å­—ç¬¦é•¿åº¦ä¼°ç®—")
            self.tokenizer = None
        
        # ğŸ”‘ æ–°å¢ï¼šç¡®ä¿LlamaIndexè®¾ç½®æ­£ç¡®åˆå§‹åŒ–
        self._setup_llamaindex_settings()
        
        # ğŸ†• åˆå§‹åŒ–embeddingç¼“å­˜
        if config.get("vector_store.enable_embedding_cache", True):
            cache_dir = config.get("vector_store.embedding_cache_dir", "./embedding_cache")
            expiry_days = config.get("vector_store.cache_expiry_days", 30)
            self.embedding_cache = EmbeddingCache(cache_dir, expiry_days)
            logger.info(f"âœ… Embeddingç¼“å­˜å·²å¯ç”¨: {cache_dir}")
        else:
            self.embedding_cache = None
            logger.info("âŒ Embeddingç¼“å­˜å·²ç¦ç”¨")
        
        # åˆå§‹åŒ– LlamaIndex çš„è¯­ä¹‰åˆ†å—å™¨
        self.semantic_splitter = SemanticSplitterNodeParser(
            buffer_size=config.get("chunking.buffer_size", 1),
            breakpoint_percentile_threshold=config.get("chunking.breakpoint_percentile_threshold", 95),
            embed_model=Settings.embed_model
        )
        
        # ğŸ†• æ·»åŠ å¤‡ç”¨åˆ†å‰²å™¨ï¼Œç”¨äºå¤„ç†è¶…é•¿chunk
        self.fallback_splitter = SentenceSplitter(
            chunk_size=self.max_tokens_per_chunk,
            chunk_overlap=config.get("chunking.fallback_chunk_overlap", 200)
        )
        
        self.chunk_nodes: List[TextNode] = []
        self.chunk_index: VectorStoreIndex = None
    
    def _setup_llamaindex_settings(self):
        """ğŸ”‘ æ–°å¢ï¼šåˆå§‹åŒ–LlamaIndexè®¾ç½®"""
        try:
            # é…ç½®LLM
            api_key = self.config.get('api.openai_key')
            base_url = self.config.get('api.base_url')
            model_name = self.config.get('api.model', 'gpt-4o-mini')
            
            if not api_key:
                raise ValueError("ç¼ºå°‘APIå¯†é’¥")
            
            logger.info(f"ğŸ”§ åˆå§‹åŒ–LLM: {model_name}")
            logger.info(f"ğŸ”§ APIåœ°å€: {base_url}")
            
            llm_kwargs = {
                "model": model_name,
                "api_key": api_key,
                "temperature": self.config.get('api.temperature', 0.1)
            }
            
            if base_url:
                llm_kwargs["api_base"] = base_url
            
            Settings.llm = OpenAI(**llm_kwargs)
            logger.info(f"âœ… LLMåˆå§‹åŒ–æˆåŠŸ: {model_name}")
            
            # é…ç½®åµŒå…¥æ¨¡å‹
            embed_kwargs = {
                "model": self.config.get('api.embedding_model', 'text-embedding-ada-002'),
                "api_key": api_key
            }
            
            if base_url:
                embed_kwargs["api_base"] = base_url
            
            Settings.embed_model = OpenAIEmbedding(**embed_kwargs)
            logger.info("âœ… åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            
            # ğŸ”— æµ‹è¯•LLMè¿æ¥
            test_response = Settings.llm.complete("æµ‹è¯•")
            logger.info(f"âœ… LLMè¿æ¥æµ‹è¯•æˆåŠŸ: {len(test_response.text)} å­—ç¬¦")
            
        except Exception as e:
            logger.error(f"âŒ LlamaIndexè®¾ç½®å¤±è´¥: {e}")
            logger.warning("âš ï¸ å°†ä½¿ç”¨ç®€å•å…³é”®è¯æå–ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ")
            Settings.llm = None  # ç¡®ä¿åç»­èƒ½æ­£ç¡®æ£€æµ‹å¤±è´¥

    def _count_tokens(self, text: str) -> int:
        """
        ğŸ†• è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            int: tokenæ•°é‡
        """
        if self.tokenizer:
            try:
                return len(self.tokenizer.encode(text))
            except Exception as e:
                logger.warning(f"Tokenè®¡æ•°å¤±è´¥: {e}ï¼Œä½¿ç”¨å­—ç¬¦é•¿åº¦ä¼°ç®—")
        
        # ä½¿ç”¨å­—ç¬¦é•¿åº¦ä¼°ç®—ï¼ˆ1 token â‰ˆ 4 å­—ç¬¦ï¼‰
        return len(text) // 4
    
    def _split_oversized_chunk(self, node: TextNode) -> List[TextNode]:
        """
        ğŸ†• åˆ†å‰²è¶…å¤§çš„chunk
        
        Args:
            node: è¶…å¤§çš„æ–‡æœ¬èŠ‚ç‚¹
            
        Returns:
            List[TextNode]: åˆ†å‰²åçš„èŠ‚ç‚¹åˆ—è¡¨
        """
        token_count = self._count_tokens(node.text)
        if token_count <= self.max_tokens_per_chunk:
            return [node]
        
        logger.warning(f"âš ï¸ Chunkè¿‡å¤§ ({token_count} tokens)ï¼Œè¿›è¡Œåˆ†å‰²...")
        
        # ä½¿ç”¨å¥å­åˆ†å‰²å™¨è¿›ä¸€æ­¥åˆ†å‰²
        doc = Document(text=node.text, metadata=node.metadata.copy())
        sub_nodes = self.fallback_splitter.get_nodes_from_documents([doc])
        
        # æ›´æ–°metadata
        for i, sub_node in enumerate(sub_nodes):
            original_chunk_id = node.metadata.get("chunk_id", "unknown")
            sub_node.metadata.update(node.metadata)
            sub_node.metadata["chunk_id"] = f"{original_chunk_id}_sub_{i}"
            sub_node.metadata["is_split"] = True
            sub_node.metadata["original_chunk_id"] = original_chunk_id
            sub_node.metadata["sub_chunk_index"] = i
            sub_node.metadata["total_sub_chunks"] = len(sub_nodes)
        
        logger.info(f"   ğŸ“ åˆ†å‰²ä¸º {len(sub_nodes)} ä¸ªå­chunk")
        return sub_nodes
    
    def _validate_chunk_sizes(self, nodes: List[TextNode]) -> List[TextNode]:
        """
        ğŸ†• éªŒè¯å¹¶ä¿®å¤chunkå¤§å°
        
        Args:
            nodes: èŠ‚ç‚¹åˆ—è¡¨
            
        Returns:
            List[TextNode]: éªŒè¯å’Œä¿®å¤åçš„èŠ‚ç‚¹åˆ—è¡¨
        """
        if not self.enable_validation:
            logger.info("âš ï¸ TokenéªŒè¯å·²ç¦ç”¨ï¼Œè·³è¿‡chunkå¤§å°æ£€æŸ¥")
            return nodes
        
        valid_nodes = []
        oversized_count = 0
        skipped_count = 0
        
        for node in nodes:
            # æ£€æŸ¥æœ€å°é•¿åº¦
            if len(node.text.strip()) < self.min_chunk_length:
                skipped_count += 1
                logger.debug(f"è·³è¿‡è¿‡çŸ­chunk: {len(node.text)} å­—ç¬¦")
                continue
            
            token_count = self._count_tokens(node.text)
            char_count = len(node.text)
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
            if token_count > self.max_tokens_per_chunk or char_count > self.max_char_per_chunk:
                oversized_count += 1
                
                if self.skip_oversized:
                    logger.warning(f"âš ï¸ è·³è¿‡è¶…å¤§chunk: {token_count} tokens, {char_count} å­—ç¬¦")
                    skipped_count += 1
                    continue
                else:
                    # åˆ†å‰²è¶…å¤§chunk
                    split_nodes = self._split_oversized_chunk(node)
                    valid_nodes.extend(split_nodes)
            else:
                valid_nodes.append(node)
        
        if oversized_count > 0:
            action = "è·³è¿‡" if self.skip_oversized else "åˆ†å‰²"
            logger.info(f"ğŸ”§ {action}äº† {oversized_count} ä¸ªè¶…å¤§chunk")
        
        if skipped_count > 0:
            logger.info(f"âš ï¸ è·³è¿‡äº† {skipped_count} ä¸ªä¸ç¬¦åˆè¦æ±‚çš„chunk")
        
        logger.info(f"âœ… æœ€ç»ˆå¾—åˆ° {len(valid_nodes)} ä¸ªæœ‰æ•ˆchunk")
        
        # æœ€ç»ˆéªŒè¯
        if self.enable_validation:
            failed_nodes = 0
            for node in valid_nodes:
                token_count = self._count_tokens(node.text)
                if token_count > self.max_tokens_per_chunk:
                    logger.error(f"âŒ ä»æœ‰chunkè¶…è¿‡é™åˆ¶: {token_count} tokens")
                    failed_nodes += 1
            
            if failed_nodes == 0:
                logger.info(f"âœ… æ‰€æœ‰chunkéƒ½åœ¨tokené™åˆ¶å†…")
        
        return valid_nodes

    def _preprocess_large_documents(self, documents: List[Document]) -> List[Document]:
        """
        ğŸ†• é¢„å¤„ç†è¶…å¤§æ–‡æ¡£ï¼Œç¡®ä¿è¾“å…¥åˆ°è¯­ä¹‰åˆ†å—å™¨çš„æ–‡æ¡£å¤§å°åˆé€‚
        
        Args:
            documents: åŸå§‹æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            List[Document]: é¢„å¤„ç†åçš„å®‰å…¨æ–‡æ¡£åˆ—è¡¨
        """
        safe_docs = []
        split_count = 0
        
        for doc_idx, doc in enumerate(documents):
            char_count = len(doc.text)
            token_count = self._count_tokens(doc.text)
            
            # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦è¿‡å¤§ï¼ˆä½¿ç”¨æ›´ä¿å®ˆçš„é™åˆ¶ï¼‰
            safe_token_limit = self.max_tokens_per_chunk // 2  # 3000 tokensï¼Œä¸ºè¯­ä¹‰åˆ†å—ç•™ä½™é‡
            safe_char_limit = self.max_char_per_chunk // 2     # 12000 å­—ç¬¦
            
            if token_count > safe_token_limit or char_count > safe_char_limit:
                logger.warning(f"âš ï¸ æ–‡æ¡£ {doc_idx} è¿‡å¤§ ({token_count} tokens, {char_count} å­—ç¬¦)ï¼Œè¿›è¡Œé¢„åˆ†å—...")
                split_count += 1
                
                # ä½¿ç”¨ç®€å•åˆ†å‰²å™¨è¿›è¡Œé¢„åˆ†å—
                temp_doc = Document(text=doc.text, metadata=doc.metadata.copy())
                pre_nodes = self.fallback_splitter.get_nodes_from_documents([temp_doc])
                
                # è½¬æ¢å›Documentå¯¹è±¡
                for i, node in enumerate(pre_nodes):
                    pre_doc = Document(
                        text=node.text,
                        metadata={
                            **doc.metadata,
                            "pre_split": True,
                            "original_doc_index": doc_idx,
                            "pre_split_index": i,
                            "total_pre_splits": len(pre_nodes)
                        }
                    )
                    safe_docs.append(pre_doc)
            else:
                # æ–‡æ¡£å¤§å°å®‰å…¨ï¼Œç›´æ¥ä½¿ç”¨
                safe_docs.append(doc)
        
        logger.info(f"ğŸ“„ é¢„å¤„ç†å®Œæˆ: {len(documents)} â†’ {len(safe_docs)} ä¸ªæ–‡æ¡£")
        if split_count > 0:
            logger.info(f"ğŸ”§ åˆ†å‰²äº† {split_count} ä¸ªè¶…å¤§æ–‡æ¡£")
        
        return safe_docs

    def chunk_and_extract_concepts(self, documents: List[Document]) -> List[TextNode]:
        """
        è¯­ä¹‰åˆ†å—å¹¶æå–æ¦‚å¿µ
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            List[TextNode]: åŒ…å«æ¦‚å¿µçš„æ–‡æœ¬èŠ‚ç‚¹åˆ—è¡¨
        """
        logger.info("å¼€å§‹è¯­ä¹‰åˆ†å—å’Œæ¦‚å¿µæå–")
        
        # éªŒè¯è¾“å…¥
        if not documents:
            logger.warning("æ–‡æ¡£åˆ—è¡¨ä¸ºç©º")
            return []
        
        # ğŸ†• æ£€æŸ¥æ˜¯å¦å·²æœ‰ç›¸åŒæ–‡æ¡£çš„å‘é‡æ•°æ®
        if self._should_skip_processing(documents):
            logger.info("ğŸ¯ æ£€æµ‹åˆ°ç›¸åŒæ–‡æ¡£å·²å¤„ç†ï¼Œå°è¯•åŠ è½½ç°æœ‰å‘é‡æ•°æ®")
            return self.chunk_nodes
        
        # ğŸ”‘ å…³é”®ä¿®æ”¹ï¼šé¢„å¤„ç†è¶…å¤§æ–‡æ¡£
        logger.info("ğŸ”„ é¢„å¤„ç†è¶…å¤§æ–‡æ¡£...")
        safe_documents = self._preprocess_large_documents(documents)
        
        # ä½¿ç”¨ LlamaIndex çš„è¯­ä¹‰åˆ†å—ï¼ˆç°åœ¨è¾“å…¥æ˜¯å®‰å…¨çš„ï¼‰
        logger.info("ğŸ”„ æ‰§è¡Œè¯­ä¹‰åˆ†å—...")
        nodes = self.semantic_splitter.get_nodes_from_documents(safe_documents)  # è¿™é‡Œç”¨safe_documents
        
        if not nodes:
            logger.warning("è¯­ä¹‰åˆ†å—æœªäº§ç”Ÿä»»ä½•èŠ‚ç‚¹")
            return []
        
        logger.info(f"ğŸ“„ åˆå§‹åˆ†å—å®Œæˆ: {len(nodes)} ä¸ªchunk")
        
        # ğŸ†• éªŒè¯å’Œä¿®å¤chunkå¤§å°
        logger.info("ğŸ” éªŒè¯chunkå¤§å°...")
        nodes = self._validate_chunk_sizes(nodes)
        
        # ğŸ†• åœ¨åˆ›å»ºå‘é‡ç´¢å¼•å‰è¿›è¡Œæœ€åæ£€æŸ¥
        logger.info("ğŸ” åˆ›å»ºå‘é‡ç´¢å¼•å‰çš„å®‰å…¨æ£€æŸ¥...")
        safe_nodes = []
        skipped_nodes = 0
        
        for i, node in enumerate(nodes):
            token_count = self._count_tokens(node.text)
            
            if token_count > self.max_tokens_per_chunk:
                logger.warning(f"âš ï¸ è·³è¿‡è¶…å¤§chunk {i}: {token_count} tokens")
                skipped_nodes += 1
                continue
            
            if len(node.text.strip()) < 10:
                logger.warning(f"âš ï¸ è·³è¿‡è¿‡çŸ­chunk {i}: {len(node.text)} å­—ç¬¦")
                skipped_nodes += 1
                continue
            
            safe_nodes.append(node)
        
        if skipped_nodes > 0:
            logger.warning(f"âš ï¸ è·³è¿‡äº† {skipped_nodes} ä¸ªä¸å®‰å…¨çš„chunk")
        
        logger.info(f"âœ… å®‰å…¨æ£€æŸ¥å®Œæˆ: {len(safe_nodes)} ä¸ªchunkå‡†å¤‡åˆ›å»ºç´¢å¼•")
        
        # ğŸ” å®‰å…¨åœ°åˆ›å»ºå‘é‡ç´¢å¼•
        try:
            logger.info("ğŸš€ å¼€å§‹åˆ›å»ºå‘é‡ç´¢å¼•...")
            if safe_nodes:
                self.chunk_index = VectorStoreIndex(safe_nodes)
                logger.info("âœ… å‘é‡ç´¢å¼•åˆ›å»ºæˆåŠŸ")
            else:
                logger.warning("âŒ æ²¡æœ‰å®‰å…¨çš„chunkå¯ä»¥åˆ›å»ºç´¢å¼•")
                return []
        except Exception as e:
            logger.error(f"âŒ å‘é‡ç´¢å¼•åˆ›å»ºå¤±è´¥: {e}")
            # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•é€ä¸ªæ·»åŠ èŠ‚ç‚¹
            logger.info("ğŸ”„ å°è¯•é€ä¸ªå¤„ç†chunk...")
            return self._create_index_safely(safe_nodes)
        
        # ä¸ºæ¯ä¸ª node æå–æ¦‚å¿µ
        for i, node in enumerate(safe_nodes):
            logger.info(f"æ­£åœ¨å¤„ç† chunk {i+1}/{len(safe_nodes)}")
            logger.info(f"chunk å†…å®¹: {node.text}")
            
            # æå–æ¦‚å¿µå¹¶å­˜å‚¨åˆ° metadata ä¸­
            concepts = self._extract_chunk_concepts(node.text)
            
            # ğŸ”‘ ä¿®æ­£ï¼šç›´æ¥å­˜å‚¨ä¸ºlistç±»å‹
            node.metadata["concepts"] = concepts if concepts else []
            node.metadata["chunk_id"] = f"chunk_{i}"
            node.metadata["chunk_length"] = len(node.text)
            node.metadata["concept_count"] = len(concepts)
            node.metadata["token_count"] = self._count_tokens(node.text)
        
        self.chunk_nodes = safe_nodes
        logger.info(f"å®Œæˆè¯­ä¹‰åˆ†å—: å…±å¤„ç† {len(self.chunk_nodes)} ä¸ª chunks")
        
        # ğŸ†• ä¿å­˜embeddingç¼“å­˜
        if self.embedding_cache:
            self.embedding_cache.save_cache()
            cache_stats = self.embedding_cache.get_cache_stats()
            logger.info(f"ğŸ’¾ ç¼“å­˜ç»Ÿè®¡: {cache_stats['total_entries']}æ¡è®°å½•, "
                       f"{cache_stats['estimated_size_mb']:.1f}MB")
        
        return self.chunk_nodes
    
    def _create_index_safely(self, nodes: List[TextNode]) -> List[TextNode]:
        """
        ğŸ†• å®‰å…¨åœ°åˆ›å»ºç´¢å¼•ï¼Œé€ä¸ªå¤„ç†èŠ‚ç‚¹
        
        Args:
            nodes: èŠ‚ç‚¹åˆ—è¡¨
            
        Returns:
            List[TextNode]: æˆåŠŸå¤„ç†çš„èŠ‚ç‚¹åˆ—è¡¨
        """
        successful_nodes = []
        
        for i, node in enumerate(nodes):
            try:
                # å°è¯•ä¸ºå•ä¸ªèŠ‚ç‚¹åˆ›å»ºå°ç´¢å¼•ä»¥éªŒè¯
                test_index = VectorStoreIndex([node])
                successful_nodes.append(node)
                logger.debug(f"âœ… èŠ‚ç‚¹ {i} å¤„ç†æˆåŠŸ")
            except Exception as e:
                logger.warning(f"âš ï¸ èŠ‚ç‚¹ {i} å¤„ç†å¤±è´¥: {e}")
                continue
        
        if successful_nodes:
            try:
                self.chunk_index = VectorStoreIndex(successful_nodes)
                logger.info(f"âœ… å®‰å…¨ç´¢å¼•åˆ›å»ºæˆåŠŸ: {len(successful_nodes)} ä¸ªèŠ‚ç‚¹")
            except Exception as e:
                logger.error(f"âŒ å³ä½¿å®‰å…¨å¤„ç†åä»ç„¶å¤±è´¥: {e}")
                return []
        
        return successful_nodes
    
    def _should_skip_processing(self, documents: List[Document]) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡å¤„ç†ï¼ˆæ–‡æ¡£å·²å­˜åœ¨äºå‘é‡æ•°æ®åº“ä¸­ï¼‰
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            bool: æ˜¯å¦è·³è¿‡å¤„ç†
        """
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„æ£€æŸ¥é€»è¾‘
        # ä¾‹å¦‚ï¼šæ ¹æ®æ–‡æ¡£å“ˆå¸Œå€¼æˆ–å…ƒæ•°æ®æ¥åˆ¤æ–­
        
        if not documents:
            return False
        
        # ç®€å•æ£€æŸ¥ï¼šå¦‚æœå½“å‰å·²æœ‰chunk_nodesä¸”æ•°é‡åˆç†ï¼Œå¯èƒ½æ˜¯é‡å¤å¤„ç†
        if self.chunk_nodes and len(self.chunk_nodes) > 0:
            logger.debug("å½“å‰å·²æœ‰chunkæ•°æ®ï¼Œå¯èƒ½æ˜¯é‡å¤å¤„ç†")
            return True
        
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šæ£€æŸ¥é€»è¾‘ï¼š
        # 1. æ£€æŸ¥å‘é‡æ•°æ®åº“ä¸­æ˜¯å¦å·²æœ‰ç›¸åŒæ–‡æ¡£çš„embedding
        # 2. æ£€æŸ¥æ–‡æ¡£å†…å®¹çš„å“ˆå¸Œå€¼
        # 3. æ£€æŸ¥æ–‡æ¡£çš„å…ƒæ•°æ®æ ‡è¯†
        
        return False
    
    def get_document_hash(self, documents: List[Document]) -> str:
        """
        ğŸ†• è·å–æ–‡æ¡£é›†åˆçš„å“ˆå¸Œå€¼ï¼Œç”¨äºè¯†åˆ«é‡å¤æ–‡æ¡£
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            str: æ–‡æ¡£é›†åˆçš„å“ˆå¸Œå€¼
        """
        import hashlib
        
        # å°†æ‰€æœ‰æ–‡æ¡£æ–‡æœ¬åˆå¹¶
        combined_text = ""
        for doc in documents:
            combined_text += doc.text
        
        # ç”Ÿæˆå“ˆå¸Œå€¼
        return hashlib.sha256(combined_text.encode('utf-8')).hexdigest()[:16]
    
    def _extract_chunk_concepts(self, chunk_text: str) -> List[str]:
        """
        ä½¿ç”¨ LlamaIndex çš„ LLM æ¥å£æå–æ¦‚å¿µ
        
        Args:
            chunk_text: chunk æ–‡æœ¬
            
        Returns:
            List[str]: æ¦‚å¿µåˆ—è¡¨
        """
        if not chunk_text or len(chunk_text.strip()) < 10:
            logger.debug("æ–‡æœ¬å¤ªçŸ­ï¼Œè·³è¿‡æ¦‚å¿µæå–")
            return []
        
        # ğŸ” æ–°å¢ï¼šæ£€æŸ¥LLMæ˜¯å¦å¯ç”¨
        if Settings.llm is None:
            logger.warning("LLMæœªåˆå§‹åŒ–ï¼Œä½¿ç”¨ç®€å•å…³é”®è¯æå–")
            return self._simple_keyword_extraction(chunk_text)
        
        num_concepts = self.config.get("concepts.concepts_per_chunk", 5)
        
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨configä¸­çš„è‹±æ–‡æç¤ºè¯æ¨¡æ¿
        prompt_template = self.config.get("prompts.concept_extraction", """
        Extract {num_concepts} core concepts from the following text. Each concept should be a concise English phrase or keyword list.
        
        Text:
        {text}
        
        Please return the concept list in JSON format using English concepts only:
        {{"concepts": ["concept1", "concept2", "concept3"]}}
        """)
        
        prompt = prompt_template.format(
            num_concepts=num_concepts,
            text=chunk_text
        )
        
        try:
            logger.debug(f"ğŸ¤– æ­£åœ¨è°ƒç”¨LLMæå–æ¦‚å¿µ...")
            response = Settings.llm.complete(prompt)
            logger.debug(f"ğŸ¤– LLMå“åº”: {response.text[:100]}...")
            
            result = json.loads(response.text.strip())
            concepts = result.get("concepts", [])
            
            # éªŒè¯å’Œæ¸…ç†æ¦‚å¿µ
            cleaned_concepts = []
            
            for concept in concepts:
                if isinstance(concept, str) and len(concept.strip()) > 0:
                    cleaned_concept = concept.strip()
                    cleaned_concepts.append(cleaned_concept)
            
            logger.debug(f"ğŸ“Š æ¦‚å¿µæå–ç»“æœ: {len(cleaned_concepts)}ä¸ªæ¦‚å¿µ")
            
            return cleaned_concepts[:num_concepts]  # é™åˆ¶æ•°é‡
            
        except json.JSONDecodeError as e:
            logger.warning(f"JSONè§£æå¤±è´¥: {e}, å“åº”å†…å®¹: {response.text[:200]}")
            return self._simple_keyword_extraction(chunk_text)
        except Exception as e:
            logger.warning(f"æ¦‚å¿µæå–å¤±è´¥: {e}")
            return self._simple_keyword_extraction(chunk_text)
    
    def _simple_keyword_extraction(self, text: str) -> List[str]:
        """
        ç®€å•çš„å…³é”®è¯æå–ä½œä¸ºå›é€€æ–¹æ¡ˆ
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            List[str]: å…³é”®è¯åˆ—è¡¨
        """
        if not text:
            return []
        
        # ç®€å•çš„è¯é¢‘ç»Ÿè®¡
        words = text.lower().split()
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        stop_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 
            'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 
            'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 
            'may', 'might', 'can', 'this', 'that', 'these', 'those'
        }
        
        filtered_words = [
            word for word in words 
            if len(word) > 2 and word not in stop_words and word.isalpha()
        ]
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        unique_words = list(set(filtered_words))
        max_keywords = self.config.get("concepts.concepts_per_chunk", 5)
        
        return unique_words[:max_keywords]
    
    def get_chunk_index(self) -> VectorStoreIndex:
        """è·å– chunk ç´¢å¼•"""
        return self.chunk_index
    
    def get_chunking_statistics(self) -> Dict[str, Any]:
        """
        è·å–åˆ†å—ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯
        """
        if not self.chunk_nodes:
            return {"total_chunks": 0}
        
        chunk_lengths = [len(node.text) for node in self.chunk_nodes]
        concept_counts = [len(node.metadata.get("concepts", [])) for node in self.chunk_nodes]
        
        stats = {
            "total_chunks": len(self.chunk_nodes),
            "avg_chunk_length": sum(chunk_lengths) / len(chunk_lengths),
            "min_chunk_length": min(chunk_lengths),
            "max_chunk_length": max(chunk_lengths),
            "total_concepts": sum(concept_counts),
            "avg_concepts_per_chunk": sum(concept_counts) / len(concept_counts) if concept_counts else 0,
            "min_concepts_per_chunk": min(concept_counts) if concept_counts else 0,
            "max_concepts_per_chunk": max(concept_counts) if concept_counts else 0
        }
        
        return stats
    
    def reset(self):
        """é‡ç½®åˆ†å—å™¨çŠ¶æ€"""
        # ğŸ†• ä¿å­˜embeddingç¼“å­˜
        if self.embedding_cache:
            self.embedding_cache.save_cache()
            logger.info("ğŸ’¾ Embeddingç¼“å­˜å·²ä¿å­˜")
        
        self.chunk_nodes = []
        self.chunk_index = None
        logger.info("è¯­ä¹‰åˆ†å—å™¨çŠ¶æ€å·²é‡ç½®")
    
    def get_chunks_by_concept_count(self, min_concepts: int = 1) -> List[TextNode]:
        """
        æ ¹æ®æ¦‚å¿µæ•°é‡è¿‡æ»¤ chunks
        
        Args:
            min_concepts: æœ€å°æ¦‚å¿µæ•°é‡
            
        Returns:
            List[TextNode]: è¿‡æ»¤åçš„ chunks
        """
        filtered_chunks = []
        for node in self.chunk_nodes:
            concept_count = len(node.metadata.get("concepts", []))
            if concept_count >= min_concepts:
                filtered_chunks.append(node)
        
        logger.info(f"è¿‡æ»¤åä¿ç•™ {len(filtered_chunks)} ä¸ª chunks (æœ€å°‘ {min_concepts} ä¸ªæ¦‚å¿µ)")
        return filtered_chunks
    
    def export_chunks_with_concepts(self, output_path: str):
        """
        å¯¼å‡º chunks å’Œæ¦‚å¿µåˆ°æ–‡ä»¶
        
        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        export_data = []
        
        for node in self.chunk_nodes:
            chunk_data = {
                "chunk_id": node.metadata.get("chunk_id"),
                "text": node.text,
                "concepts": node.metadata.get("concepts", []),
                "chunk_length": len(node.text),
                "concept_count": len(node.metadata.get("concepts", []))
            }
            export_data.append(chunk_data)
        
        import json
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"å·²å¯¼å‡º {len(export_data)} ä¸ª chunks åˆ°: {output_path}")
    
    def get_concepts_from_node(self, node) -> List[str]:
        """
        ğŸ†• ä»èŠ‚ç‚¹çš„metadataä¸­æ¢å¤conceptsåˆ—è¡¨
        
        Args:
            node: æ–‡æœ¬èŠ‚ç‚¹
            
        Returns:
            List[str]: æ¦‚å¿µåˆ—è¡¨
        """
        concepts_str = node.metadata.get("concepts", "")
        if concepts_str:
            try:
                return json.loads(concepts_str)
            except json.JSONDecodeError:
                logger.warning("æ— æ³•è§£ææ¦‚å¿µå­—ç¬¦ä¸²ï¼Œè¿”å›ç©ºåˆ—è¡¨")
                return []
        return [] 