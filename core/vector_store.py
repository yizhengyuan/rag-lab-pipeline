"""
å‘é‡å­˜å‚¨ç®¡ç†æ¨¡å—
"""

import os
import json
import logging
import shutil
from typing import List, Optional, Dict, Any, Union
from pathlib import Path
from datetime import datetime
from llama_index.core import VectorStoreIndex, StorageContext, Document
from llama_index.core.schema import TextNode, BaseNode
from llama_index.core.vector_stores.simple import SimpleVectorStore
from llama_index.core.vector_stores.types import VectorStore

# å…ˆå®šä¹‰logger
logger = logging.getLogger(__name__)

# æ·»åŠ Chromaæ”¯æŒ
try:
    from llama_index.vector_stores.chroma import ChromaVectorStore
    import chromadb
    CHROMA_AVAILABLE = True
except ImportError:
    CHROMA_AVAILABLE = False
    logger.warning("Chromaæœªå®‰è£…ï¼Œå°†ä½¿ç”¨SimpleVectorStore")

from .nodes import ConceptNode, EvidenceNode
from utils.helpers import FileHelper

class VectorStoreManager:
    """å‘é‡å­˜å‚¨ç®¡ç†å™¨ - ä½¿ç”¨ LlamaIndex çš„å‘é‡å­˜å‚¨ç³»ç»Ÿ"""
    
    def __init__(self, config):
        """
        åˆå§‹åŒ–å‘é‡å­˜å‚¨ç®¡ç†å™¨
        
        Args:
            config: é…ç½®å¯¹è±¡
        """
        self.config = config
        self.store_type = config.get("vector_store.type", "simple")
        self.persist_directory = config.get("vector_store.persist_directory", "./vector_db")
        self.collection_name = config.get("vector_store.collection_name", "concepts")
        self.dimension = config.get("vector_store.dimension", 1536)
        
        # ç¡®ä¿æŒä¹…åŒ–ç›®å½•å­˜åœ¨
        Path(self.persist_directory).mkdir(parents=True, exist_ok=True)
        
        # Chromaå®¢æˆ·ç«¯
        self.chroma_client = None
        if self.store_type == "chroma" and CHROMA_AVAILABLE:
            self._init_chroma_client()
        
        # å­˜å‚¨ç´¢å¼•
        self.chunk_index: Optional[VectorStoreIndex] = None
        self.concept_index: Optional[VectorStoreIndex] = None
        self.evidence_index: Optional[VectorStoreIndex] = None
        
        # ç´¢å¼•å…ƒæ•°æ®
        self.index_metadata = {
            "chunks": {"created_at": None, "node_count": 0, "last_updated": None},
            "concepts": {"created_at": None, "node_count": 0, "last_updated": None},
            "evidence": {"created_at": None, "node_count": 0, "last_updated": None}
        }
        
        # åŠ è½½å…ƒæ•°æ®
        self._load_metadata()
    
    def _init_chroma_client(self):
        """åˆå§‹åŒ–Chromaå®¢æˆ·ç«¯"""
        try:
            # ä½¿ç”¨æŒä¹…åŒ–å®¢æˆ·ç«¯
            self.chroma_client = chromadb.PersistentClient(path=self.persist_directory)
            logger.info(f"âœ… Chromaå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ: {self.persist_directory}")
        except Exception as e:
            logger.error(f"âŒ Chromaå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.warning("âš ï¸ å°†å›é€€åˆ°SimpleVectorStore")
            self.store_type = "simple"
            self.chroma_client = None
    
    def _get_vector_store(self, collection_suffix: str = "") -> VectorStore:
        """
        è·å–å‘é‡å­˜å‚¨å®ä¾‹
        
        Args:
            collection_suffix: é›†åˆåç§°åç¼€ï¼Œç”¨äºåŒºåˆ†ä¸åŒç±»å‹çš„ç´¢å¼•
        """
        if self.store_type == "chroma" and self.chroma_client and CHROMA_AVAILABLE:
            try:
                collection_name = f"{self.collection_name}_{collection_suffix}" if collection_suffix else self.collection_name
                
                # è·å–æˆ–åˆ›å»ºé›†åˆ
                chroma_collection = self.chroma_client.get_or_create_collection(collection_name)
                
                # åˆ›å»ºChromaVectorStore
                vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
                logger.info(f"âœ… ä½¿ç”¨Chromaå‘é‡å­˜å‚¨: {collection_name}")
                return vector_store
                
            except Exception as e:
                logger.error(f"âŒ åˆ›å»ºChromaå‘é‡å­˜å‚¨å¤±è´¥: {e}")
                logger.warning("âš ï¸ å›é€€åˆ°SimpleVectorStore")
        
        # å›é€€åˆ°SimpleVectorStore
        logger.info("ğŸ“ ä½¿ç”¨SimpleVectorStore")
        return SimpleVectorStore()
    
    def create_chunk_index(self, nodes: List[TextNode], persist: bool = True) -> VectorStoreIndex:
        """
        åˆ›å»ºæ–‡æ¡£å—ç´¢å¼•
        
        Args:
            nodes: æ–‡æœ¬èŠ‚ç‚¹åˆ—è¡¨
            persist: æ˜¯å¦æŒä¹…åŒ–å­˜å‚¨
            
        Returns:
            VectorStoreIndex: å‘é‡ç´¢å¼•
        """
        logger.info(f"åˆ›å»ºæ–‡æ¡£å—ç´¢å¼•ï¼Œå…± {len(nodes)} ä¸ªèŠ‚ç‚¹")
        
        try:
            # è·å–å‘é‡å­˜å‚¨ï¼ˆä½¿ç”¨chunksåç¼€ï¼‰
            vector_store = self._get_vector_store("chunks")
            storage_context = StorageContext.from_defaults(vector_store=vector_store)
            
            # åˆ›å»ºç´¢å¼•
            self.chunk_index = VectorStoreIndex(
                nodes=nodes,
                storage_context=storage_context,
                show_progress=True
            )
            
            # æ›´æ–°å…ƒæ•°æ®
            self.index_metadata["chunks"] = {
                "created_at": datetime.now().isoformat(),
                "node_count": len(nodes),
                "last_updated": datetime.now().isoformat()
            }
            
            # å¯¹äºChromaï¼Œæ•°æ®å·²ç»è‡ªåŠ¨æŒä¹…åŒ–
            if self.store_type == "chroma":
                logger.info("âœ… æ–‡æ¡£å—embeddingå·²è‡ªåŠ¨ä¿å­˜åˆ°Chromaæ•°æ®åº“")
            elif persist:
                # éChromaçš„æŒä¹…åŒ–å­˜å‚¨
                chunk_persist_dir = os.path.join(self.persist_directory, "chunks")
                self._persist_index(self.chunk_index, chunk_persist_dir, "chunks")
            
            logger.info("æ–‡æ¡£å—ç´¢å¼•åˆ›å»ºæˆåŠŸ")
            return self.chunk_index
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæ–‡æ¡£å—ç´¢å¼•å¤±è´¥: {e}")
            raise e
    
    def create_concept_index(self, nodes: List[Union[ConceptNode, TextNode]], persist: bool = True) -> VectorStoreIndex:
        """
        åˆ›å»ºæ¦‚å¿µç´¢å¼•
        
        Args:
            nodes: æ¦‚å¿µèŠ‚ç‚¹åˆ—è¡¨
            persist: æ˜¯å¦æŒä¹…åŒ–å­˜å‚¨
            
        Returns:
            VectorStoreIndex: æ¦‚å¿µå‘é‡ç´¢å¼•
        """
        logger.info(f"åˆ›å»ºæ¦‚å¿µç´¢å¼•ï¼Œå…± {len(nodes)} ä¸ªæ¦‚å¿µ")
        
        try:
            # è·å–å‘é‡å­˜å‚¨ï¼ˆä½¿ç”¨conceptsåç¼€ï¼‰
            vector_store = self._get_vector_store("concepts")
            storage_context = StorageContext.from_defaults(vector_store=vector_store)
            
            # åˆ›å»ºç´¢å¼•
            self.concept_index = VectorStoreIndex(
                nodes=nodes,
                storage_context=storage_context,
                show_progress=True
            )
            
            # æ›´æ–°å…ƒæ•°æ®
            self.index_metadata["concepts"] = {
                "created_at": datetime.now().isoformat(),
                "node_count": len(nodes),
                "last_updated": datetime.now().isoformat()
            }
            
            # å¯¹äºChromaï¼Œæ•°æ®å·²ç»è‡ªåŠ¨æŒä¹…åŒ–
            if self.store_type == "chroma":
                logger.info("âœ… æ¦‚å¿µembeddingå·²è‡ªåŠ¨ä¿å­˜åˆ°Chromaæ•°æ®åº“")
            elif persist:
                # éChromaçš„æŒä¹…åŒ–å­˜å‚¨
                concept_persist_dir = os.path.join(self.persist_directory, "concepts")
                self._persist_index(self.concept_index, concept_persist_dir, "concepts")
            
            logger.info("æ¦‚å¿µç´¢å¼•åˆ›å»ºæˆåŠŸ")
            return self.concept_index
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæ¦‚å¿µç´¢å¼•å¤±è´¥: {e}")
            raise e
    
    def create_evidence_index(self, nodes: List[Union[EvidenceNode, TextNode]], persist: bool = True) -> VectorStoreIndex:
        """
        åˆ›å»ºè¯æ®ç´¢å¼•
        
        Args:
            nodes: è¯æ®èŠ‚ç‚¹åˆ—è¡¨
            persist: æ˜¯å¦æŒä¹…åŒ–å­˜å‚¨
            
        Returns:
            VectorStoreIndex: è¯æ®å‘é‡ç´¢å¼•
        """
        logger.info(f"åˆ›å»ºè¯æ®ç´¢å¼•ï¼Œå…± {len(nodes)} ä¸ªè¯æ®")
        
        try:
            # è·å–å‘é‡å­˜å‚¨ï¼ˆä½¿ç”¨evidenceåç¼€ï¼‰
            vector_store = self._get_vector_store("evidence")
            storage_context = StorageContext.from_defaults(vector_store=vector_store)
            
            # åˆ›å»ºç´¢å¼•
            self.evidence_index = VectorStoreIndex(
                nodes=nodes,
                storage_context=storage_context,
                show_progress=True
            )
            
            # æ›´æ–°å…ƒæ•°æ®
            self.index_metadata["evidence"] = {
                "created_at": datetime.now().isoformat(),
                "node_count": len(nodes),
                "last_updated": datetime.now().isoformat()
            }
            
            # å¯¹äºChromaï¼Œæ•°æ®å·²ç»è‡ªåŠ¨æŒä¹…åŒ–
            if self.store_type == "chroma":
                logger.info("âœ… è¯æ®embeddingå·²è‡ªåŠ¨ä¿å­˜åˆ°Chromaæ•°æ®åº“")
            elif persist:
                # éChromaçš„æŒä¹…åŒ–å­˜å‚¨
                evidence_persist_dir = os.path.join(self.persist_directory, "evidence")
                self._persist_index(self.evidence_index, evidence_persist_dir, "evidence")
            
            logger.info("è¯æ®ç´¢å¼•åˆ›å»ºæˆåŠŸ")
            return self.evidence_index
            
        except Exception as e:
            logger.error(f"åˆ›å»ºè¯æ®ç´¢å¼•å¤±è´¥: {e}")
            raise e
    
    def _persist_index(self, index: VectorStoreIndex, persist_dir: str, index_type: str):
        """æŒä¹…åŒ–ç´¢å¼•"""
        try:
            Path(persist_dir).mkdir(parents=True, exist_ok=True)
            index.storage_context.persist(persist_dir=persist_dir)
            
            # ä¿å­˜é¢å¤–çš„èŠ‚ç‚¹ä¿¡æ¯
            self._save_node_metadata(index, persist_dir, index_type)
            
            logger.info(f"{index_type}ç´¢å¼•å·²æŒä¹…åŒ–åˆ°: {persist_dir}")
        except Exception as e:
            logger.error(f"æŒä¹…åŒ–{index_type}ç´¢å¼•å¤±è´¥: {e}")
            raise e
    
    def _save_node_metadata(self, index: VectorStoreIndex, persist_dir: str, index_type: str):
        """ä¿å­˜èŠ‚ç‚¹å…ƒæ•°æ®"""
        try:
            nodes_info = []
            for node_id, node in index.docstore.docs.items():
                node_info = {
                    "node_id": node_id,
                    "text_preview": node.text[:200] if hasattr(node, 'text') else "",
                    "metadata": node.metadata if hasattr(node, 'metadata') else {},
                    "node_type": getattr(node, 'node_type', 'unknown')
                }
                nodes_info.append(node_info)
            
            metadata_file = os.path.join(persist_dir, f"{index_type}_metadata.json")
            FileHelper.save_json(nodes_info, metadata_file)
            
        except Exception as e:
            logger.warning(f"ä¿å­˜{index_type}èŠ‚ç‚¹å…ƒæ•°æ®å¤±è´¥: {e}")
    
    def load_chunk_index(self) -> Optional[VectorStoreIndex]:
        """
        åŠ è½½å·²æŒä¹…åŒ–çš„æ–‡æ¡£å—ç´¢å¼•
        
        Returns:
            Optional[VectorStoreIndex]: åŠ è½½çš„ç´¢å¼•ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        return self._load_index("chunks", "chunk_index")
    
    def load_concept_index(self) -> Optional[VectorStoreIndex]:
        """
        åŠ è½½å·²æŒä¹…åŒ–çš„æ¦‚å¿µç´¢å¼•
        
        Returns:
            Optional[VectorStoreIndex]: åŠ è½½çš„ç´¢å¼•ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        return self._load_index("concepts", "concept_index")
    
    def load_evidence_index(self) -> Optional[VectorStoreIndex]:
        """
        åŠ è½½å·²æŒä¹…åŒ–çš„è¯æ®ç´¢å¼•
        
        Returns:
            Optional[VectorStoreIndex]: åŠ è½½çš„ç´¢å¼•ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        return self._load_index("evidence", "evidence_index")
    
    def _load_index(self, index_name: str, attr_name: str) -> Optional[VectorStoreIndex]:
        """é€šç”¨çš„ç´¢å¼•åŠ è½½æ–¹æ³•"""
        persist_dir = os.path.join(self.persist_directory, index_name)
        
        if not os.path.exists(persist_dir):
            logger.info(f"æœªæ‰¾åˆ°æŒä¹…åŒ–çš„{index_name}ç´¢å¼•")
            return None
        
        try:
            # é‡å»ºå­˜å‚¨ä¸Šä¸‹æ–‡
            storage_context = StorageContext.from_defaults(persist_dir=persist_dir)
            
            # åŠ è½½ç´¢å¼•
            index = VectorStoreIndex.from_storage_context(storage_context)
            setattr(self, attr_name, index)
            
            logger.info(f"æˆåŠŸåŠ è½½{index_name}ç´¢å¼•: {persist_dir}")
            return index
            
        except Exception as e:
            logger.warning(f"åŠ è½½{index_name}ç´¢å¼•å¤±è´¥: {e}")
            return None
    
    def update_index(self, index_type: str, new_nodes: List[BaseNode]) -> bool:
        """
        æ›´æ–°ç´¢å¼•ï¼ˆæ·»åŠ æ–°èŠ‚ç‚¹ï¼‰
        
        Args:
            index_type: ç´¢å¼•ç±»å‹ ("chunks", "concepts", "evidence")
            new_nodes: æ–°èŠ‚ç‚¹åˆ—è¡¨
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸæ›´æ–°
        """
        try:
            if index_type == "chunks" and self.chunk_index:
                for node in new_nodes:
                    self.chunk_index.insert(node)
                self.index_metadata["chunks"]["last_updated"] = datetime.now().isoformat()
                self.index_metadata["chunks"]["node_count"] += len(new_nodes)
                
            elif index_type == "concepts" and self.concept_index:
                for node in new_nodes:
                    self.concept_index.insert(node)
                self.index_metadata["concepts"]["last_updated"] = datetime.now().isoformat()
                self.index_metadata["concepts"]["node_count"] += len(new_nodes)
                
            elif index_type == "evidence" and self.evidence_index:
                for node in new_nodes:
                    self.evidence_index.insert(node)
                self.index_metadata["evidence"]["last_updated"] = datetime.now().isoformat()
                self.index_metadata["evidence"]["node_count"] += len(new_nodes)
            else:
                logger.error(f"æ— æ•ˆçš„ç´¢å¼•ç±»å‹æˆ–ç´¢å¼•ä¸å­˜åœ¨: {index_type}")
                return False
            
            # ä¿å­˜å…ƒæ•°æ®
            self._save_metadata()
            
            logger.info(f"æˆåŠŸæ›´æ–°{index_type}ç´¢å¼•ï¼Œæ·»åŠ äº†{len(new_nodes)}ä¸ªèŠ‚ç‚¹")
            return True
            
        except Exception as e:
            logger.error(f"æ›´æ–°{index_type}ç´¢å¼•å¤±è´¥: {e}")
            return False
    
    def delete_nodes(self, index_type: str, node_ids: List[str]) -> bool:
        """
        ä»ç´¢å¼•ä¸­åˆ é™¤èŠ‚ç‚¹
        
        Args:
            index_type: ç´¢å¼•ç±»å‹
            node_ids: è¦åˆ é™¤çš„èŠ‚ç‚¹IDåˆ—è¡¨
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸåˆ é™¤
        """
        try:
            index = getattr(self, f"{index_type}_index", None)
            if not index:
                logger.error(f"{index_type}ç´¢å¼•ä¸å­˜åœ¨")
                return False
            
            for node_id in node_ids:
                index.delete(node_id)
            
            # æ›´æ–°å…ƒæ•°æ®
            self.index_metadata[index_type]["last_updated"] = datetime.now().isoformat()
            self.index_metadata[index_type]["node_count"] -= len(node_ids)
            self._save_metadata()
            
            logger.info(f"æˆåŠŸä»{index_type}ç´¢å¼•åˆ é™¤{len(node_ids)}ä¸ªèŠ‚ç‚¹")
            return True
            
        except Exception as e:
            logger.error(f"åˆ é™¤{index_type}ç´¢å¼•èŠ‚ç‚¹å¤±è´¥: {e}")
            return False
    
    def clear_indexes(self):
        """æ¸…é™¤æ‰€æœ‰ç´¢å¼•"""
        self.chunk_index = None
        self.concept_index = None
        self.evidence_index = None
        
        # é‡ç½®å…ƒæ•°æ®
        for index_type in self.index_metadata:
            self.index_metadata[index_type] = {
                "created_at": None, 
                "node_count": 0, 
                "last_updated": None
            }
        
        logger.info("å·²æ¸…é™¤æ‰€æœ‰ç´¢å¼•")
    
    def delete_persisted_indexes(self, index_types: List[str] = None):
        """
        åˆ é™¤æŒä¹…åŒ–çš„ç´¢å¼•æ–‡ä»¶
        
        Args:
            index_types: è¦åˆ é™¤çš„ç´¢å¼•ç±»å‹åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºåˆ é™¤æ‰€æœ‰
        """
        if index_types is None:
            index_types = ["chunks", "concepts", "evidence"]
        
        for index_type in index_types:
            persist_dir = os.path.join(self.persist_directory, index_type)
            try:
                if os.path.exists(persist_dir):
                    shutil.rmtree(persist_dir)
                    logger.info(f"å·²åˆ é™¤{index_type}ç´¢å¼•: {persist_dir}")
                    
                    # é‡ç½®å…ƒæ•°æ®
                    self.index_metadata[index_type] = {
                        "created_at": None, 
                        "node_count": 0, 
                        "last_updated": None
                    }
                    
            except Exception as e:
                logger.error(f"åˆ é™¤{index_type}ç´¢å¼•å¤±è´¥: {e}")
        
        self._save_metadata()
    
    def backup_indexes(self, backup_dir: str) -> bool:
        """
        å¤‡ä»½ç´¢å¼•
        
        Args:
            backup_dir: å¤‡ä»½ç›®å½•
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸå¤‡ä»½
        """
        try:
            backup_path = Path(backup_dir)
            backup_path.mkdir(parents=True, exist_ok=True)
            
            # åˆ›å»ºæ—¶é—´æˆ³ç›®å½•
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            timestamped_backup = backup_path / f"backup_{timestamp}"
            timestamped_backup.mkdir(exist_ok=True)
            
            # å¤åˆ¶ç´¢å¼•ç›®å½•
            for index_type in ["chunks", "concepts", "evidence"]:
                source_dir = os.path.join(self.persist_directory, index_type)
                if os.path.exists(source_dir):
                    target_dir = timestamped_backup / index_type
                    shutil.copytree(source_dir, target_dir)
                    logger.info(f"å·²å¤‡ä»½{index_type}ç´¢å¼•åˆ°: {target_dir}")
            
            # å¤‡ä»½å…ƒæ•°æ®
            metadata_backup = timestamped_backup / "metadata.json"
            FileHelper.save_json(self.index_metadata, str(metadata_backup))
            
            logger.info(f"ç´¢å¼•å¤‡ä»½å®Œæˆ: {timestamped_backup}")
            return True
            
        except Exception as e:
            logger.error(f"å¤‡ä»½ç´¢å¼•å¤±è´¥: {e}")
            return False
    
    def restore_indexes(self, backup_dir: str) -> bool:
        """
        ä»å¤‡ä»½æ¢å¤ç´¢å¼•
        
        Args:
            backup_dir: å¤‡ä»½ç›®å½•
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸæ¢å¤
        """
        try:
            backup_path = Path(backup_dir)
            if not backup_path.exists():
                logger.error(f"å¤‡ä»½ç›®å½•ä¸å­˜åœ¨: {backup_dir}")
                return False
            
            # æ¸…é™¤å½“å‰ç´¢å¼•
            self.clear_indexes()
            self.delete_persisted_indexes()
            
            # æ¢å¤ç´¢å¼•ç›®å½•
            for index_type in ["chunks", "concepts", "evidence"]:
                source_dir = backup_path / index_type
                if source_dir.exists():
                    target_dir = Path(self.persist_directory) / index_type
                    shutil.copytree(source_dir, target_dir)
                    logger.info(f"å·²æ¢å¤{index_type}ç´¢å¼•ä»: {source_dir}")
            
            # æ¢å¤å…ƒæ•°æ®
            metadata_backup = backup_path / "metadata.json"
            if metadata_backup.exists():
                self.index_metadata = FileHelper.load_json(str(metadata_backup))
            
            # é‡æ–°åŠ è½½ç´¢å¼•
            self.rebuild_indexes_from_persisted()
            
            logger.info(f"ç´¢å¼•æ¢å¤å®Œæˆ: {backup_dir}")
            return True
            
        except Exception as e:
            logger.error(f"æ¢å¤ç´¢å¼•å¤±è´¥: {e}")
            return False
    
    def get_index_info(self) -> Dict[str, Any]:
        """
        è·å–ç´¢å¼•ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: ç´¢å¼•ä¿¡æ¯
        """
        info = {
            "store_type": self.store_type,
            "persist_directory": self.persist_directory,
            "collection_name": self.collection_name,
            "dimension": self.dimension,
            "indexes": {}
        }
        
        # æ·»åŠ å„ä¸ªç´¢å¼•çš„ä¿¡æ¯
        for index_type in ["chunks", "concepts", "evidence"]:
            index = getattr(self, f"{index_type}_index", None)
            persist_dir = os.path.join(self.persist_directory, index_type)
            
            index_info = {
                "exists": index is not None,
                "persisted": os.path.exists(persist_dir),
                "metadata": self.index_metadata.get(index_type, {})
            }
            
            # å°è¯•è·å–å®é™…èŠ‚ç‚¹æ•°é‡
            if index:
                try:
                    index_info["actual_node_count"] = len(index.docstore.docs)
                except:
                    index_info["actual_node_count"] = "unknown"
            
            info["indexes"][index_type] = index_info
        
        return info
    
    def get_storage_size(self) -> Dict[str, Any]:
        """è·å–å­˜å‚¨å¤§å°ä¿¡æ¯"""
        def get_dir_size(path):
            total = 0
            try:
                for entry in os.scandir(path):
                    if entry.is_file():
                        total += entry.stat().st_size
                    elif entry.is_dir():
                        total += get_dir_size(entry.path)
            except:
                pass
            return total
        
        size_info = {}
        total_size = 0
        
        for index_type in ["chunks", "concepts", "evidence"]:
            persist_dir = os.path.join(self.persist_directory, index_type)
            if os.path.exists(persist_dir):
                size = get_dir_size(persist_dir)
                size_info[index_type] = {
                    "size_bytes": size,
                    "size_mb": round(size / (1024 * 1024), 2)
                }
                total_size += size
            else:
                size_info[index_type] = {"size_bytes": 0, "size_mb": 0}
        
        size_info["total"] = {
            "size_bytes": total_size,
            "size_mb": round(total_size / (1024 * 1024), 2)
        }
        
        return size_info
    
    def rebuild_indexes_from_persisted(self) -> bool:
        """
        ä»æŒä¹…åŒ–æ–‡ä»¶é‡å»ºç´¢å¼•
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸé‡å»º
        """
        success_count = 0
        
        # å°è¯•åŠ è½½å„ä¸ªç´¢å¼•
        if self.load_chunk_index():
            success_count += 1
        
        if self.load_concept_index():
            success_count += 1
            
        if self.load_evidence_index():
            success_count += 1
        
        if success_count > 0:
            logger.info(f"æˆåŠŸé‡å»º{success_count}ä¸ªç´¢å¼•")
            return True
        else:
            logger.warning("æ²¡æœ‰æˆåŠŸé‡å»ºä»»ä½•ç´¢å¼•")
            return False
    
    def _load_metadata(self):
        """åŠ è½½å…ƒæ•°æ®"""
        metadata_file = os.path.join(self.persist_directory, "index_metadata.json")
        if os.path.exists(metadata_file):
            try:
                self.index_metadata = FileHelper.load_json(metadata_file)
                logger.debug("æˆåŠŸåŠ è½½ç´¢å¼•å…ƒæ•°æ®")
            except Exception as e:
                logger.warning(f"åŠ è½½ç´¢å¼•å…ƒæ•°æ®å¤±è´¥: {e}")
    
    def _save_metadata(self):
        """ä¿å­˜å…ƒæ•°æ®"""
        metadata_file = os.path.join(self.persist_directory, "index_metadata.json")
        try:
            FileHelper.save_json(self.index_metadata, metadata_file)
            logger.debug("æˆåŠŸä¿å­˜ç´¢å¼•å…ƒæ•°æ®")
        except Exception as e:
            logger.warning(f"ä¿å­˜ç´¢å¼•å…ƒæ•°æ®å¤±è´¥: {e}")
    
    def optimize_indexes(self) -> bool:
        """
        ä¼˜åŒ–ç´¢å¼•ï¼ˆé‡å»ºä»¥æé«˜æ€§èƒ½ï¼‰
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸä¼˜åŒ–
        """
        try:
            logger.info("å¼€å§‹ä¼˜åŒ–ç´¢å¼•...")
            
            # å¤‡ä»½å½“å‰ç´¢å¼•
            backup_success = self.backup_indexes(
                os.path.join(self.persist_directory, "optimization_backup")
            )
            
            if not backup_success:
                logger.error("å¤‡ä»½å¤±è´¥ï¼Œå–æ¶ˆä¼˜åŒ–")
                return False
            
            # æ”¶é›†æ‰€æœ‰èŠ‚ç‚¹
            all_nodes = {"chunks": [], "concepts": [], "evidence": []}
            
            for index_type in ["chunks", "concepts", "evidence"]:
                index = getattr(self, f"{index_type}_index", None)
                if index:
                    nodes = list(index.docstore.docs.values())
                    all_nodes[index_type] = nodes
            
            # æ¸…é™¤å½“å‰ç´¢å¼•
            self.clear_indexes()
            self.delete_persisted_indexes()
            
            # é‡å»ºç´¢å¼•
            rebuild_success = True
            for index_type, nodes in all_nodes.items():
                if nodes:
                    try:
                        if index_type == "chunks":
                            self.create_chunk_index(nodes)
                        elif index_type == "concepts":
                            self.create_concept_index(nodes)
                        elif index_type == "evidence":
                            self.create_evidence_index(nodes)
                    except Exception as e:
                        logger.error(f"é‡å»º{index_type}ç´¢å¼•å¤±è´¥: {e}")
                        rebuild_success = False
            
            if rebuild_success:
                logger.info("ç´¢å¼•ä¼˜åŒ–å®Œæˆ")
                # åˆ é™¤å¤‡ä»½
                backup_dir = os.path.join(self.persist_directory, "optimization_backup")
                if os.path.exists(backup_dir):
                    shutil.rmtree(backup_dir)
                return True
            else:
                logger.error("ç´¢å¼•ä¼˜åŒ–å¤±è´¥ï¼Œå°è¯•æ¢å¤å¤‡ä»½")
                return self.restore_indexes(
                    os.path.join(self.persist_directory, "optimization_backup")
                )
                
        except Exception as e:
            logger.error(f"ç´¢å¼•ä¼˜åŒ–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return False 