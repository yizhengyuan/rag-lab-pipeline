"""
Chroma å®é™…ä½¿ç”¨æŒ‡å—
åœ¨ä½ çš„é¡¹ç›®ä¸­å¦‚ä½•ç›´æ¥ä½¿ç”¨ Chroma å‘é‡æ•°æ®åº“

è¿™ä¸ªæ–‡ä»¶å±•ç¤ºäº†æœ€å¸¸ç”¨çš„ä½¿ç”¨åœºæ™¯å’Œä»£ç æ¨¡å¼
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from core.chunking import SemanticChunker
from core.vector_store import VectorStoreManager
from utils.config_loader import ConfigLoader
from llama_index.core import Document

# ============================================================================
# ğŸš€ åœºæ™¯1ï¼šåŸºæœ¬æ–‡æ¡£å¤„ç†æµç¨‹
# ============================================================================

def basic_document_processing():
    """æœ€åŸºæœ¬çš„æ–‡æ¡£å¤„ç†æµç¨‹"""
    
    print("ğŸš€ åŸºæœ¬æ–‡æ¡£å¤„ç†æµç¨‹")
    
    # 1. åŠ è½½é…ç½®ï¼ˆç¡®ä¿ vector_store.type = "chroma"ï¼‰
    config = ConfigLoader.load_config()
    
    # 2. åˆå§‹åŒ–ç»„ä»¶
    chunker = SemanticChunker(config)
    vector_manager = VectorStoreManager(config)
    
    # 3. å‡†å¤‡ä½ çš„æ–‡æ¡£
    documents = [
        Document(text="ä½ çš„æ–‡æ¡£å†…å®¹1..."),
        Document(text="ä½ çš„æ–‡æ¡£å†…å®¹2..."),
        # æ›´å¤šæ–‡æ¡£...
    ]
    
    # 4. å¤„ç†æ–‡æ¡£ï¼ˆç¬¬ä¸€æ¬¡ä¼šè°ƒç”¨APIï¼Œåç»­ä½¿ç”¨ç¼“å­˜ï¼‰
    chunk_nodes = chunker.chunk_and_extract_concepts(documents)
    
    # 5. å­˜å‚¨åˆ°Chromaå‘é‡æ•°æ®åº“
    chunk_index = vector_manager.create_chunk_index(chunk_nodes, persist=True)
    
    print(f"âœ… å¤„ç†å®Œæˆ: {len(chunk_nodes)} ä¸ªchunkså·²å­˜å‚¨åˆ°Chroma")
    
    return chunk_index

# ============================================================================
# ğŸ” åœºæ™¯2ï¼šå‘é‡æ£€ç´¢å’ŒæŸ¥è¯¢
# ============================================================================

def vector_search_example(chunk_index):
    """å‘é‡æ£€ç´¢ç¤ºä¾‹"""
    
    print("\nğŸ” å‘é‡æ£€ç´¢ç¤ºä¾‹")
    
    # åˆ›å»ºæŸ¥è¯¢å¼•æ“
    query_engine = chunk_index.as_query_engine(
        similarity_top_k=5,  # è¿”å›æœ€ç›¸ä¼¼çš„5ä¸ªç»“æœ
        verbose=True
    )
    
    # æ‰§è¡ŒæŸ¥è¯¢
    queries = [
        "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
        "Transformeræ¨¡å‹çš„ä¼˜åŠ¿",
        "äººå·¥æ™ºèƒ½çš„åº”ç”¨é¢†åŸŸ"
    ]
    
    for query in queries:
        print(f"\nâ“ æŸ¥è¯¢: {query}")
        try:
            response = query_engine.query(query)
            print(f"ğŸ“ å›ç­”: {response.response[:100]}...")
            
            # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„æºæ–‡æ¡£
            if hasattr(response, 'source_nodes'):
                print(f"ğŸ“š æ‰¾åˆ° {len(response.source_nodes)} ä¸ªç›¸å…³æ–‡æ¡£")
                
        except Exception as e:
            print(f"âš ï¸ æŸ¥è¯¢å¤±è´¥: {e}")

# ============================================================================
# ğŸ’¾ åœºæ™¯3ï¼šç¼“å­˜çŠ¶æ€æ£€æŸ¥
# ============================================================================

def check_cache_status():
    """æ£€æŸ¥ç¼“å­˜çŠ¶æ€"""
    
    print("\nğŸ’¾ ç¼“å­˜çŠ¶æ€æ£€æŸ¥")
    
    config = ConfigLoader.load_config()
    chunker = SemanticChunker(config)
    vector_manager = VectorStoreManager(config)
    
    # æ£€æŸ¥embeddingç¼“å­˜
    if chunker.embedding_cache:
        cache_stats = chunker.embedding_cache.get_cache_stats()
        print(f"ğŸ“Š Embeddingç¼“å­˜:")
        print(f"   æ¡ç›®æ•°: {cache_stats['total_entries']}")
        print(f"   å¤§å°: {cache_stats['estimated_size_mb']:.2f} MB")
        print(f"   ç›®å½•: {cache_stats['cache_directory']}")
    
    # æ£€æŸ¥å‘é‡æ•°æ®åº“çŠ¶æ€
    index_info = vector_manager.get_index_info()
    print(f"\nğŸ—ƒï¸ å‘é‡æ•°æ®åº“çŠ¶æ€:")
    print(f"   ç±»å‹: {index_info['store_type']}")
    print(f"   ç›®å½•: {index_info['persist_directory']}")
    
    for index_type, info in index_info['indexes'].items():
        if info['exists'] or info['persisted']:
            node_count = info['metadata'].get('node_count', 0)
            print(f"   {index_type}: {node_count} ä¸ªèŠ‚ç‚¹")

# ============================================================================
# ğŸ—‚ï¸ åœºæ™¯4ï¼šå¤šç±»å‹æ•°æ®å­˜å‚¨
# ============================================================================

def multi_type_storage():
    """å¤šç±»å‹æ•°æ®å­˜å‚¨ç¤ºä¾‹"""
    
    print("\nğŸ—‚ï¸ å¤šç±»å‹æ•°æ®å­˜å‚¨")
    
    config = ConfigLoader.load_config()
    vector_manager = VectorStoreManager(config)
    
    from core.nodes import ConceptNode, EvidenceNode
    from llama_index.core.schema import TextNode
    
    # 1. å­˜å‚¨æ¦‚å¿µ
    concept_nodes = [
        ConceptNode(text="æœºå™¨å­¦ä¹ ", concept_type="æŠ€æœ¯"),
        ConceptNode(text="æ·±åº¦å­¦ä¹ ", concept_type="æŠ€æœ¯"),
    ]
    concept_index = vector_manager.create_concept_index(concept_nodes)
    print("âœ… æ¦‚å¿µæ•°æ®å·²å­˜å‚¨")
    
    # 2. å­˜å‚¨è¯æ®
    evidence_nodes = [
        TextNode(
            text="ç ”ç©¶è¡¨æ˜ï¼Œæ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Š",
            metadata={"type": "evidence", "concept": "æ·±åº¦å­¦ä¹ "}
        )
    ]
    evidence_index = vector_manager.create_evidence_index(evidence_nodes)
    print("âœ… è¯æ®æ•°æ®å·²å­˜å‚¨")
    
    return concept_index, evidence_index

# ============================================================================
# ğŸ“¦ åœºæ™¯5ï¼šæ•°æ®å¤‡ä»½å’Œæ¢å¤
# ============================================================================

def backup_and_restore():
    """æ•°æ®å¤‡ä»½å’Œæ¢å¤"""
    
    print("\nğŸ“¦ æ•°æ®å¤‡ä»½å’Œæ¢å¤")
    
    config = ConfigLoader.load_config()
    vector_manager = VectorStoreManager(config)
    
    # åˆ›å»ºå¤‡ä»½
    backup_dir = "./my_backup"
    success = vector_manager.backup_indexes(backup_dir)
    
    if success:
        print(f"âœ… æ•°æ®å·²å¤‡ä»½åˆ°: {backup_dir}")
        
        # æ˜¾ç¤ºå¤‡ä»½å¤§å°
        storage_info = vector_manager.get_storage_size()
        total_size = storage_info.get('total', {}).get('size_mb', 0)
        print(f"ğŸ’½ å¤‡ä»½å¤§å°: {total_size:.2f} MB")
    else:
        print("âŒ å¤‡ä»½å¤±è´¥")

# ============================================================================
# ğŸ§¹ åœºæ™¯6ï¼šæ•°æ®æ¸…ç†å’Œç»´æŠ¤
# ============================================================================

def data_maintenance():
    """æ•°æ®æ¸…ç†å’Œç»´æŠ¤"""
    
    print("\nğŸ§¹ æ•°æ®æ¸…ç†å’Œç»´æŠ¤")
    
    config = ConfigLoader.load_config()
    chunker = SemanticChunker(config)
    vector_manager = VectorStoreManager(config)
    
    # 1. æ¸…ç†è¿‡æœŸç¼“å­˜
    if chunker.embedding_cache:
        initial_count = chunker.embedding_cache.get_cache_stats()['total_entries']
        chunker.embedding_cache.clear_expired()
        final_count = chunker.embedding_cache.get_cache_stats()['total_entries']
        cleaned = initial_count - final_count
        print(f"ğŸ—‘ï¸ æ¸…ç†äº† {cleaned} ä¸ªè¿‡æœŸç¼“å­˜")
    
    # 2. ä¼˜åŒ–ç´¢å¼•ï¼ˆå¯é€‰ï¼Œç”¨äºå¤§é‡æ•°æ®æ—¶æå‡æ€§èƒ½ï¼‰
    try:
        success = vector_manager.optimize_indexes()
        if success:
            print("âš¡ ç´¢å¼•ä¼˜åŒ–å®Œæˆ")
        else:
            print("âš ï¸ ç´¢å¼•ä¼˜åŒ–å¤±è´¥")
    except Exception as e:
        print(f"âš ï¸ ç´¢å¼•ä¼˜åŒ–å¤±è´¥: {e}")

# ============================================================================
# ğŸ“‹ å¸¸ç”¨çš„å®ç”¨å‡½æ•°
# ============================================================================

def quick_setup():
    """å¿«é€Ÿè®¾ç½® - ä¸€é”®åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
    
    config = ConfigLoader.load_config()
    chunker = SemanticChunker(config)
    vector_manager = VectorStoreManager(config)
    
    print("âœ… Chromaç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
    return config, chunker, vector_manager

def process_documents_with_cache_check(documents):
    """å¤„ç†æ–‡æ¡£å¹¶æ£€æŸ¥ç¼“å­˜ä½¿ç”¨æƒ…å†µ"""
    
    config, chunker, vector_manager = quick_setup()
    
    # æ£€æŸ¥ç¼“å­˜çŠ¶æ€
    initial_cache = 0
    if chunker.embedding_cache:
        initial_cache = chunker.embedding_cache.get_cache_stats()['total_entries']
    
    # å¤„ç†æ–‡æ¡£
    chunk_nodes = chunker.chunk_and_extract_concepts(documents)
    
    # æ£€æŸ¥ç¼“å­˜å˜åŒ–
    final_cache = 0
    if chunker.embedding_cache:
        final_cache = chunker.embedding_cache.get_cache_stats()['total_entries']
    
    new_cache_entries = final_cache - initial_cache
    
    print(f"ğŸ“Š å¤„ç†ç»“æœ:")
    print(f"   æ–‡æ¡£æ•°: {len(documents)}")
    print(f"   Chunks: {len(chunk_nodes)}")
    print(f"   æ–°å¢ç¼“å­˜: {new_cache_entries}")
    
    # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
    chunk_index = vector_manager.create_chunk_index(chunk_nodes)
    
    return chunk_index, chunk_nodes

def get_storage_summary():
    """è·å–å­˜å‚¨æ‘˜è¦ä¿¡æ¯"""
    
    config = ConfigLoader.load_config()
    vector_manager = VectorStoreManager(config)
    chunker = SemanticChunker(config)
    
    # å‘é‡æ•°æ®åº“ä¿¡æ¯
    index_info = vector_manager.get_index_info()
    storage_info = vector_manager.get_storage_size()
    
    # ç¼“å­˜ä¿¡æ¯
    cache_stats = {}
    if chunker.embedding_cache:
        cache_stats = chunker.embedding_cache.get_cache_stats()
    
    summary = {
        "vector_db": {
            "type": index_info['store_type'],
            "directory": index_info['persist_directory'],
            "total_size_mb": storage_info.get('total', {}).get('size_mb', 0),
            "indexes": {}
        },
        "embedding_cache": cache_stats
    }
    
    # ç´¢å¼•è¯¦æƒ…
    for index_type, info in index_info['indexes'].items():
        if info['exists'] or info['persisted']:
            summary["vector_db"]["indexes"][index_type] = {
                "node_count": info['metadata'].get('node_count', 0),
                "last_updated": info['metadata'].get('last_updated', 'N/A')
            }
    
    return summary

# ============================================================================
# ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹
# ============================================================================

if __name__ == "__main__":
    print("ğŸ¯ Chroma å®é™…ä½¿ç”¨æ¼”ç¤º")
    print("=" * 50)
    
    # ç¤ºä¾‹æ–‡æ¡£
    sample_documents = [
        Document(
            text="GPTæ˜¯ä¸€ç§åŸºäºTransformeræ¶æ„çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰å¼ºå¤§çš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚",
            metadata={"source": "ai_textbook", "chapter": "language_models"}
        ),
        Document(
            text="BERTé€šè¿‡åŒå‘ç¼–ç å™¨è¡¨ç¤ºæ¥ç†è§£è¯­è¨€ä¸Šä¸‹æ–‡ï¼Œåœ¨å¤šç§NLPä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚",
            metadata={"source": "ai_textbook", "chapter": "language_models"}
        )
    ]
    
    try:
        # 1. åŸºæœ¬å¤„ç†æµç¨‹
        chunk_index = basic_document_processing()
        
        # 2. å‘é‡æ£€ç´¢æµ‹è¯•
        vector_search_example(chunk_index)
        
        # 3. æ£€æŸ¥çŠ¶æ€
        check_cache_status()
        
        # 4. å¤šç±»å‹å­˜å‚¨
        concept_index, evidence_index = multi_type_storage()
        
        # 5. å¤‡ä»½æ•°æ®
        backup_and_restore()
        
        # 6. æ•°æ®ç»´æŠ¤
        data_maintenance()
        
        # 7. å­˜å‚¨æ‘˜è¦
        print("\nğŸ“‹ å­˜å‚¨æ‘˜è¦:")
        summary = get_storage_summary()
        print(f"   å‘é‡æ•°æ®åº“: {summary['vector_db']['type']}")
        print(f"   æ€»å¤§å°: {summary['vector_db']['total_size_mb']:.2f} MB")
        if summary['embedding_cache']:
            print(f"   ç¼“å­˜æ¡ç›®: {summary['embedding_cache']['total_entries']}")
        
        print("\nâœ… æ‰€æœ‰åŠŸèƒ½æ¼”ç¤ºå®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("   1. å°†ä½ çš„å®é™…æ–‡æ¡£æ›¿æ¢ sample_documents")
    print("   2. æ ¹æ®éœ€è¦è°ƒæ•´ similarity_top_k å‚æ•°")
    print("   3. å®šæœŸè¿è¡Œæ•°æ®ç»´æŠ¤åŠŸèƒ½")
    print("   4. æŸ¥çœ‹ç”Ÿæˆçš„ ./vector_db/ å’Œ ./embedding_cache/ ç›®å½•") 