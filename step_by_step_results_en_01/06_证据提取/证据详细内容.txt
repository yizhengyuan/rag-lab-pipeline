证据提取详细结果
================================================================================

证据 1
--------------------------------------------------
证据ID: evidence_0
关联概念: 认知与行为的探索与发展
相关性分数: 3.7500
来源分块: ['1f85d471-100d-4d0b-9843-0307fbd68483', '6f98970e-595f-4c96-b15d-668ea6632f19']
证据长度: 29 字符
证据完整内容:
抱歉，无法提供与“认知与行为的探索与发展”相关的证据片段。

================================================================================

证据 2
--------------------------------------------------
证据ID: evidence_1
关联概念: 优先选择的会议预印本
相关性分数: 2.7000
来源分块: ['d66849fa-95eb-4130-a3f4-101686790f24', '1f85d471-100d-4d0b-9843-0307fbd68483', '4522cdff-27b7-4da7-bf0e-9d7296413fcc', '070ede4c-f9e1-4bb4-bd37-40796a4ae622', 'bcab7e3e-3d56-445b-b837-f879a600d995']
证据长度: 115 字符
证据完整内容:
在WMT 2014英语到德语翻译任务中，大型变换器模型（表2中的Transformer (big)）比之前报告的最佳模型（包括集成模型）提高了超过2.0 BLEU，确立了28.4的新状态下BLEU分数。该模型的配置列在表3的底部。

================================================================================

证据 3
--------------------------------------------------
证据ID: evidence_2
关联概念: 文化与国际交流
相关性分数: 3.0000
来源分块: ['d66849fa-95eb-4130-a3f4-101686790f24']
证据长度: 25 字符
证据完整内容:
抱歉，无法提供与“文化与国际交流”相关的证据片段。

================================================================================

证据 4
--------------------------------------------------
证据ID: evidence_3
关联概念: 研究成果的可用性与优化状态
相关性分数: 3.2500
来源分块: ['ffbb0596-c0a4-4439-a524-66c25ed1fac9', '1f85d471-100d-4d0b-9843-0307fbd68483', '249bf066-2487-46be-af59-08361fc246b7', 'a8eed2a4-36d6-40a2-a9bc-98a9b7460958']
证据长度: 137 字符
证据完整内容:
我们展示了Transformer在英语成分解析任务上的良好泛化能力。尽管缺乏特定任务的调优，我们的模型表现出色，超越了所有先前报告的模型，除了递归神经网络语法模型。与递归神经网络序列到序列模型相比，Transformer即使在仅使用WSJ训练集的情况下，也优于伯克利解析器。

================================================================================

证据 5
--------------------------------------------------
证据ID: evidence_4
关联概念: 替代或附加的方式
相关性分数: 3.5000
来源分块: ['d66849fa-95eb-4130-a3f4-101686790f24', '6f98970e-595f-4c96-b15d-668ea6632f19', 'a8eed2a4-36d6-40a2-a9bc-98a9b7460958', '1f85d471-100d-4d0b-9843-0307fbd68483', '070ede4c-f9e1-4bb4-bd37-40796a4ae622']
证据长度: 590 字符
证据完整内容:
Attention Is All You Need
The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.

================================================================================

