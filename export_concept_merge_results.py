"""
æ¦‚å¿µåˆå¹¶ç»“æœå¯¼å‡ºå·¥å…·
====================================

è¿™ä¸ªè„šæœ¬å°†æ¦‚å¿µåˆå¹¶(concept merge)çš„ç»“æœå¯¼å‡ºä¸ºå¯è¯»çš„æ–‡æœ¬æ–‡ä»¶
åŒ…å«ï¼š
- åˆå¹¶å‰åçš„æ¦‚å¿µå¯¹æ¯”
- æ¯ä¸ªåˆå¹¶åæ¦‚å¿µçš„è¯¦ç»†ä¿¡æ¯
- æ¥æºchunkå’Œç½®ä¿¡åº¦ä¿¡æ¯
"""

import os
import json
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional
from llama_index.core import SimpleDirectoryReader

# å¯¼å…¥æ‚¨çš„æ¨¡å—
from core.chunking import SemanticChunker  
from core.concept_merger import ConceptMerger
from config import load_config_from_yaml

logger = logging.getLogger(__name__)

def export_concept_merge_results(pdf_path: str, config_path: str = "config/config.yml", output_dir: str = "./concept_merge_export"):
    """
    å¯¼å‡ºæ¦‚å¿µåˆå¹¶çš„å®Œæ•´å¤„ç†ç»“æœ
    
    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„  
        output_dir: è¾“å‡ºç›®å½•
    """
    
    file_name = os.path.basename(pdf_path)
    base_name = os.path.splitext(file_name)[0]
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(os.path.join(output_dir, "concepts"), exist_ok=True)
    
    logger.info(f"ğŸ”„ å¼€å§‹æ¦‚å¿µåˆå¹¶å¤„ç†: {pdf_path}")
    logger.info(f"âš™ï¸ é…ç½®æ–‡ä»¶: {config_path}")
    logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    logger.info("=" * 80)
    
    try:
        # 1. åŠ è½½é…ç½®
        logger.info("âš™ï¸ æ­¥éª¤1: åŠ è½½é…ç½®...")
        config = load_config_from_yaml(config_path)
        logger.info("   âœ… é…ç½®åŠ è½½å®Œæˆ")
        
        # 2. åˆå§‹åŒ–å¤„ç†å™¨
        logger.info("ğŸ§  æ­¥éª¤2: åˆå§‹åŒ–å¤„ç†å™¨...")
        chunker = SemanticChunker(config)
        concept_merger = ConceptMerger(config)
        logger.info("   âœ… å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # 3. åŠ è½½PDFæ–‡æ¡£
        logger.info("ğŸ“„ æ­¥éª¤3: åŠ è½½PDFæ–‡æ¡£...")
        reader = SimpleDirectoryReader(input_files=[pdf_path])
        documents = reader.load_data()
        logger.info(f"   âœ… PDFåŠ è½½å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£")
        
        # 4. æ‰§è¡Œè¯­ä¹‰åˆ†å—å’Œæ¦‚å¿µæå–
        logger.info("âœ‚ï¸ æ­¥éª¤4: æ‰§è¡Œè¯­ä¹‰åˆ†å—å’Œæ¦‚å¿µæå–...")
        chunk_nodes = chunker.chunk_and_extract_concepts(documents)
        logger.info(f"   âœ… åˆ†å—å®Œæˆ: {len(chunk_nodes)} ä¸ªchunk")
        
        # 5. æ‰§è¡Œæ¦‚å¿µåˆå¹¶
        logger.info("ğŸ”— æ­¥éª¤5: æ‰§è¡Œæ¦‚å¿µåˆå¹¶...")
        merged_concept_nodes = concept_merger.merge_document_concepts(chunk_nodes)
        logger.info(f"   âœ… æ¦‚å¿µåˆå¹¶å®Œæˆ: {len(merged_concept_nodes)} ä¸ªåˆå¹¶åæ¦‚å¿µ")
        
        # 6. æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        logger.info("ğŸ“Š æ­¥éª¤6: æ”¶é›†ç»Ÿè®¡ä¿¡æ¯...")
        merge_stats = collect_merge_statistics(chunk_nodes, merged_concept_nodes, concept_merger)
        logger.info(f"   âœ… ç»Ÿè®¡å®Œæˆ")
        
        # 7. å¯¼å‡ºæ¦‚å¿µåˆå¹¶æ€»è§ˆ
        logger.info("ğŸ’¾ æ­¥éª¤7: å¯¼å‡ºæ¦‚å¿µåˆå¹¶æ€»è§ˆ...")
        overview_path = export_merge_overview(merged_concept_nodes, merge_stats, base_name, output_dir)
        logger.info(f"   âœ… æ€»è§ˆæ–‡ä»¶å·²ä¿å­˜: {overview_path}")
        
        # 8. å¯¼å‡ºæ¯ä¸ªåˆå¹¶åæ¦‚å¿µçš„è¯¦ç»†ä¿¡æ¯
        logger.info("ğŸ“„ æ­¥éª¤8: å¯¼å‡ºè¯¦ç»†æ¦‚å¿µæ–‡ä»¶...")
        concept_files = export_individual_concepts(merged_concept_nodes, chunk_nodes, base_name, output_dir)
        logger.info(f"   âœ… å·²å¯¼å‡º {len(concept_files)} ä¸ªæ¦‚å¿µæ–‡ä»¶")
        
        # 9. å¯¼å‡ºJSONæ ¼å¼çš„å®Œæ•´æ•°æ®
        logger.info("ğŸ“‹ æ­¥éª¤9: å¯¼å‡ºJSONæ ¼å¼...")
        json_path = export_merge_json(merged_concept_nodes, merge_stats, base_name, output_dir)
        logger.info(f"   âœ… JSONæ–‡ä»¶å·²ä¿å­˜: {json_path}")
        
        # 10. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        logger.info("ğŸ“ˆ æ­¥éª¤10: ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š...")
        report_path = generate_merge_report(
            pdf_path, config_path, merge_stats, concept_files, 
            overview_path, json_path, base_name, output_dir
        )
        logger.info(f"   âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ: {report_path}")
        
        print(f"\nâœ… æ¦‚å¿µåˆå¹¶ç»“æœå¯¼å‡ºå®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ“Š æ€»è§ˆæ–‡ä»¶: {overview_path}")
        print(f"ğŸ“‹ è¯¦ç»†æŠ¥å‘Š: {report_path}")
        print(f"ğŸ“„ JSONæ ¼å¼: {json_path}")
        print(f"ğŸ“ å•ç‹¬æ¦‚å¿µæ–‡ä»¶: {output_dir}/concepts/")
        print(f"\nğŸ“ˆ åˆå¹¶ç»Ÿè®¡:")
        print(f"   - åŸå§‹æ¦‚å¿µæ•°: {merge_stats['original_concepts_count']}")
        print(f"   - åˆå¹¶åæ¦‚å¿µæ•°: {merge_stats['merged_concepts_count']}")
        print(f"   - åˆå¹¶å‡å°‘ç‡: {merge_stats['reduction_ratio']:.1%}")
        print(f"   - å¹³å‡ç½®ä¿¡åº¦: {merge_stats['avg_confidence']:.3f}")
        
        return {
            "success": True,
            "overview_file": overview_path,
            "concept_files": concept_files,
            "json_export": json_path,
            "report": report_path,
            "stats": merge_stats
        }
        
    except Exception as e:
        logger.error(f"âŒ å¤„ç†å¤±è´¥: {str(e)}")
        print(f"\nâŒ å¤„ç†å¤±è´¥: {str(e)}")
        return {"success": False, "error": str(e)}

def collect_merge_statistics(chunk_nodes: List, merged_concept_nodes: List, concept_merger: ConceptMerger) -> Dict[str, Any]:
    """æ”¶é›†æ¦‚å¿µåˆå¹¶çš„ç»Ÿè®¡ä¿¡æ¯"""
    
    # æ”¶é›†æ‰€æœ‰åŸå§‹æ¦‚å¿µ
    original_concepts = []
    for node in chunk_nodes:
        concepts = node.metadata.get("concepts", [])
        original_concepts.extend(concepts)
    
    # åŸºæœ¬ç»Ÿè®¡
    original_count = len(original_concepts)
    merged_count = len(merged_concept_nodes)
    reduction_ratio = (original_count - merged_count) / original_count if original_count > 0 else 0
    
    # ç½®ä¿¡åº¦ç»Ÿè®¡
    confidence_scores = [node.confidence_score for node in merged_concept_nodes if hasattr(node, 'confidence_score')]
    avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0
    max_confidence = max(confidence_scores) if confidence_scores else 0
    min_confidence = min(confidence_scores) if confidence_scores else 0
    
    # ç±»åˆ«ç»Ÿè®¡
    categories = [getattr(node, 'category', 'æœªåˆ†ç±»') for node in merged_concept_nodes]
    category_counts = {}
    for cat in categories:
        category_counts[cat] = category_counts.get(cat, 0) + 1
    
    # æ¥æºchunkç»Ÿè®¡
    chunk_coverage = set()
    for node in merged_concept_nodes:
        if hasattr(node, 'source_chunks'):
            chunk_coverage.update(node.source_chunks)
    
    return {
        "original_concepts_count": original_count,
        "merged_concepts_count": merged_count,
        "reduction_ratio": reduction_ratio,
        "avg_confidence": avg_confidence,
        "max_confidence": max_confidence,
        "min_confidence": min_confidence,
        "category_distribution": category_counts,
        "chunk_coverage": len(chunk_coverage),
        "total_chunks": len(chunk_nodes),
        "processing_time": datetime.now().isoformat()
    }

def export_merge_overview(merged_concept_nodes: List, merge_stats: Dict[str, Any], base_name: str, output_dir: str) -> str:
    """å¯¼å‡ºæ¦‚å¿µåˆå¹¶æ€»è§ˆæ–‡ä»¶"""
    
    overview_path = os.path.join(output_dir, f"{base_name}_concept_merge_overview.txt")
    
    with open(overview_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("æ¦‚å¿µåˆå¹¶(Concept Merge)ç»“æœæ€»è§ˆ\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"æ–‡æ¡£: {base_name}\n")
        f.write(f"å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"å¤„ç†å™¨: core/concept_merger.py ConceptMerger\n")
        f.write("=" * 80 + "\n\n")
        
        # ç»Ÿè®¡ä¿¡æ¯
        f.write("ğŸ“Š åˆå¹¶ç»Ÿè®¡ä¿¡æ¯:\n\n")
        f.write(f"  åŸå§‹æ¦‚å¿µæ€»æ•°: {merge_stats['original_concepts_count']}\n")
        f.write(f"  åˆå¹¶åæ¦‚å¿µæ•°: {merge_stats['merged_concepts_count']}\n")
        f.write(f"  æ¦‚å¿µå‡å°‘æ•°é‡: {merge_stats['original_concepts_count'] - merge_stats['merged_concepts_count']}\n")
        f.write(f"  åˆå¹¶å‡å°‘ç‡: {merge_stats['reduction_ratio']:.1%}\n")
        f.write(f"  æ¶‰åŠchunkæ•°: {merge_stats['chunk_coverage']} / {merge_stats['total_chunks']}\n")
        f.write(f"  å¹³å‡ç½®ä¿¡åº¦: {merge_stats['avg_confidence']:.3f}\n")
        f.write(f"  æœ€é«˜ç½®ä¿¡åº¦: {merge_stats['max_confidence']:.3f}\n")
        f.write(f"  æœ€ä½ç½®ä¿¡åº¦: {merge_stats['min_confidence']:.3f}\n\n")
        
        # ç±»åˆ«åˆ†å¸ƒ
        f.write("ğŸ“‚ æ¦‚å¿µç±»åˆ«åˆ†å¸ƒ:\n\n")
        for category, count in merge_stats['category_distribution'].items():
            percentage = count / merge_stats['merged_concepts_count'] * 100
            f.write(f"  {category}: {count} ä¸ªæ¦‚å¿µ ({percentage:.1f}%)\n")
        f.write("\n")
        
        f.write("=" * 80 + "\n\n")
        
        # åˆå¹¶åçš„æ¦‚å¿µåˆ—è¡¨
        f.write("ğŸ§  åˆå¹¶åçš„æ¦‚å¿µåˆ—è¡¨:\n\n")
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        sorted_concepts = sorted(merged_concept_nodes, key=lambda x: getattr(x, 'confidence_score', 0), reverse=True)
        
        for i, node in enumerate(sorted_concepts, 1):
            concept_name = getattr(node, 'concept_name', node.text[:50])
            confidence = getattr(node, 'confidence_score', 0)
            category = getattr(node, 'category', 'æœªåˆ†ç±»')
            source_chunks = getattr(node, 'source_chunks', [])
            keywords = getattr(node, 'keywords', [])
            
            f.write(f"{i:3d}. ã€{category}ã€‘{concept_name}\n")
            f.write(f"     ç½®ä¿¡åº¦: {confidence:.3f}\n")
            f.write(f"     æ¥æºchunks: {len(source_chunks)} ä¸ª\n")
            if keywords:
                f.write(f"     å…³é”®è¯: {', '.join(keywords[:5])}\n")
            f.write(f"     æ¦‚å¿µæ–‡æœ¬: {node.text[:100]}{'...' if len(node.text) > 100 else ''}\n")
            f.write("\n")
        
        f.write("=" * 80 + "\n")
        f.write("ğŸ“„ æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯: è¯·å‚è€ƒ concepts/ ç›®å½•ä¸‹çš„å•ç‹¬æ¦‚å¿µæ–‡ä»¶\n")
        f.write("ğŸ“‹ JSONæ ¼å¼æ•°æ®: è¯·å‚è€ƒå¯¹åº”çš„ _merge_data.json æ–‡ä»¶\n")
        f.write("=" * 80 + "\n")
    
    return overview_path

def export_individual_concepts(merged_concept_nodes: List, chunk_nodes: List, base_name: str, output_dir: str) -> List[str]:
    """å¯¼å‡ºæ¯ä¸ªåˆå¹¶åæ¦‚å¿µçš„è¯¦ç»†æ–‡ä»¶"""
    
    concept_files = []
    concepts_dir = os.path.join(output_dir, "concepts")
    
    # åˆ›å»ºchunkæ–‡æœ¬æ˜ å°„
    chunk_text_map = {}
    for node in chunk_nodes:
        chunk_id = node.metadata.get("chunk_id", node.node_id)
        chunk_text_map[chunk_id] = node.text
    
    for i, concept_node in enumerate(merged_concept_nodes):
        # åˆ›å»ºæ–‡ä»¶å
        concept_name = getattr(concept_node, 'concept_name', f'concept_{i}')
        safe_name = "".join(c for c in concept_name if c.isalnum() or c in (' ', '-', '_')).strip()
        safe_name = safe_name.replace(' ', '_')[:50]  # é™åˆ¶é•¿åº¦
        
        concept_filename = f"{base_name}_concept_{i:03d}_{safe_name}.txt"
        concept_path = os.path.join(concepts_dir, concept_filename)
        
        with open(concept_path, 'w', encoding='utf-8') as f:
            # å¤´éƒ¨ä¿¡æ¯
            f.write("=" * 80 + "\n")
            f.write("æ¦‚å¿µåˆå¹¶(Concept Merge)è¯¦ç»†ç»“æœ\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"æ¦‚å¿µç¼–å·: {i+1}\n")
            f.write(f"æ¦‚å¿µID: {getattr(concept_node, 'node_id', 'N/A')}\n")
            f.write(f"æ¦‚å¿µåç§°: {getattr(concept_node, 'concept_name', 'N/A')}\n")
            f.write(f"æ¦‚å¿µç±»åˆ«: {getattr(concept_node, 'category', 'æœªåˆ†ç±»')}\n")
            f.write(f"ç½®ä¿¡åº¦åˆ†æ•°: {getattr(concept_node, 'confidence_score', 0):.3f}\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"å¤„ç†å™¨: core/concept_merger.py ConceptMerger\n")
            f.write("=" * 80 + "\n\n")
            
            # æ¦‚å¿µè¯¦ç»†ä¿¡æ¯
            f.write("ğŸ§  æ¦‚å¿µè¯¦ç»†ä¿¡æ¯:\n\n")
            f.write(f"å®Œæ•´æ¦‚å¿µæ–‡æœ¬:\n")
            f.write(f"{concept_node.text}\n\n")
            
            if hasattr(concept_node, 'definition') and concept_node.definition:
                f.write(f"æ¦‚å¿µå®šä¹‰:\n")
                f.write(f"{concept_node.definition}\n\n")
            
            # å…³é”®è¯
            keywords = getattr(concept_node, 'keywords', [])
            if keywords:
                f.write(f"ğŸ”‘ å…³é”®è¯:\n")
                for j, keyword in enumerate(keywords, 1):
                    f.write(f"  {j}. {keyword}\n")
                f.write("\n")
            
            # æ¥æºä¿¡æ¯
            source_chunks = getattr(concept_node, 'source_chunks', [])
            if source_chunks:
                f.write(f"ğŸ“„ æ¥æºchunkä¿¡æ¯ (å…±{len(source_chunks)}ä¸ª):\n\n")
                for j, chunk_id in enumerate(source_chunks, 1):
                    f.write(f"  {j}. Chunk ID: {chunk_id}\n")
                    if chunk_id in chunk_text_map:
                        chunk_text = chunk_text_map[chunk_id]
                        preview = chunk_text[:200] + "..." if len(chunk_text) > 200 else chunk_text
                        f.write(f"     æ–‡æœ¬é¢„è§ˆ: {preview}\n")
                    f.write("\n")
            
            # å…ƒæ•°æ®ä¿¡æ¯
            if hasattr(concept_node, 'metadata') and concept_node.metadata:
                f.write(f"ğŸ”§ æŠ€æœ¯å…ƒæ•°æ®:\n\n")
                for key, value in concept_node.metadata.items():
                    if key not in ['concept_name', 'definition', 'keywords', 'category']:
                        f.write(f"  {key}: {value}\n")
                f.write("\n")
            
            f.write("=" * 80 + "\n")
            f.write("è¯´æ˜: æ­¤æ¦‚å¿µæ˜¯é€šè¿‡ConceptMergeråˆå¹¶ç›¸ä¼¼æ¦‚å¿µåç”Ÿæˆçš„\n")
            f.write("=" * 80 + "\n")
        
        concept_files.append(concept_path)
        logger.info(f"   ğŸ’¾ å·²ä¿å­˜æ¦‚å¿µæ–‡ä»¶: {concept_filename}")
    
    return concept_files

def export_merge_json(merged_concept_nodes: List, merge_stats: Dict[str, Any], base_name: str, output_dir: str) -> str:
    """å¯¼å‡ºJSONæ ¼å¼çš„æ¦‚å¿µåˆå¹¶æ•°æ®"""
    
    json_path = os.path.join(output_dir, f"{base_name}_concept_merge_data.json")
    
    # æ„å»ºJSONæ•°æ®
    json_data = {
        "metadata": {
            "document_name": base_name,
            "processing_time": datetime.now().isoformat(),
            "processor": "core/concept_merger.py ConceptMerger",
            "statistics": merge_stats
        },
        "merged_concepts": []
    }
    
    for i, node in enumerate(merged_concept_nodes):
        concept_data = {
            "index": i,
            "concept_id": getattr(node, 'node_id', f'concept_{i}'),
            "concept_name": getattr(node, 'concept_name', ''),
            "concept_text": node.text,
            "definition": getattr(node, 'definition', ''),
            "category": getattr(node, 'category', 'æœªåˆ†ç±»'),
            "confidence_score": getattr(node, 'confidence_score', 0),
            "keywords": getattr(node, 'keywords', []),
            "source_chunks": getattr(node, 'source_chunks', []),
            "metadata": getattr(node, 'metadata', {})
        }
        json_data["merged_concepts"].append(concept_data)
    
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)
    
    return json_path

def generate_merge_report(pdf_path: str, config_path: str, merge_stats: Dict[str, Any], 
                         concept_files: List[str], overview_path: str, json_path: str, 
                         base_name: str, output_dir: str) -> str:
    """ç”Ÿæˆæ¦‚å¿µåˆå¹¶çš„è¯¦ç»†æŠ¥å‘Š"""
    
    report_path = os.path.join(output_dir, f"{base_name}_concept_merge_report.md")
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(f"# æ¦‚å¿µåˆå¹¶(Concept Merge)å¤„ç†æŠ¥å‘Š\n\n")
        f.write(f"**å¤„ç†æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write(f"## ğŸ“„ è¾“å…¥ä¿¡æ¯\n\n")
        f.write(f"- **PDFæ–‡ä»¶**: `{pdf_path}`\n")
        f.write(f"- **é…ç½®æ–‡ä»¶**: `{config_path}`\n")
        f.write(f"- **å¤„ç†æµç¨‹**: Semantic Chunking â†’ Concept Extraction â†’ Concept Merging\n")
        f.write(f"- **æ ¸å¿ƒå¤„ç†å™¨**: `core/concept_merger.py` ä¸­çš„ `ConceptMerger`\n\n")
        
        f.write(f"## ğŸ“Š åˆå¹¶ç»Ÿè®¡ä¿¡æ¯\n\n")
        f.write(f"### æ¦‚å¿µæ•°é‡å˜åŒ–\n")
        f.write(f"- **åŸå§‹æ¦‚å¿µæ€»æ•°**: {merge_stats['original_concepts_count']}\n")
        f.write(f"- **åˆå¹¶åæ¦‚å¿µæ•°**: {merge_stats['merged_concepts_count']}\n")
        f.write(f"- **å‡å°‘çš„æ¦‚å¿µæ•°**: {merge_stats['original_concepts_count'] - merge_stats['merged_concepts_count']}\n")
        f.write(f"- **åˆå¹¶å‡å°‘ç‡**: {merge_stats['reduction_ratio']:.1%}\n\n")
        
        f.write(f"### è´¨é‡æŒ‡æ ‡\n")
        f.write(f"- **å¹³å‡ç½®ä¿¡åº¦**: {merge_stats['avg_confidence']:.3f}\n")
        f.write(f"- **æœ€é«˜ç½®ä¿¡åº¦**: {merge_stats['max_confidence']:.3f}\n")
        f.write(f"- **æœ€ä½ç½®ä¿¡åº¦**: {merge_stats['min_confidence']:.3f}\n")
        f.write(f"- **æ¶‰åŠchunkæ•°**: {merge_stats['chunk_coverage']} / {merge_stats['total_chunks']} ({merge_stats['chunk_coverage']/merge_stats['total_chunks']:.1%})\n\n")
        
        f.write(f"### æ¦‚å¿µç±»åˆ«åˆ†å¸ƒ\n")
        for category, count in merge_stats['category_distribution'].items():
            percentage = count / merge_stats['merged_concepts_count'] * 100
            f.write(f"- **{category}**: {count} ä¸ªæ¦‚å¿µ ({percentage:.1f}%)\n")
        f.write("\n")
        
        f.write(f"## ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶\n\n")
        f.write(f"### æ€»è§ˆæ–‡ä»¶\n")
        f.write(f"- `{os.path.basename(overview_path)}` - æ¦‚å¿µåˆå¹¶æ€»è§ˆå’Œæ¦‚å¿µåˆ—è¡¨\n\n")
        
        f.write(f"### è¯¦ç»†æ¦‚å¿µæ–‡ä»¶ ({len(concept_files)}ä¸ª)\n")
        for i, concept_file in enumerate(concept_files, 1):
            filename = os.path.basename(concept_file)
            f.write(f"{i:3d}. `{filename}`\n")
        f.write("\n")
        
        f.write(f"### æ•°æ®æ–‡ä»¶\n")
        f.write(f"- `{os.path.basename(json_path)}` - JSONæ ¼å¼çš„å®Œæ•´æ¦‚å¿µåˆå¹¶æ•°æ®\n\n")
        
        f.write(f"## ğŸ”§ æŠ€æœ¯è¯´æ˜\n\n")
        f.write(f"æ­¤æ¦‚å¿µåˆå¹¶ç»“æœæ˜¯é€šè¿‡ä»¥ä¸‹æ­¥éª¤ç”Ÿæˆçš„ï¼š\n\n")
        f.write(f"1. **æ–‡æ¡£åˆ†å—**: ä½¿ç”¨ `SemanticChunker` å°†æ–‡æ¡£åˆ†è§£ä¸ºè¯­ä¹‰å—\n")
        f.write(f"2. **æ¦‚å¿µæå–**: ä»æ¯ä¸ªchunkä¸­æå–æ¦‚å¿µ\n")
        f.write(f"3. **æ¦‚å¿µæ”¶é›†**: æ”¶é›†æ‰€æœ‰chunkçº§åˆ«çš„æ¦‚å¿µ\n")
        f.write(f"4. **ç›¸ä¼¼åº¦è®¡ç®—**: ä½¿ç”¨LlamaIndexçš„åµŒå…¥ç³»ç»Ÿè®¡ç®—æ¦‚å¿µç›¸ä¼¼åº¦\n")
        f.write(f"5. **æ¦‚å¿µèšç±»**: åŸºäºç›¸ä¼¼åº¦é˜ˆå€¼è¿›è¡Œæ¦‚å¿µèšç±»\n")
        f.write(f"6. **æ¦‚å¿µåˆå¹¶**: ä½¿ç”¨LLMåˆå¹¶ç›¸ä¼¼æ¦‚å¿µçš„æ–‡æœ¬è¡¨è¿°\n")
        f.write(f"7. **è´¨é‡è¯„ä¼°**: è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°å’Œæ¦‚å¿µåˆ†ç±»\n\n")
        
        f.write(f"## ğŸ“– æ–‡ä»¶è¯´æ˜\n\n")
        f.write(f"### æ€»è§ˆæ–‡ä»¶å†…å®¹\n")
        f.write(f"- ğŸ“Š å®Œæ•´çš„åˆå¹¶ç»Ÿè®¡ä¿¡æ¯\n")
        f.write(f"- ğŸ“‚ æ¦‚å¿µç±»åˆ«åˆ†å¸ƒ\n")
        f.write(f"- ğŸ§  æ‰€æœ‰åˆå¹¶åæ¦‚å¿µçš„åˆ—è¡¨ï¼ˆæŒ‰ç½®ä¿¡åº¦æ’åºï¼‰\n\n")
        
        f.write(f"### å•ç‹¬æ¦‚å¿µæ–‡ä»¶å†…å®¹\n")
        f.write(f"æ¯ä¸ªæ¦‚å¿µæ–‡ä»¶ (`concepts/` ç›®å½•) åŒ…å«ï¼š\n")
        f.write(f"- ğŸ§  å®Œæ•´çš„æ¦‚å¿µæ–‡æœ¬å’Œå®šä¹‰\n")
        f.write(f"- ğŸ”‘ æå–çš„å…³é”®è¯\n")
        f.write(f"- ğŸ“„ æ¥æºchunkä¿¡æ¯å’Œæ–‡æœ¬é¢„è§ˆ\n")
        f.write(f"- ğŸ”§ æŠ€æœ¯å…ƒæ•°æ®å’Œå¤„ç†ä¿¡æ¯\n\n")
        
        f.write(f"### ä½¿ç”¨å»ºè®®\n")
        f.write(f"1. **å¿«é€Ÿæµè§ˆ**: å…ˆæŸ¥çœ‹æ€»è§ˆæ–‡ä»¶äº†è§£æ•´ä½“æƒ…å†µ\n")
        f.write(f"2. **æ·±å…¥ç ”ç©¶**: æŸ¥çœ‹å•ç‹¬æ¦‚å¿µæ–‡ä»¶äº†è§£å…·ä½“æ¦‚å¿µ\n")
        f.write(f"3. **æ•°æ®åˆ†æ**: ä½¿ç”¨JSONæ–‡ä»¶è¿›è¡Œç¨‹åºåŒ–åˆ†æ\n")
        f.write(f"4. **è´¨é‡æ£€æŸ¥**: å…³æ³¨ç½®ä¿¡åº¦è¾ƒä½çš„æ¦‚å¿µï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨è°ƒæ•´\n\n")
        
        f.write(f"## ğŸ¯ æ¦‚å¿µåˆå¹¶çš„ä»·å€¼\n\n")
        f.write(f"é€šè¿‡æ¦‚å¿µåˆå¹¶ï¼Œæˆ‘ä»¬å®ç°äº†ï¼š\n")
        f.write(f"- **å»é‡**: æ¶ˆé™¤äº†é‡å¤å’Œç›¸ä¼¼çš„æ¦‚å¿µ\n")
        f.write(f"- **æŠ½è±¡åŒ–**: å°†å…·ä½“æ¦‚å¿µæå‡åˆ°æ›´é€šç”¨çš„å±‚æ¬¡\n")
        f.write(f"- **ç»“æ„åŒ–**: å»ºç«‹äº†æ¸…æ™°çš„æ¦‚å¿µå±‚æ¬¡å’Œåˆ†ç±»\n")
        f.write(f"- **å¯æ£€ç´¢**: æä¾›äº†é«˜è´¨é‡çš„æ¦‚å¿µç´¢å¼•ç”¨äºåç»­æ£€ç´¢\n\n")
        
        f.write(f"è¿™äº›åˆå¹¶åçš„æ¦‚å¿µå¯ä»¥ç”¨äºï¼š\n")
        f.write(f"- ğŸ“ æ–‡æ¡£æ‘˜è¦ç”Ÿæˆ\n")
        f.write(f"- ğŸ” æ™ºèƒ½æ£€ç´¢å’Œé—®ç­”\n")
        f.write(f"- ğŸ“Š çŸ¥è¯†å›¾è°±æ„å»º\n")
        f.write(f"- ğŸ“ è‡ªåŠ¨åŒ–å­¦ä¹ å†…å®¹ç”Ÿæˆ\n")
    
    return report_path

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="å¯¼å‡ºæ¦‚å¿µåˆå¹¶(Concept Merge)çš„å¤„ç†ç»“æœ")
    parser.add_argument("--pdf", required=True, help="PDFæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--config", default="config/config.yml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", default="./concept_merge_export", help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    # æ‰§è¡Œå¯¼å‡º
    result = export_concept_merge_results(args.pdf, args.config, args.output)
    
    if not result.get("success"):
        exit(1)

if __name__ == "__main__":
    main() 