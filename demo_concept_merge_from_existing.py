"""
åŸºäºå·²æœ‰chunkingç»“æœçš„æ¦‚å¿µåˆå¹¶æ¼”ç¤º
====================================

è¿™ä¸ªè„šæœ¬è¯»å–å·²æœ‰çš„chunkingç»“æœï¼Œæ¨¡æ‹Ÿæ¦‚å¿µåˆå¹¶è¿‡ç¨‹ï¼Œ
å¹¶å°†ç»“æœå¯¼å‡ºä¸ºå¯è¯»çš„txtæ–‡ä»¶
"""

import os
import json
import re
from datetime import datetime
from typing import List, Dict, Any
from collections import Counter

def demo_concept_merge_from_chunking():
    """åŸºäºå·²æœ‰çš„chunkingç»“æœæ¼”ç¤ºæ¦‚å¿µåˆå¹¶"""
    
    # è¯»å–chunkingç»“æœ
    chunking_dir = "./chunking_export/chunks"
    output_dir = "./concept_merge_demo"
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(os.path.join(output_dir, "concepts"), exist_ok=True)
    
    print("ğŸ”„ å¼€å§‹åŸºäºå·²æœ‰chunkingç»“æœçš„æ¦‚å¿µåˆå¹¶æ¼”ç¤º...")
    print(f"ğŸ“ è¾“å…¥ç›®å½•: {chunking_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print("=" * 80)
    
    # 1. æ”¶é›†æ‰€æœ‰æ¦‚å¿µ
    print("ğŸ“Š æ­¥éª¤1: æ”¶é›†æ‰€æœ‰chunkä¸­çš„æ¦‚å¿µ...")
    all_concepts, concept_to_chunks = collect_concepts_from_files(chunking_dir)
    print(f"   âœ… æ€»å…±æ”¶é›†åˆ° {len(all_concepts)} ä¸ªæ¦‚å¿µ")
    
    # 2. æ¦‚å¿µé¢‘ç‡åˆ†æå’Œåˆå¹¶
    print("ğŸ”— æ­¥éª¤2: åˆ†ææ¦‚å¿µé¢‘ç‡å’Œç›¸ä¼¼æ€§...")
    merged_concepts = simulate_concept_merging(all_concepts, concept_to_chunks)
    print(f"   âœ… åˆå¹¶åå¾—åˆ° {len(merged_concepts)} ä¸ªæ¦‚å¿µ")
    
    # 3. ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
    print("ğŸ“ˆ æ­¥éª¤3: ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯...")
    stats = generate_merge_statistics(all_concepts, merged_concepts, concept_to_chunks)
    print(f"   âœ… ç»Ÿè®¡å®Œæˆ")
    
    # 4. å¯¼å‡ºæ€»è§ˆæ–‡ä»¶
    print("ğŸ’¾ æ­¥éª¤4: å¯¼å‡ºæ¦‚å¿µåˆå¹¶æ€»è§ˆ...")
    overview_path = export_demo_overview(merged_concepts, stats, output_dir)
    print(f"   âœ… æ€»è§ˆæ–‡ä»¶: {overview_path}")
    
    # 5. å¯¼å‡ºè¯¦ç»†æ¦‚å¿µæ–‡ä»¶
    print("ğŸ“„ æ­¥éª¤5: å¯¼å‡ºè¯¦ç»†æ¦‚å¿µæ–‡ä»¶...")
    concept_files = export_demo_concepts(merged_concepts, concept_to_chunks, chunking_dir, output_dir)
    print(f"   âœ… å·²å¯¼å‡º {len(concept_files)} ä¸ªæ¦‚å¿µæ–‡ä»¶")
    
    # 6. å¯¼å‡ºJSONæ•°æ®
    print("ğŸ“‹ æ­¥éª¤6: å¯¼å‡ºJSONæ•°æ®...")
    json_path = export_demo_json(merged_concepts, stats, output_dir)
    print(f"   âœ… JSONæ–‡ä»¶: {json_path}")
    
    # 7. ç”ŸæˆæŠ¥å‘Š
    print("ğŸ“– æ­¥éª¤7: ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š...")
    report_path = generate_demo_report(stats, overview_path, concept_files, json_path, output_dir)
    print(f"   âœ… æŠ¥å‘Šæ–‡ä»¶: {report_path}")
    
    print(f"\nâœ… æ¦‚å¿µåˆå¹¶æ¼”ç¤ºå®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ“Š æ€»è§ˆæ–‡ä»¶: {overview_path}")
    print(f"ğŸ“‹ è¯¦ç»†æŠ¥å‘Š: {report_path}")
    print(f"\nğŸ“ˆ åˆå¹¶ç»Ÿè®¡:")
    print(f"   - åŸå§‹æ¦‚å¿µæ•°: {stats['original_concepts_count']}")
    print(f"   - åˆå¹¶åæ¦‚å¿µæ•°: {stats['merged_concepts_count']}")
    print(f"   - åˆå¹¶å‡å°‘ç‡: {stats['reduction_ratio']:.1%}")
    print(f"   - é«˜é¢‘æ¦‚å¿µæ•°: {stats['high_frequency_concepts']}")

def collect_concepts_from_files(chunking_dir: str) -> tuple:
    """ä»chunkingæ–‡ä»¶ä¸­æ”¶é›†æ¦‚å¿µ"""
    all_concepts = []
    concept_to_chunks = {}
    
    # æ‰«ææ‰€æœ‰chunkæ–‡ä»¶
    if not os.path.exists(chunking_dir):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {chunking_dir}")
        return [], {}
    
    for filename in os.listdir(chunking_dir):
        if filename.endswith('.txt'):
            file_path = os.path.join(chunking_dir, filename)
            chunk_id = filename.replace('.txt', '')
            
            # è¯»å–æ–‡ä»¶å¹¶æå–æ¦‚å¿µ
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                concepts = extract_concepts_from_content(content)
                
                for concept in concepts:
                    all_concepts.append(concept)
                    if concept not in concept_to_chunks:
                        concept_to_chunks[concept] = []
                    concept_to_chunks[concept].append(chunk_id)
    
    return all_concepts, concept_to_chunks

def extract_concepts_from_content(content: str) -> List[str]:
    """ä»chunkæ–‡ä»¶å†…å®¹ä¸­æå–æ¦‚å¿µ"""
    concepts = []
    
    # æŸ¥æ‰¾æ¦‚å¿µéƒ¨åˆ†
    concept_section = False
    lines = content.split('\n')
    
    for line in lines:
        line = line.strip()
        if 'ğŸ§  æå–çš„æ¦‚å¿µ' in line:
            concept_section = True
            continue
        elif concept_section and line.startswith('='):
            break
        elif concept_section and line:
            # æå–æ¦‚å¿µï¼ˆå»æ‰åºå·ï¼‰
            match = re.match(r'\s*\d+\.\s*(.+)', line)
            if match:
                concept = match.group(1).strip()
                if concept:
                    concepts.append(concept)
    
    return concepts

def simulate_concept_merging(all_concepts: List[str], concept_to_chunks: Dict[str, List[str]]) -> List[Dict[str, Any]]:
    """æ¨¡æ‹Ÿæ¦‚å¿µåˆå¹¶è¿‡ç¨‹"""
    
    # ç»Ÿè®¡æ¦‚å¿µé¢‘ç‡
    concept_freq = Counter(all_concepts)
    
    # æŒ‰é¢‘ç‡å’Œç›¸ä¼¼æ€§è¿›è¡Œæ¦‚å¿µåˆ†ç»„å’Œåˆå¹¶
    merged_concepts = []
    processed_concepts = set()
    
    # æ ¹æ®é¢‘ç‡æ’åº
    sorted_concepts = sorted(concept_freq.items(), key=lambda x: x[1], reverse=True)
    
    concept_id = 0
    for concept, frequency in sorted_concepts:
        if concept in processed_concepts:
            continue
        
        # å¯»æ‰¾ç›¸ä¼¼æ¦‚å¿µè¿›è¡Œåˆå¹¶
        similar_concepts = find_similar_concepts(concept, concept_freq, processed_concepts)
        processed_concepts.update(similar_concepts)
        
        # åˆ›å»ºåˆå¹¶åçš„æ¦‚å¿µ
        merged_concept = {
            "id": f"merged_concept_{concept_id:03d}",
            "name": concept,  # ä½¿ç”¨é¢‘ç‡æœ€é«˜çš„ä½œä¸ºä»£è¡¨
            "text": concept,
            "category": classify_concept(concept),
            "confidence_score": calculate_confidence(concept, frequency, len(all_concepts)),
            "frequency": frequency,
            "source_concepts": similar_concepts,
            "source_chunks": list(set().union(*[concept_to_chunks.get(c, []) for c in similar_concepts])),
            "keywords": extract_keywords(concept)
        }
        
        merged_concepts.append(merged_concept)
        concept_id += 1
    
    return merged_concepts

def find_similar_concepts(target_concept: str, concept_freq: Counter, processed: set) -> List[str]:
    """å¯»æ‰¾ç›¸ä¼¼çš„æ¦‚å¿µ"""
    similar = [target_concept]
    target_lower = target_concept.lower()
    
    for concept, freq in concept_freq.items():
        if concept in processed or concept == target_concept:
            continue
        
        concept_lower = concept.lower()
        
        # ç®€å•çš„ç›¸ä¼¼æ€§åˆ¤æ–­
        if (target_lower in concept_lower or concept_lower in target_lower or
            calculate_word_overlap(target_lower, concept_lower) > 0.5):
            similar.append(concept)
    
    return similar

def calculate_word_overlap(concept1: str, concept2: str) -> float:
    """è®¡ç®—è¯æ±‡é‡å åº¦"""
    words1 = set(concept1.split())
    words2 = set(concept2.split())
    
    if not words1 or not words2:
        return 0.0
    
    intersection = words1.intersection(words2)
    union = words1.union(words2)
    
    return len(intersection) / len(union)

def classify_concept(concept: str) -> str:
    """ç®€å•çš„æ¦‚å¿µåˆ†ç±»"""
    concept_lower = concept.lower()
    
    if any(word in concept_lower for word in ['æ¨¡å‹', 'æ¶æ„', 'ç½‘ç»œ', 'model', 'architecture', 'network']):
        return "æ¨¡å‹æ¶æ„"
    elif any(word in concept_lower for word in ['æœºåˆ¶', 'æ–¹æ³•', 'ç®—æ³•', 'mechanism', 'method', 'algorithm']):
        return "æ–¹æ³•æŠ€æœ¯"
    elif any(word in concept_lower for word in ['ä»»åŠ¡', 'åº”ç”¨', 'task', 'application']):
        return "åº”ç”¨ä»»åŠ¡"
    elif any(word in concept_lower for word in ['è¯„ä¼°', 'æŒ‡æ ‡', 'åˆ†æ•°', 'evaluation', 'metric', 'score']):
        return "è¯„ä¼°æŒ‡æ ‡"
    elif any(word in concept_lower for word in ['æ•°æ®', 'è¯­æ–™', 'data', 'dataset']):
        return "æ•°æ®èµ„æº"
    else:
        return "å…¶ä»–"

def calculate_confidence(concept: str, frequency: int, total_concepts: int) -> float:
    """è®¡ç®—æ¦‚å¿µç½®ä¿¡åº¦"""
    freq_score = frequency / total_concepts
    length_score = min(len(concept.split()) / 5, 1.0)  # åå¥½å¤šè¯æ¦‚å¿µ
    return (freq_score + length_score) / 2

def extract_keywords(concept: str) -> List[str]:
    """æå–æ¦‚å¿µå…³é”®è¯"""
    # ç®€å•çš„å…³é”®è¯æå–
    words = concept.split()
    
    # è¿‡æ»¤åœç”¨è¯
    stop_words = {'çš„', 'å’Œ', 'ä¸', 'æˆ–', 'æ˜¯', 'æœ‰', 'åœ¨', 'for', 'and', 'or', 'the', 'a', 'an', 'of', 'to', 'in'}
    keywords = [word for word in words if word.lower() not in stop_words and len(word) > 1]
    
    return keywords[:5]  # æœ€å¤š5ä¸ªå…³é”®è¯

def generate_merge_statistics(all_concepts: List[str], merged_concepts: List[Dict], concept_to_chunks: Dict) -> Dict[str, Any]:
    """ç”Ÿæˆåˆå¹¶ç»Ÿè®¡ä¿¡æ¯"""
    original_count = len(all_concepts)
    merged_count = len(merged_concepts)
    reduction_ratio = (original_count - merged_count) / original_count if original_count > 0 else 0
    
    # ç½®ä¿¡åº¦ç»Ÿè®¡
    confidence_scores = [concept['confidence_score'] for concept in merged_concepts]
    avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0
    
    # ç±»åˆ«ç»Ÿè®¡
    categories = [concept['category'] for concept in merged_concepts]
    category_counts = Counter(categories)
    
    # é«˜é¢‘æ¦‚å¿µç»Ÿè®¡ï¼ˆé¢‘ç‡>=2ï¼‰
    high_freq_concepts = len([c for c in merged_concepts if c['frequency'] >= 2])
    
    return {
        "original_concepts_count": original_count,
        "merged_concepts_count": merged_count,
        "reduction_ratio": reduction_ratio,
        "avg_confidence": avg_confidence,
        "max_confidence": max(confidence_scores) if confidence_scores else 0,
        "min_confidence": min(confidence_scores) if confidence_scores else 0,
        "category_distribution": dict(category_counts),
        "high_frequency_concepts": high_freq_concepts,
        "total_chunks": len(set().union(*concept_to_chunks.values())) if concept_to_chunks else 0,
        "processing_time": datetime.now().isoformat()
    }

def export_demo_overview(merged_concepts: List[Dict], stats: Dict, output_dir: str) -> str:
    """å¯¼å‡ºæ¦‚å¿µåˆå¹¶æ€»è§ˆ"""
    overview_path = os.path.join(output_dir, "attention_is_all_you_need_concept_merge_overview.txt")
    
    with open(overview_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("æ¦‚å¿µåˆå¹¶(Concept Merge)ç»“æœæ€»è§ˆ - åŸºäºchunkingç»“æœçš„æ¼”ç¤º\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"æ–‡æ¡£: Attention Is All You Need\n")
        f.write(f"å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ•°æ®æ¥æº: chunking_export/chunks/ ç›®å½•\n")
        f.write(f"å¤„ç†æ–¹å¼: æ¼”ç¤ºç‰ˆæ¦‚å¿µåˆå¹¶ç®—æ³•\n")
        f.write("=" * 80 + "\n\n")
        
        # ç»Ÿè®¡ä¿¡æ¯
        f.write("ğŸ“Š åˆå¹¶ç»Ÿè®¡ä¿¡æ¯:\n\n")
        f.write(f"  åŸå§‹æ¦‚å¿µæ€»æ•°: {stats['original_concepts_count']}\n")
        f.write(f"  åˆå¹¶åæ¦‚å¿µæ•°: {stats['merged_concepts_count']}\n")
        f.write(f"  æ¦‚å¿µå‡å°‘æ•°é‡: {stats['original_concepts_count'] - stats['merged_concepts_count']}\n")
        f.write(f"  åˆå¹¶å‡å°‘ç‡: {stats['reduction_ratio']:.1%}\n")
        f.write(f"  é«˜é¢‘æ¦‚å¿µæ•°: {stats['high_frequency_concepts']} ä¸ªï¼ˆå‡ºç°é¢‘ç‡>=2ï¼‰\n")
        f.write(f"  å¹³å‡ç½®ä¿¡åº¦: {stats['avg_confidence']:.3f}\n")
        f.write(f"  æœ€é«˜ç½®ä¿¡åº¦: {stats['max_confidence']:.3f}\n")
        f.write(f"  æœ€ä½ç½®ä¿¡åº¦: {stats['min_confidence']:.3f}\n\n")
        
        # ç±»åˆ«åˆ†å¸ƒ
        f.write("ğŸ“‚ æ¦‚å¿µç±»åˆ«åˆ†å¸ƒ:\n\n")
        for category, count in stats['category_distribution'].items():
            percentage = count / stats['merged_concepts_count'] * 100 if stats['merged_concepts_count'] > 0 else 0
            f.write(f"  {category}: {count} ä¸ªæ¦‚å¿µ ({percentage:.1f}%)\n")
        f.write("\n")
        
        f.write("=" * 80 + "\n\n")
        
        # åˆå¹¶åçš„æ¦‚å¿µåˆ—è¡¨
        f.write("ğŸ§  åˆå¹¶åçš„æ¦‚å¿µåˆ—è¡¨ï¼ˆæŒ‰ç½®ä¿¡åº¦æ’åºï¼‰:\n\n")
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        sorted_concepts = sorted(merged_concepts, key=lambda x: x['confidence_score'], reverse=True)
        
        for i, concept in enumerate(sorted_concepts, 1):
            f.write(f"{i:3d}. ã€{concept['category']}ã€‘{concept['name']}\n")
            f.write(f"     ç½®ä¿¡åº¦: {concept['confidence_score']:.3f} | ")
            f.write(f"é¢‘ç‡: {concept['frequency']} | ")
            f.write(f"æ¥æºchunks: {len(concept['source_chunks'])} ä¸ª\n")
            if concept['keywords']:
                f.write(f"     å…³é”®è¯: {', '.join(concept['keywords'])}\n")
            if len(concept['source_concepts']) > 1:
                f.write(f"     åˆå¹¶äº†: {len(concept['source_concepts'])} ä¸ªç›¸ä¼¼æ¦‚å¿µ\n")
            f.write("\n")
        
        f.write("=" * 80 + "\n")
        f.write("ğŸ“„ æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯: è¯·å‚è€ƒ concepts/ ç›®å½•ä¸‹çš„å•ç‹¬æ¦‚å¿µæ–‡ä»¶\n")
        f.write("ğŸ“‹ JSONæ ¼å¼æ•°æ®: è¯·å‚è€ƒå¯¹åº”çš„ _merge_data.json æ–‡ä»¶\n")
        f.write("ğŸ’¡ è¿™æ˜¯åŸºäºå·²æœ‰chunkingç»“æœçš„æ¼”ç¤ºç‰ˆconcept mergeæ•ˆæœ\n")
        f.write("=" * 80 + "\n")
    
    return overview_path

def export_demo_concepts(merged_concepts: List[Dict], concept_to_chunks: Dict, 
                        chunking_dir: str, output_dir: str) -> List[str]:
    """å¯¼å‡ºè¯¦ç»†æ¦‚å¿µæ–‡ä»¶"""
    concept_files = []
    concepts_dir = os.path.join(output_dir, "concepts")
    
    # è¯»å–chunkæ–‡æœ¬å†…å®¹
    chunk_texts = {}
    for filename in os.listdir(chunking_dir):
        if filename.endswith('.txt'):
            chunk_id = filename.replace('.txt', '')
            with open(os.path.join(chunking_dir, filename), 'r', encoding='utf-8') as f:
                content = f.read()
                # æå–ä¸»è¦æ–‡æœ¬å†…å®¹
                text_start = content.find("ğŸ“„ Chunk å®Œæ•´æ–‡æœ¬å†…å®¹:")
                if text_start != -1:
                    text_content = content[text_start:]
                    text_end = text_content.find("\n\n============================================================")
                    if text_end != -1:
                        chunk_texts[chunk_id] = text_content[len("ğŸ“„ Chunk å®Œæ•´æ–‡æœ¬å†…å®¹:"):text_end].strip()
                    else:
                        chunk_texts[chunk_id] = text_content[len("ğŸ“„ Chunk å®Œæ•´æ–‡æœ¬å†…å®¹:"):].strip()
    
    for i, concept in enumerate(merged_concepts):
        # åˆ›å»ºå®‰å…¨çš„æ–‡ä»¶å
        safe_name = "".join(c for c in concept['name'] if c.isalnum() or c in (' ', '-', '_')).strip()
        safe_name = safe_name.replace(' ', '_')[:30]  # é™åˆ¶é•¿åº¦
        
        concept_filename = f"concept_{i:03d}_{safe_name}.txt"
        concept_path = os.path.join(concepts_dir, concept_filename)
        
        with open(concept_path, 'w', encoding='utf-8') as f:
            # å¤´éƒ¨ä¿¡æ¯
            f.write("=" * 80 + "\n")
            f.write("æ¦‚å¿µåˆå¹¶(Concept Merge)è¯¦ç»†ç»“æœ - æ¼”ç¤ºç‰ˆ\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"æ¦‚å¿µç¼–å·: {i+1}\n")
            f.write(f"æ¦‚å¿µID: {concept['id']}\n")
            f.write(f"æ¦‚å¿µåç§°: {concept['name']}\n")
            f.write(f"æ¦‚å¿µç±»åˆ«: {concept['category']}\n")
            f.write(f"ç½®ä¿¡åº¦åˆ†æ•°: {concept['confidence_score']:.3f}\n")
            f.write(f"å‡ºç°é¢‘ç‡: {concept['frequency']} æ¬¡\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("=" * 80 + "\n\n")
            
            # æ¦‚å¿µè¯¦ç»†ä¿¡æ¯
            f.write("ğŸ§  æ¦‚å¿µè¯¦ç»†ä¿¡æ¯:\n\n")
            f.write(f"æ¦‚å¿µæ–‡æœ¬: {concept['text']}\n\n")
            
            # å…³é”®è¯
            if concept['keywords']:
                f.write(f"ğŸ”‘ å…³é”®è¯:\n")
                for j, keyword in enumerate(concept['keywords'], 1):
                    f.write(f"  {j}. {keyword}\n")
                f.write("\n")
            
            # åˆå¹¶ä¿¡æ¯
            if len(concept['source_concepts']) > 1:
                f.write(f"ğŸ”— æ¦‚å¿µåˆå¹¶ä¿¡æ¯:\n")
                f.write(f"  æœ¬æ¦‚å¿µç”±ä»¥ä¸‹ {len(concept['source_concepts'])} ä¸ªç›¸ä¼¼æ¦‚å¿µåˆå¹¶è€Œæˆ:\n")
                for j, source in enumerate(concept['source_concepts'], 1):
                    f.write(f"    {j}. {source}\n")
                f.write("\n")
            
            # æ¥æºchunkä¿¡æ¯
            f.write(f"ğŸ“„ æ¥æºchunkä¿¡æ¯ (å…±{len(concept['source_chunks'])}ä¸ª):\n\n")
            for j, chunk_id in enumerate(concept['source_chunks'], 1):
                f.write(f"  {j}. Chunk ID: {chunk_id}\n")
                if chunk_id in chunk_texts:
                    preview = chunk_texts[chunk_id][:300] + "..." if len(chunk_texts[chunk_id]) > 300 else chunk_texts[chunk_id]
                    f.write(f"     æ–‡æœ¬é¢„è§ˆ: {preview}\n")
                f.write("\n")
            
            f.write("=" * 80 + "\n")
            f.write("ğŸ’¡ è¿™æ˜¯åŸºäºchunkingç»“æœçš„æ¼”ç¤ºç‰ˆconcept merge\n")
            f.write("ğŸ’¡ å®é™…çš„æ¦‚å¿µåˆå¹¶ä¼šä½¿ç”¨æ›´å¤æ‚çš„è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—\n")
            f.write("=" * 80 + "\n")
        
        concept_files.append(concept_path)
    
    return concept_files

def export_demo_json(merged_concepts: List[Dict], stats: Dict, output_dir: str) -> str:
    """å¯¼å‡ºJSONæ ¼å¼æ•°æ®"""
    json_path = os.path.join(output_dir, "attention_is_all_you_need_concept_merge_data.json")
    
    json_data = {
        "metadata": {
            "document_name": "Attention Is All You Need",
            "processing_time": datetime.now().isoformat(),
            "processor": "æ¼”ç¤ºç‰ˆæ¦‚å¿µåˆå¹¶ç®—æ³•",
            "source": "chunking_export/chunks/",
            "statistics": stats
        },
        "merged_concepts": merged_concepts
    }
    
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)
    
    return json_path

def generate_demo_report(stats: Dict, overview_path: str, concept_files: List[str], 
                        json_path: str, output_dir: str) -> str:
    """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
    report_path = os.path.join(output_dir, "attention_is_all_you_need_concept_merge_report.md")
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(f"# æ¦‚å¿µåˆå¹¶(Concept Merge)æ¼”ç¤ºæŠ¥å‘Š\n\n")
        f.write(f"**æ–‡æ¡£**: Attention Is All You Need\n")
        f.write(f"**å¤„ç†æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**æ¼”ç¤ºè¯´æ˜**: åŸºäºå·²æœ‰chunkingç»“æœçš„æ¦‚å¿µåˆå¹¶æ¼”ç¤º\n\n")
        
        f.write(f"## ğŸ“Š åˆå¹¶æ•ˆæœç»Ÿè®¡\n\n")
        f.write(f"| æŒ‡æ ‡ | æ•°å€¼ |\n")
        f.write(f"|------|------|\n")
        f.write(f"| åŸå§‹æ¦‚å¿µæ€»æ•° | {stats['original_concepts_count']} |\n")
        f.write(f"| åˆå¹¶åæ¦‚å¿µæ•° | {stats['merged_concepts_count']} |\n")
        f.write(f"| åˆå¹¶å‡å°‘ç‡ | {stats['reduction_ratio']:.1%} |\n")
        f.write(f"| é«˜é¢‘æ¦‚å¿µæ•° | {stats['high_frequency_concepts']} |\n")
        f.write(f"| å¹³å‡ç½®ä¿¡åº¦ | {stats['avg_confidence']:.3f} |\n\n")
        
        f.write(f"## ğŸ“‚ æ¦‚å¿µç±»åˆ«åˆ†å¸ƒ\n\n")
        for category, count in stats['category_distribution'].items():
            percentage = count / stats['merged_concepts_count'] * 100 if stats['merged_concepts_count'] > 0 else 0
            f.write(f"- **{category}**: {count} ä¸ªæ¦‚å¿µ ({percentage:.1f}%)\n")
        f.write("\n")
        
        f.write(f"## ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶\n\n")
        f.write(f"- **æ€»è§ˆæ–‡ä»¶**: `{os.path.basename(overview_path)}`\n")
        f.write(f"- **JSONæ•°æ®**: `{os.path.basename(json_path)}`\n")
        f.write(f"- **è¯¦ç»†æ¦‚å¿µæ–‡ä»¶**: {len(concept_files)} ä¸ª (åœ¨ `concepts/` ç›®å½•)\n\n")
        
        f.write(f"## ğŸ¯ æ¼”ç¤ºè¯´æ˜\n\n")
        f.write(f"è¿™æ˜¯ä¸€ä¸ªåŸºäºå·²æœ‰chunkingç»“æœçš„æ¦‚å¿µåˆå¹¶æ¼”ç¤ºï¼Œå±•ç¤ºäº†concept mergeåŠŸèƒ½çš„æ•ˆæœï¼š\n\n")
        f.write(f"1. **æ¦‚å¿µæ”¶é›†**: ä» `chunking_export/chunks/` ç›®å½•è¯»å–æ‰€æœ‰æ¦‚å¿µ\n")
        f.write(f"2. **é¢‘ç‡åˆ†æ**: ç»Ÿè®¡æ¯ä¸ªæ¦‚å¿µçš„å‡ºç°é¢‘ç‡\n")
        f.write(f"3. **ç›¸ä¼¼æ€§æ£€æµ‹**: åŸºäºè¯æ±‡é‡å åº¦æ£€æµ‹ç›¸ä¼¼æ¦‚å¿µ\n")
        f.write(f"4. **æ¦‚å¿µåˆå¹¶**: å°†ç›¸ä¼¼æ¦‚å¿µåˆå¹¶ä¸ºæ›´é€šç”¨çš„è¡¨è¿°\n")
        f.write(f"5. **åˆ†ç±»æ ‡æ³¨**: ä¸ºæ¯ä¸ªæ¦‚å¿µåˆ†é…ç±»åˆ«æ ‡ç­¾\n")
        f.write(f"6. **ç½®ä¿¡åº¦è®¡ç®—**: åŸºäºé¢‘ç‡å’Œå…¶ä»–å› ç´ è®¡ç®—ç½®ä¿¡åº¦\n\n")
        
        f.write(f"## ğŸ’¡ å®é™…åº”ç”¨ä»·å€¼\n\n")
        f.write(f"é€šè¿‡æ¦‚å¿µåˆå¹¶ï¼Œæˆ‘ä»¬å°† {stats['original_concepts_count']} ä¸ªåŸå§‹æ¦‚å¿µç²¾ç‚¼ä¸º {stats['merged_concepts_count']} ä¸ªé«˜è´¨é‡æ¦‚å¿µï¼Œ")
        f.write(f"å‡å°‘äº† {stats['reduction_ratio']:.1%} çš„å†—ä½™ï¼ŒåŒæ—¶ä¿æŒäº†æ¦‚å¿µçš„å®Œæ•´æ€§å’Œå¯ç”¨æ€§ã€‚\n\n")
        f.write(f"è¿™äº›åˆå¹¶åçš„æ¦‚å¿µå¯ä»¥ç”¨äºï¼š\n")
        f.write(f"- ğŸ“– æ–‡æ¡£çŸ¥è¯†å›¾è°±æ„å»º\n")
        f.write(f"- ğŸ” æ™ºèƒ½æ£€ç´¢ç³»ç»Ÿ\n")
        f.write(f"- ğŸ“ è‡ªåŠ¨æ‘˜è¦ç”Ÿæˆ\n")
        f.write(f"- ğŸ“ å­¦ä¹ å†…å®¹æ¨è\n")
    
    return report_path

if __name__ == "__main__":
    demo_concept_merge_from_chunking() 