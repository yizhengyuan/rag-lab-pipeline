#!/usr/bin/env python3
"""
æµ‹è¯•ä¸¤ä¸ªæ–°æ•°æ®é›†çš„pipelineåŠŸèƒ½å¹¶è¯„ä¼°ç”Ÿæˆæ•°æ®è´¨é‡
"""

import sys
import os
import logging
import json
from typing import List, Dict, Any

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger(__name__)

def analyze_qa_quality(qa_pairs: List[Dict], file_name: str) -> Dict[str, Any]:
    """åˆ†æé—®ç­”å¯¹çš„è´¨é‡"""
    logger.info(f"ğŸ“Š åˆ†æ {file_name} çš„é—®ç­”å¯¹è´¨é‡...")
    
    if not qa_pairs:
        return {
            "total_pairs": 0,
            "quality_score": 0,
            "issues": ["æ²¡æœ‰ç”Ÿæˆé—®ç­”å¯¹"],
            "recommendations": ["æ£€æŸ¥æ•°æ®ç”Ÿæˆé…ç½®å’ŒAPIè¿æ¥"]
        }
    
    quality_analysis = {
        "total_pairs": len(qa_pairs),
        "question_types": {},
        "difficulty_levels": {},
        "quality_issues": [],
        "good_examples": [],
        "quality_score": 0
    }
    
    for i, qa in enumerate(qa_pairs):
        # ç»Ÿè®¡é—®é¢˜ç±»å‹
        q_type = qa.get('type', 'unknown')
        quality_analysis["question_types"][q_type] = quality_analysis["question_types"].get(q_type, 0) + 1
        
        # ç»Ÿè®¡éš¾åº¦çº§åˆ«
        difficulty = qa.get('difficulty', 'unknown')
        quality_analysis["difficulty_levels"][difficulty] = quality_analysis["difficulty_levels"].get(difficulty, 0) + 1
        
        # è´¨é‡æ£€æŸ¥
        question = qa.get('question', '')
        answer = qa.get('answer', '')
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¤„ç†é”™è¯¯
        if 'Unable to provide' in answer or 'processing error' in answer:
            quality_analysis["quality_issues"].append(f"é—®ç­”å¯¹{i+1}: ç”Ÿæˆå¤±è´¥ - {answer}")
        elif len(question) < 10:
            quality_analysis["quality_issues"].append(f"é—®ç­”å¯¹{i+1}: é—®é¢˜è¿‡çŸ­")
        elif len(answer) < 20:
            quality_analysis["quality_issues"].append(f"é—®ç­”å¯¹{i+1}: ç­”æ¡ˆè¿‡çŸ­")
        else:
            quality_analysis["good_examples"].append({
                "question": question,
                "answer": answer[:100] + "..." if len(answer) > 100 else answer,
                "type": q_type,
                "difficulty": difficulty
            })
    
    # è®¡ç®—è´¨é‡å¾—åˆ†
    total_pairs = len(qa_pairs)
    good_pairs = len(quality_analysis["good_examples"])
    quality_analysis["quality_score"] = (good_pairs / total_pairs * 100) if total_pairs > 0 else 0
    
    return quality_analysis

def test_dataset(file_path: str, output_suffix: str) -> Dict[str, Any]:
    """æµ‹è¯•å•ä¸ªæ•°æ®é›†"""
    logger.info(f"ğŸ§ª æµ‹è¯•æ•°æ®é›†: {file_path}")
    
    try:
        from integrated_pipeline_new import ModularIntegratedPipeline
        
        # åˆ›å»ºpipelineï¼Œå¢åŠ é—®é¢˜æ•°é‡ä»¥è·å¾—æ›´å¥½çš„æµ‹è¯•ç»“æœ
        pipeline = ModularIntegratedPipeline(
            config_path="config.yml",
            output_dir=f"./test_quality_{output_suffix}",
            questions_per_type={
                "remember": 2,
                "understand": 2,
                "apply": 1,
                "analyze": 1,
                "evaluate": 0,  # æš‚æ—¶å‡å°‘ä»¥é¿å…APIé—®é¢˜
                "create": 0     # æš‚æ—¶å‡å°‘ä»¥é¿å…APIé—®é¢˜
            }
        )
        
        # å¤„ç†æ–‡æ¡£
        results = pipeline.process_single_file(file_path, save_intermediate=True)
        
        if not results:
            logger.error(f"âŒ {file_path} å¤„ç†å¤±è´¥")
            return {"success": False, "error": "å¤„ç†å¤±è´¥"}
        
        # åˆ†æç»“æœè´¨é‡
        qa_analysis = analyze_qa_quality(results.get('qa_pairs', []), file_path)
        
        # æ„å»ºæµ‹è¯•ç»“æœ
        test_result = {
            "success": True,
            "file_path": file_path,
            "processing_stats": {
                "text_length": results.get('file_info', {}).get('text_length', 0),
                "concepts_count": len(results.get('concept_results', {}).get('concept_nodes', [])),
                "evidence_count": len(results.get('concept_results', {}).get('evidence_nodes', [])),
                "qa_pairs_count": len(results.get('qa_pairs', [])),
                "training_data_count": len(results.get('training_data', []))
            },
            "qa_quality_analysis": qa_analysis
        }
        
        return test_result
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯• {file_path} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return {"success": False, "error": str(e)}

def compare_datasets(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """æ¯”è¾ƒä¸¤ä¸ªæ•°æ®é›†çš„å¤„ç†ç»“æœ"""
    logger.info("ğŸ“Š æ¯”è¾ƒæ•°æ®é›†å¤„ç†ç»“æœ...")
    
    comparison = {
        "dataset_comparison": [],
        "overall_summary": {},
        "recommendations": []
    }
    
    for result in results:
        if not result.get('success'):
            continue
            
        dataset_summary = {
            "file": result['file_path'],
            "text_length": result['processing_stats']['text_length'],
            "qa_quality_score": result['qa_quality_analysis']['quality_score'],
            "total_qa_pairs": result['qa_quality_analysis']['total_pairs'],
            "good_examples_count": len(result['qa_quality_analysis']['good_examples'])
        }
        comparison["dataset_comparison"].append(dataset_summary)
    
    # æ€»ç»“
    if comparison["dataset_comparison"]:
        avg_quality = sum(ds['qa_quality_score'] for ds in comparison["dataset_comparison"]) / len(comparison["dataset_comparison"])
        total_qa_pairs = sum(ds['total_qa_pairs'] for ds in comparison["dataset_comparison"])
        
        comparison["overall_summary"] = {
            "average_quality_score": avg_quality,
            "total_qa_pairs_generated": total_qa_pairs,
            "datasets_processed": len(comparison["dataset_comparison"])
        }
        
        # æ¨è
        if avg_quality < 50:
            comparison["recommendations"].append("è´¨é‡å¾—åˆ†è¾ƒä½ï¼Œå»ºè®®ä¼˜åŒ–APIé…ç½®å’Œæç¤ºè¯")
        if total_qa_pairs < 4:
            comparison["recommendations"].append("ç”Ÿæˆçš„é—®ç­”å¯¹æ•°é‡è¾ƒå°‘ï¼Œå»ºè®®å¢åŠ æ¯ç§ç±»å‹çš„é—®é¢˜æ•°é‡")
        
        comparison["recommendations"].append("è€ƒè™‘å¢åŠ æ›´å¤šç±»å‹çš„è®¤çŸ¥å±‚æ¬¡é—®é¢˜")
        comparison["recommendations"].append("ä¼˜åŒ–æ¦‚å¿µæå–ä»¥è·å¾—æ›´å¥½çš„ä¸Šä¸‹æ–‡ä¿¡æ¯")
    
    return comparison

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹æµ‹è¯•ä¸¤ä¸ªæ–°æ•°æ®é›†çš„è´¨é‡...")
    
    datasets = [
        ("en.wikipedia.org_wiki_IEEE_Frank_Rosenblatt_Award.md", "ieee_award"),
        ("ieeexplore.ieee.org_author_37271220500.md", "ieee_author")
    ]
    
    results = []
    
    for file_path, suffix in datasets:
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ§ª æµ‹è¯•æ•°æ®é›†: {file_path}")
        logger.info(f"{'='*60}")
        
        if not os.path.exists(file_path):
            logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            continue
            
        result = test_dataset(file_path, suffix)
        results.append(result)
        
        # æ˜¾ç¤ºå•ä¸ªæ•°æ®é›†ç»“æœ
        if result.get('success'):
            stats = result['processing_stats']
            qa_analysis = result['qa_quality_analysis']
            
            logger.info(f"âœ… å¤„ç†æˆåŠŸ!")
            logger.info(f"   ğŸ“„ æ–‡æœ¬é•¿åº¦: {stats['text_length']} å­—ç¬¦")
            logger.info(f"   ğŸ§  æ¦‚å¿µæ•°é‡: {stats['concepts_count']}")
            logger.info(f"   ğŸ“ é—®ç­”å¯¹æ•°é‡: {stats['qa_pairs_count']}")
            logger.info(f"   ğŸ† è´¨é‡å¾—åˆ†: {qa_analysis['quality_score']:.1f}%")
            
            if qa_analysis['good_examples']:
                logger.info(f"   ğŸ“‹ ä¼˜è´¨ç¤ºä¾‹:")
                for i, example in enumerate(qa_analysis['good_examples'][:2]):
                    logger.info(f"      {i+1}. [{example['type']}] {example['question']}")
                    logger.info(f"         ç­”æ¡ˆ: {example['answer']}")
            
            if qa_analysis['quality_issues']:
                logger.info(f"   âš ï¸  è´¨é‡é—®é¢˜:")
                for issue in qa_analysis['quality_issues'][:3]:
                    logger.info(f"      - {issue}")
        else:
            logger.error(f"âŒ å¤„ç†å¤±è´¥: {result.get('error', 'Unknown error')}")
    
    # æ¯”è¾ƒç»“æœ
    logger.info(f"\n{'='*60}")
    logger.info("ğŸ“Š æ•°æ®é›†æ¯”è¾ƒåˆ†æ")
    logger.info(f"{'='*60}")
    
    comparison = compare_datasets(results)
    
    if comparison["dataset_comparison"]:
        logger.info("ğŸ“ˆ æ•°æ®é›†å¯¹æ¯”:")
        for ds in comparison["dataset_comparison"]:
            logger.info(f"   {ds['file']}:")
            logger.info(f"     - æ–‡æœ¬é•¿åº¦: {ds['text_length']} å­—ç¬¦")
            logger.info(f"     - é—®ç­”å¯¹æ•°é‡: {ds['total_qa_pairs']}")
            logger.info(f"     - è´¨é‡å¾—åˆ†: {ds['qa_quality_score']:.1f}%")
        
        summary = comparison["overall_summary"]
        logger.info(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        logger.info(f"   - å¹³å‡è´¨é‡å¾—åˆ†: {summary['average_quality_score']:.1f}%")
        logger.info(f"   - æ€»é—®ç­”å¯¹æ•°é‡: {summary['total_qa_pairs_generated']}")
        logger.info(f"   - å¤„ç†æ•°æ®é›†æ•°é‡: {summary['datasets_processed']}")
        
        if comparison["recommendations"]:
            logger.info(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
            for rec in comparison["recommendations"]:
                logger.info(f"   - {rec}")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    output_file = "dataset_quality_analysis.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            "results": results,
            "comparison": comparison
        }, f, ensure_ascii=False, indent=2)
    
    logger.info(f"\nğŸ“ è¯¦ç»†åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # ç»“è®º
    successful_tests = sum(1 for r in results if r.get('success'))
    total_tests = len(results)
    
    logger.info(f"\nğŸ¯ æµ‹è¯•æ€»ç»“: {successful_tests}/{total_tests} æ•°æ®é›†å¤„ç†æˆåŠŸ")
    
    if successful_tests == total_tests and comparison["overall_summary"].get('average_quality_score', 0) > 60:
        logger.info("ğŸ‰ pipelineå¯¹æ–°æ•°æ®é›†çš„å¤„ç†è´¨é‡è‰¯å¥½ï¼")
        return True
    elif successful_tests == total_tests:
        logger.info("âœ… pipelineèƒ½å¤Ÿå¤„ç†æ–°æ•°æ®é›†ï¼Œä½†è´¨é‡æœ‰å¾…æ”¹è¿›")
        return True
    else:
        logger.error("âš ï¸  pipelineåœ¨å¤„ç†æ–°æ•°æ®é›†æ—¶é‡åˆ°é—®é¢˜")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 