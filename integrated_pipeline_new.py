"""
æ•´åˆç‰ˆ Pipeline - ä½¿ç”¨æ–°çš„æ¨¡å—åŒ–ç»“æ„
==============================================================

åŠŸèƒ½ï¼š
1. è¾“å…¥ PDF æˆ–å…¶ä»–æ ¼å¼æ–‡æ¡£
2. ä½¿ç”¨æ¨¡å—åŒ–çš„ ImprovedConceptBasedPipeline è¿›è¡Œæ¦‚å¿µæå–
3. ä½¿ç”¨ data_generate_0526.py çš„ SimpleDataGenerator ç”Ÿæˆé—®ç­”å¯¹
4. è¾“å‡ºå®Œæ•´çš„è®­ç»ƒæ•°æ®

æ›´æ–°ï¼šä½¿ç”¨æ–°çš„æ¨¡å—åŒ–ç»“æ„ï¼Œæ”¯æŒé…ç½®æ–‡ä»¶
"""

import os
import json
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
import traceback

# ä½¿ç”¨æ–°çš„æ¨¡å—åŒ– Pipeline
from pipeline_new import ImprovedConceptBasedPipeline
from config import load_config_from_yaml

# å¯¼å…¥ LlamaIndex ç»„ä»¶
from llama_index.core import SimpleDirectoryReader, Document

# å¯¼å…¥ data_generate_0526.py çš„ç±»å’Œå‡½æ•°
from data_generate_0526 import (
    SimpleDataGenerator, 
    FileProcessor, 
    QUESTIONS_PER_TYPE
)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModularIntegratedPipeline:
    """æ•´åˆç‰ˆ Pipeline - ä½¿ç”¨æ¨¡å—åŒ–ç»“æ„"""
    
    def __init__(self, 
                 config_path: str = "config/config.yml",
                 output_dir: str = "./integrated_output",
                 questions_per_type: Dict[str, int] = None,
                 # å‘åå…¼å®¹å‚æ•°
                 openai_api_key: str = None,
                 base_url: str = None,
                 model_name: str = None):
        """
        åˆå§‹åŒ–æ•´åˆ Pipeline
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            questions_per_type: æ¯ç§è®¤çŸ¥å±‚æ¬¡çš„é—®é¢˜æ•°é‡é…ç½®
            openai_api_key: OpenAI API å¯†é’¥ï¼ˆå‘åå…¼å®¹ï¼‰
            base_url: API åŸºç¡€ URLï¼ˆå‘åå…¼å®¹ï¼‰
            model_name: æ¨¡å‹åç§°ï¼ˆå‘åå…¼å®¹ï¼‰
        """
        self.output_dir = output_dir
        self.questions_per_type = questions_per_type or QUESTIONS_PER_TYPE
        
        logger.info("ğŸ”§ åˆå§‹åŒ–æ¨¡å—åŒ–æ•´åˆ Pipeline")
        
        # åˆå§‹åŒ–æ¦‚å¿µæå– Pipeline
        if openai_api_key or base_url or model_name:
            # å‘åå…¼å®¹æ¨¡å¼
            self.concept_pipeline = ImprovedConceptBasedPipeline(
                openai_api_key=openai_api_key,
                base_url=base_url,
                model_name=model_name
            )
            # ä» Pipeline é…ç½®ä¸­è·å–æ¨¡å‹ä¿¡æ¯
            self.model_name = self.concept_pipeline.config.model_name
        else:
            # æ–°çš„é…ç½®æ–‡ä»¶æ¨¡å¼
            self.concept_pipeline = ImprovedConceptBasedPipeline(config_path=config_path)
            self.model_name = self.concept_pipeline.config.model_name
        
        # åˆå§‹åŒ–é—®ç­”ç”Ÿæˆå™¨
        # ä½¿ç”¨æ¦‚å¿µç®¡é“çš„é…ç½®æ¥åˆå§‹åŒ–é—®ç­”ç”Ÿæˆå™¨
        # ä¸´æ—¶è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä¼ é€’é…ç½®
        os.environ['OPENAI_API_KEY'] = self.concept_pipeline.config.openai_api_key
        if hasattr(self.concept_pipeline.config, 'base_url'):
            os.environ['BASE_URL'] = self.concept_pipeline.config.base_url
        
        self.qa_generator = SimpleDataGenerator(
            model_name=self.model_name,
            questions_per_type=self.questions_per_type
        )
        
        self.file_processor = FileProcessor()
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.ensure_output_dirs()
        
        logger.info(f"   âœ… ä½¿ç”¨æ¨¡å‹: {self.model_name}")
        logger.info(f"   âœ… è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def ensure_output_dirs(self):
        """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
    
    def process_single_file(self, file_path: str, save_intermediate: bool = True) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶çš„å®Œæ•´æµç¨‹
        
        Args:
            file_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
            save_intermediate: æ˜¯å¦ä¿å­˜ä¸­é—´ç»“æœ
            
        Returns:
            åŒ…å«æ‰€æœ‰ç»“æœçš„å­—å…¸
        """
        try:
            file_name = os.path.basename(file_path)
            base_name = os.path.splitext(file_name)[0]
            
            logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path}")
            logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
            logger.info("=" * 80)
            
            # æ­¥éª¤1: åŠ è½½æ–‡æ¡£
            logger.info("ğŸ“„ æ­¥éª¤1: åŠ è½½æ–‡æ¡£...")
            documents = self._load_document(file_path)
            
            # æå–å®Œæ•´æ–‡æœ¬ç”¨äºé—®ç­”ç”Ÿæˆ
            full_text = "\n\n".join([doc.text for doc in documents])
            logger.info(f"   âœ… æ–‡æ¡£åŠ è½½å®Œæˆï¼Œæ€»å­—ç¬¦æ•°: {len(full_text)}")
            
            # æ­¥éª¤2: æ¦‚å¿µæå– Pipeline
            logger.info("ğŸ”¬ æ­¥éª¤2: æ‰§è¡Œæ¨¡å—åŒ–æ¦‚å¿µæå– Pipeline...")
            try:
                concept_results = self.concept_pipeline.run_pipeline(documents)
                
                logger.info(f"   âœ… æ¦‚å¿µæå–å®Œæˆ:")
                logger.info(f"      - Chunks: {len(concept_results['chunk_nodes'])}")
                logger.info(f"      - Concepts: {len(concept_results['concept_nodes'])}")
                logger.info(f"      - Evidence: {len(concept_results['evidence_nodes'])}")
                
                # è·å– Pipeline ç»Ÿè®¡ä¿¡æ¯
                stats = self.concept_pipeline.get_pipeline_statistics()
                logger.info(f"   ğŸ“Š Pipeline ç»Ÿè®¡: {stats}")
                
            except Exception as e:
                logger.error(f"   âŒ æ¦‚å¿µæå–å¤±è´¥: {e}")
                logger.info(f"   ğŸ”„ è·³è¿‡æ¦‚å¿µæå–ï¼Œç»§ç»­é—®ç­”ç”Ÿæˆ...")
                concept_results = {
                    "chunk_nodes": [],
                    "concept_nodes": [],
                    "concept_to_chunks": {},
                    "evidence_nodes": [],
                    "indexes": {}
                }
            
            # æ­¥éª¤3: é—®ç­”å¯¹ç”Ÿæˆ
            logger.info("â“ æ­¥éª¤3: ç”Ÿæˆé—®ç­”å¯¹...")
            qa_pairs = self.qa_generator.generate_qa_pairs_from_text(full_text)
            logger.info(f"   âœ… é—®ç­”å¯¹ç”Ÿæˆå®Œæˆï¼Œå…± {len(qa_pairs)} ä¸ª")
            
            # æ­¥éª¤4: åˆ›å»ºè®­ç»ƒæ•°æ®
            logger.info("ğŸ“Š æ­¥éª¤4: åˆ›å»ºè®­ç»ƒæ•°æ®...")
            training_data = []
            for qa_pair in qa_pairs:
                training_item = self.qa_generator.create_training_data(qa_pair, full_text, file_name)
                if training_item:
                    training_data.append(training_item)
            logger.info(f"   âœ… è®­ç»ƒæ•°æ®åˆ›å»ºå®Œæˆï¼Œå…± {len(training_data)} æ¡")
            
            # æ­¥éª¤5: ä¿å­˜ç»“æœ
            logger.info("ğŸ’¾ æ­¥éª¤5: ä¿å­˜ç»“æœ...")
            results = {
                "file_info": {
                    "file_path": file_path,
                    "file_name": file_name,
                    "base_name": base_name,
                    "text_length": len(full_text)
                },
                "config_info": {
                    "model_name": self.model_name,
                    "questions_per_type": self.questions_per_type,
                    "pipeline_stats": stats if 'stats' in locals() else {}
                },
                "concept_results": concept_results,
                "qa_pairs": qa_pairs,
                "training_data": training_data,
                "processing_time": datetime.now().isoformat()
            }
            
            if save_intermediate:
                self._save_results(results, base_name)
            
            logger.info(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {file_path}")
            logger.info(f"ğŸ“ˆ ç”Ÿæˆç»Ÿè®¡:")
            logger.info(f"   - æ¦‚å¿µæ•°é‡: {len(concept_results['concept_nodes'])}")
            logger.info(f"   - è¯æ®æ•°é‡: {len(concept_results['evidence_nodes'])}")
            logger.info(f"   - é—®ç­”å¯¹æ•°é‡: {len(qa_pairs)}")
            logger.info(f"   - è®­ç»ƒæ•°æ®æ¡æ•°: {len(training_data)}")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            return {}
    
    def _load_document(self, file_path: str) -> List[Document]:
        """åŠ è½½æ–‡æ¡£"""
        ext = os.path.splitext(file_path)[1].lower()
        
        if ext == '.pdf':
            # å¯¹äº PDFï¼Œä½¿ç”¨ LlamaIndex çš„ SimpleDirectoryReader
            reader = SimpleDirectoryReader(input_files=[file_path])
            return reader.load_data()
        else:
            # å¯¹äºå…¶ä»–æ ¼å¼ï¼Œä½¿ç”¨ FileProcessor æå–æ–‡æœ¬ï¼Œç„¶ååˆ›å»º Document
            text = self.file_processor.extract_text(file_path)
            return [Document(text=text, metadata={"file_path": file_path})]
    
    def _save_results(self, results: Dict[str, Any], base_name: str):
        """ä¿å­˜å•ä¸ªæ–‡ä»¶çš„ç»“æœ"""
        # ä¿å­˜æ¦‚å¿µæå–ç»“æœï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if results["concept_results"]["concept_nodes"]:
            concept_output_path = os.path.join(self.output_dir, f"{base_name}_concepts.json")
            self.concept_pipeline.save_results(results["concept_results"], concept_output_path)
        
        # ä¿å­˜é—®ç­”å¯¹ï¼ˆJSONæ ¼å¼ï¼‰
        qa_output_path = os.path.join(self.output_dir, f"{base_name}_qa_pairs.json")
        with open(qa_output_path, 'w', encoding='utf-8') as f:
            json.dump(results["qa_pairs"], f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜è®­ç»ƒæ•°æ®ï¼ˆJSONLæ ¼å¼ï¼‰
        training_output_path = os.path.join(self.output_dir, f"{base_name}_training.jsonl")
        with open(training_output_path, 'w', encoding='utf-8') as f:
            for data in results["training_data"]:
                f.write(json.dumps(data, ensure_ascii=False) + "\n")
        
        # ä¿å­˜å®Œæ•´ç»“æœæ‘˜è¦
        summary_output_path = os.path.join(self.output_dir, f"{base_name}_summary.json")
        summary = {
            "file_info": results["file_info"],
            "config_info": results["config_info"],
            "statistics": {
                "concepts_count": len(results["concept_results"]["concept_nodes"]),
                "evidence_count": len(results["concept_results"]["evidence_nodes"]),
                "qa_pairs_count": len(results["qa_pairs"]),
                "training_data_count": len(results["training_data"])
            },
            "processing_time": results["processing_time"]
        }
        with open(summary_output_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        logger.info(f"   âœ… ç»“æœå·²ä¿å­˜:")
        if results["concept_results"]["concept_nodes"]:
            logger.info(f"      - æ¦‚å¿µæå–: {concept_output_path}")
        logger.info(f"      - é—®ç­”å¯¹: {qa_output_path}")
        logger.info(f"      - è®­ç»ƒæ•°æ®: {training_output_path}")
        logger.info(f"      - æ‘˜è¦: {summary_output_path}")

def add_cli():
    """æ·»åŠ å‘½ä»¤è¡Œæ¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ¨¡å—åŒ–æ•´åˆç‰ˆ Pipeline - æ¦‚å¿µæå– + é—®ç­”å¯¹ç”Ÿæˆ')
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--input', '-i', required=True, help='è¾“å…¥æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„')
    parser.add_argument('--output', '-o', default='./integrated_output', help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('--config', '-c', default='config/config.yml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    # å‘åå…¼å®¹å‚æ•°
    parser.add_argument('--api-key', '-k', help='OpenAI API å¯†é’¥ï¼ˆå‘åå…¼å®¹ï¼‰')
    parser.add_argument('--base-url', '-b', help='API åŸºç¡€ URLï¼ˆå‘åå…¼å®¹ï¼‰')
    parser.add_argument('--model', '-m', help='æ¨¡å‹åç§°ï¼ˆå‘åå…¼å®¹ï¼‰')
    
    # é—®é¢˜æ•°é‡é…ç½®
    parser.add_argument('--remember', type=int, default=2, help='Rememberç±»å‹é—®é¢˜æ•°é‡')
    parser.add_argument('--understand', type=int, default=2, help='Understandç±»å‹é—®é¢˜æ•°é‡')
    parser.add_argument('--apply', type=int, default=1, help='Applyç±»å‹é—®é¢˜æ•°é‡')
    parser.add_argument('--analyze', type=int, default=1, help='Analyzeç±»å‹é—®é¢˜æ•°é‡')
    parser.add_argument('--evaluate', type=int, default=1, help='Evaluateç±»å‹é—®é¢˜æ•°é‡')
    parser.add_argument('--create', type=int, default=1, help='Createç±»å‹é—®é¢˜æ•°é‡')
    
    args = parser.parse_args()
    
    # æ„å»ºé—®é¢˜æ•°é‡é…ç½®
    questions_per_type = {
        "remember": args.remember,
        "understand": args.understand,
        "apply": args.apply,
        "analyze": args.analyze,
        "evaluate": args.evaluate,
        "create": args.create
    }
    
    # åˆ›å»ºæ•´åˆ Pipeline
    pipeline = ModularIntegratedPipeline(
        config_path=args.config,
        output_dir=args.output,
        questions_per_type=questions_per_type,
        # å‘åå…¼å®¹å‚æ•°
        openai_api_key=args.api_key,
        base_url=args.base_url,
        model_name=args.model
    )
    
    logger.info(f"é…ç½®çš„é—®é¢˜æ•°é‡: {questions_per_type}")
    
    # å¤„ç†è¾“å…¥
    if os.path.isdir(args.input):
        # è¿™é‡Œå¯ä»¥æ·»åŠ ç›®å½•å¤„ç†é€»è¾‘
        logger.info("ç›®å½•å¤„ç†åŠŸèƒ½å¾…å®ç°")
    else:
        pipeline.process_single_file(args.input)

if __name__ == "__main__":
    add_cli() 