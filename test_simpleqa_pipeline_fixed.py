"""
SimpleQA Historyæ•°æ®é›† Pipelineæµ‹è¯•è„šæœ¬ - ä¿®å¤ç‰ˆ + QAç”Ÿæˆ
==========================================================

ä¿®å¤å†…å®¹ï¼š
- ä¿®å¤æ–‡æ¡£åŠ è½½é—®é¢˜
- æ”¹ä¸ºæ‰‹åŠ¨è¯»å–markdownæ–‡ä»¶
- æ·»åŠ æ›´è¯¦ç»†çš„é”™è¯¯å¤„ç†
- æ•´åˆé—®ç­”ç”ŸæˆåŠŸèƒ½ï¼šæ¦‚å¿µæå– -> è¯æ®æå– -> é—®ç­”å¯¹ç”Ÿæˆ
"""

import os
import json
import time
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
from pathlib import Path
import traceback
import uuid
from PyPDF2 import PdfReader  # ğŸ†• æ·»åŠ PDFå¤„ç†æ”¯æŒ

# å¯¼å…¥pipelineç›¸å…³æ¨¡å—
from pipeline_new import ImprovedConceptBasedPipeline
from llama_index.core import Document

# ğŸ†• å¯¼å…¥é—®ç­”ç”Ÿæˆæ¨¡å—
from data_generate_0526 import SimpleDataGenerator, FileProcessor

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('simpleqa_test_fixed.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class SimpleQATestSuiteWithQAGeneration:
    """SimpleQAæ•°æ®é›†æµ‹è¯•å¥—ä»¶ - ä¿®å¤ç‰ˆ + QAç”ŸæˆåŠŸèƒ½ + PDFæ”¯æŒ"""
    
    def __init__(self, 
                 data_dir: str = "simpleqa_histroy_md",
                 output_dir: str = "simpleqa_test_results_fixed",
                 config_path: str = None,
                 enable_qa_generation: bool = True,
                 qa_model_name: str = "gpt-4o-mini",
                 questions_per_type: Dict[str, int] = None,
                 # ğŸ†• æ–°å¢å‚æ•°
                 input_file: str = None):
        """
        åˆå§‹åŒ–æµ‹è¯•å¥—ä»¶
        
        Args:
            data_dir: SimpleQAæ•°æ®é›†ç›®å½•
            output_dir: æµ‹è¯•ç»“æœè¾“å‡ºç›®å½•
            config_path: Pipelineé…ç½®æ–‡ä»¶è·¯å¾„
            enable_qa_generation: æ˜¯å¦å¯ç”¨é—®ç­”ç”Ÿæˆ
            qa_model_name: é—®ç­”ç”Ÿæˆä½¿ç”¨çš„æ¨¡å‹
            questions_per_type: æ¯ç§ç±»å‹é—®é¢˜çš„æ•°é‡
            input_file: ğŸ†• å•ä¸ªè¾“å…¥æ–‡ä»¶è·¯å¾„ (æ”¯æŒPDF/TXT/MD)
        """
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.config_path = config_path
        self.enable_qa_generation = enable_qa_generation
        self.input_file = Path(input_file) if input_file else None  # ğŸ†•
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–pipeline
        if config_path:
            self.pipeline = ImprovedConceptBasedPipeline(config_path=config_path)
        else:
            self.pipeline = ImprovedConceptBasedPipeline()
        
        # ğŸ†• åˆå§‹åŒ–é—®ç­”ç”Ÿæˆå™¨
        if self.enable_qa_generation:
            self.questions_per_type = questions_per_type or {
                "remember": 2,
                "understand": 2,
                "apply": 1,
                "analyze": 1,
                "evaluate": 1,
                "create": 1
            }
            self.qa_generator = SimpleDataGenerator(
                model_name=qa_model_name,
                questions_per_type=self.questions_per_type
            )
            logger.info(f"âœ… é—®ç­”ç”Ÿæˆå™¨å·²å¯ç”¨: {qa_model_name}")
            logger.info(f"   é—®é¢˜é…ç½®: {self.questions_per_type}")
        else:
            self.qa_generator = None
            logger.info("âŒ é—®ç­”ç”Ÿæˆå™¨å·²ç¦ç”¨")
        
        # æµ‹è¯•ç»Ÿè®¡
        self.test_stats = {
            "total_questions": 0,
            "successful_tests": 0,
            "failed_tests": 0,
            "total_documents": 0,
            "total_processing_time": 0.0,
            "average_processing_time": 0.0,
            # ğŸ†• QAç”Ÿæˆç»Ÿè®¡
            "total_qa_pairs": 0,
            "qa_generation_time": 0.0,
            "qa_generation_errors": 0
        }
        
        # ğŸ†• ç¡®å®šå¤„ç†æ¨¡å¼
        if self.input_file and self.input_file.exists():
            logger.info(f"ğŸ“„ å•æ–‡ä»¶å¤„ç†æ¨¡å¼: {self.input_file}")
            logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        else:
            logger.info(f"ğŸ“ æ‰¹é‡å¤„ç†æ¨¡å¼: {self.data_dir}")
            logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def get_available_questions(self) -> List[str]:
        """è·å–æ‰€æœ‰å¯ç”¨çš„é—®é¢˜æ–‡ä»¶å¤¹"""
        questions = []
        if self.data_dir.exists():
            for item in self.data_dir.iterdir():
                if item.is_dir() and item.name.startswith("question_"):
                    questions.append(item.name)
        return sorted(questions)
    
    def load_question_documents(self, question_folder: str) -> List[Document]:
        """
        åŠ è½½æŒ‡å®šé—®é¢˜æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡æ¡£ - ä¿®å¤ç‰ˆ
        
        Args:
            question_folder: é—®é¢˜æ–‡ä»¶å¤¹åç§° (å¦‚ 'question_100')
            
        Returns:
            Documentåˆ—è¡¨
        """
        question_path = self.data_dir / question_folder
        if not question_path.exists():
            raise ValueError(f"é—®é¢˜æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {question_path}")
        
        documents = []
        
        # æ‰‹åŠ¨éå†å¹¶åŠ è½½markdownæ–‡ä»¶
        md_files = list(question_path.glob("*.md"))
        logger.info(f"åœ¨ {question_folder} ä¸­æ‰¾åˆ° {len(md_files)} ä¸ªmarkdownæ–‡ä»¶")
        
        for md_file in md_files:
            try:
                # ç›´æ¥è¯»å–æ–‡ä»¶å†…å®¹
                with open(md_file, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read().strip()
                
                # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©º
                if not content:
                    logger.warning(f"æ–‡ä»¶ {md_file.name} å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
                    continue
                
                # åˆ›å»ºDocumentå¯¹è±¡
                doc = Document(
                    text=content,
                    metadata={
                        "file_name": md_file.name,
                        "file_path": str(md_file),
                        "question_folder": question_folder,
                        "source_dataset": "simpleqa_history",
                        "file_size": len(content)
                    }
                )
                documents.append(doc)
                logger.debug(f"âœ… æˆåŠŸåŠ è½½æ–‡ä»¶: {md_file.name} ({len(content)} å­—ç¬¦)")
                
            except Exception as e:
                logger.warning(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {md_file.name}: {e}")
                continue
        
        if not documents:
            logger.warning(f"âš ï¸ é—®é¢˜ {question_folder} æœªåŠ è½½åˆ°ä»»ä½•æœ‰æ•ˆæ–‡æ¡£")
        else:
            total_chars = sum(len(doc.text) for doc in documents)
            logger.info(f"âœ… é—®é¢˜ {question_folder}: æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£ï¼Œæ€»è®¡ {total_chars} å­—ç¬¦")
        
        return documents
    
    def extract_evidence_texts(self, pipeline_results: Dict[str, Any]) -> List[str]:
        """
        ğŸ†• ä»Pipelineç»“æœä¸­æå–Evidenceæ–‡æœ¬
        
        Args:
            pipeline_results: Pipelineå¤„ç†ç»“æœ
            
        Returns:
            Evidenceæ–‡æœ¬åˆ—è¡¨
        """
        logger.info("ğŸ“ æå–Evidenceæ–‡æœ¬ç”¨äºQAç”Ÿæˆ...")
        
        evidence_texts = []
        evidence_nodes = pipeline_results.get("evidence_nodes", [])
        
        for evidence_node in evidence_nodes:
            try:
                # è·å–evidenceæ–‡æœ¬
                evidence_text = evidence_node.text
                if evidence_text and len(evidence_text.strip()) > 20:  # æœ€å°‘20å­—ç¬¦
                    evidence_texts.append(evidence_text.strip())
                    logger.debug(f"æå–Evidence: {evidence_text[:100]}...")
            except Exception as e:
                logger.warning(f"æå–Evidenceæ–‡æœ¬å¤±è´¥: {e}")
        
        logger.info(f"âœ… æå–äº† {len(evidence_texts)} ä¸ªEvidenceæ–‡æœ¬")
        return evidence_texts
    
    def generate_qa_pairs(self, evidence_texts: List[str], question_folder: str) -> List[Dict[str, Any]]:
        """
        ğŸ†• åŸºäºEvidenceæ–‡æœ¬ç”Ÿæˆé—®ç­”å¯¹
        
        Args:
            evidence_texts: Evidenceæ–‡æœ¬åˆ—è¡¨
            question_folder: é—®é¢˜æ–‡ä»¶å¤¹åç§°
            
        Returns:
            é—®ç­”å¯¹åˆ—è¡¨
        """
        if not self.enable_qa_generation or not evidence_texts:
            return []
        
        logger.info("â“ åŸºäºEvidenceç”Ÿæˆé—®ç­”å¯¹...")
        
        all_qa_pairs = []
        qa_start_time = time.time()
        
        for i, evidence_text in enumerate(evidence_texts):
            try:
                logger.info(f"   å¤„ç†Evidence {i+1}/{len(evidence_texts)} ({len(evidence_text)} å­—ç¬¦)...")
                
                # ä¸ºæ¯ä¸ªEvidenceç”Ÿæˆé—®ç­”å¯¹
                qa_pairs = self.qa_generator.generate_qa_pairs_from_text(evidence_text)
                
                # ä¸ºæ¯ä¸ªé—®ç­”å¯¹æ·»åŠ æ¥æºä¿¡æ¯
                for qa_pair in qa_pairs:
                    qa_pair["evidence_source"] = f"{question_folder}_evidence_{i}"
                    qa_pair["evidence_text"] = evidence_text[:200] + "..." if len(evidence_text) > 200 else evidence_text
                    qa_pair["question_folder"] = question_folder
                    qa_pair["generation_timestamp"] = datetime.now().isoformat()
                
                all_qa_pairs.extend(qa_pairs)
                logger.info(f"   ä»Evidence {i+1} ç”Ÿæˆäº† {len(qa_pairs)} ä¸ªé—®ç­”å¯¹")
                
            except Exception as e:
                logger.error(f"ä»Evidence {i+1} ç”Ÿæˆé—®ç­”å¯¹å¤±è´¥: {e}")
                self.test_stats["qa_generation_errors"] += 1
                continue
        
        qa_time = time.time() - qa_start_time
        self.test_stats["total_qa_pairs"] += len(all_qa_pairs)
        self.test_stats["qa_generation_time"] += qa_time
        
        logger.info(f"âœ… ç”Ÿæˆ {len(all_qa_pairs)} ä¸ªé—®ç­”å¯¹ï¼Œè€—æ—¶: {qa_time:.2f}ç§’")
        return all_qa_pairs
    
    def create_training_data(self, qa_pairs: List[Dict[str, Any]], question_folder: str, 
                            pipeline_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        ğŸ†• åˆ›å»ºè®­ç»ƒæ•°æ®æ ¼å¼ - ç¬¦åˆæ ‡å‡†æ ¼å¼
        
        Args:
            qa_pairs: é—®ç­”å¯¹åˆ—è¡¨
            question_folder: é—®é¢˜æ–‡ä»¶å¤¹åç§°
            pipeline_results: Pipelineå¤„ç†ç»“æœï¼ˆç”¨äºæå–Contextï¼‰
            
        Returns:
            è®­ç»ƒæ•°æ®åˆ—è¡¨
        """
        training_data = []
        
        # è·å–pipelineç›¸å…³æ•°æ®
        chunk_nodes = pipeline_results.get("chunk_nodes", [])
        evidence_nodes = pipeline_results.get("evidence_nodes", [])
        concept_to_chunks = pipeline_results.get("concept_to_chunks", {})
        
        for i, qa_pair in enumerate(qa_pairs):
            try:
                # ç”Ÿæˆå”¯ä¸€ID
                unique_id = uuid.uuid4().hex
                
                # æ„å»ºContext - åŸºäºEvidenceå’Œç›¸å…³çš„Chunks
                context = self._build_context_for_qa(
                    qa_pair, 
                    chunk_nodes, 
                    evidence_nodes, 
                    question_folder, 
                    i
                )
                
                # åˆ›å»ºæ ‡å‡†æ ¼å¼çš„è®­ç»ƒæ•°æ®é¡¹
                training_item = {
                    "_id": unique_id,
                    "Question": qa_pair["question"],
                    "Answer": qa_pair["answer"],
                    "Type": qa_pair["type"],
                    "Difficulty": qa_pair["difficulty"],
                    "Domain": getattr(self.qa_generator, 'domain', 'general'),
                    "Rationale": qa_pair.get("rationale", ""),
                    "Context": context
                }
                
                training_data.append(training_item)
                
            except Exception as e:
                logger.warning(f"åˆ›å»ºè®­ç»ƒæ•°æ®é¡¹ {i} å¤±è´¥: {e}")
                continue
        
        logger.info(f"âœ… åˆ›å»ºäº† {len(training_data)} ä¸ªæ ‡å‡†æ ¼å¼è®­ç»ƒæ•°æ®é¡¹")
        return training_data
    
    def _build_context_for_qa(self, qa_pair: Dict[str, Any], chunk_nodes: List, 
                             evidence_nodes: List, question_folder: str, qa_index: int) -> List[Dict[str, Any]]:
        """
        ğŸ†• ä¸ºé—®ç­”å¯¹æ„å»ºContextæ•°ç»„
        
        Args:
            qa_pair: é—®ç­”å¯¹ä¿¡æ¯
            chunk_nodes: æ–‡æ¡£å—èŠ‚ç‚¹åˆ—è¡¨
            evidence_nodes: è¯æ®èŠ‚ç‚¹åˆ—è¡¨
            question_folder: é—®é¢˜æ–‡ä»¶å¤¹åç§°
            qa_index: é—®ç­”å¯¹ç´¢å¼•
            
        Returns:
            Contextæ•°ç»„
        """
        context = []
        chunk_id_counter = 0
        doc_id = f"{question_folder}_doc_{qa_index}"
        
        # 1. å¤„ç†ä¸å½“å‰QAç›¸å…³çš„Evidenceä½œä¸ºæ”¯æŒæ€§chunk
        qa_evidence_source = qa_pair.get("evidence_source", "")
        related_evidence = None
        
        for evidence_node in evidence_nodes:
            if qa_evidence_source in str(evidence_node.metadata.get("source_chunks", [])):
                related_evidence = evidence_node
                break
        
        if related_evidence:
            # æ·»åŠ æ”¯æŒæ€§Evidence chunk
            chunk_id = f"chunk_{chunk_id_counter}"
            evidence_keywords = self._extract_keywords_from_text(
                qa_pair["question"], 
                related_evidence.text
            )
            
            context.append({
                "chunk_id": chunk_id,
                "content": related_evidence.text,
                "type": "fully_support",
                "reason": "Evidence extracted by pipeline that directly supports the question",
                "keywords": evidence_keywords,
                "DocId": doc_id
            })
            chunk_id_counter += 1
        
        # 2. æ·»åŠ ç›¸å…³çš„æ–‡æ¡£chunksä½œä¸ºéƒ¨åˆ†æ”¯æŒ
        max_support_chunks = 2
        added_support_chunks = 0
        
        for chunk_node in chunk_nodes[:5]:  # é™åˆ¶æ£€æŸ¥å‰5ä¸ªchunk
            if added_support_chunks >= max_support_chunks:
                break
            
            # æ£€æŸ¥chunkæ˜¯å¦ä¸é—®é¢˜ç›¸å…³
            if self._is_chunk_relevant_to_question(qa_pair["question"], chunk_node.text):
                chunk_id = f"chunk_{chunk_id_counter}"
                chunk_keywords = self._extract_keywords_from_text(
                    qa_pair["question"], 
                    chunk_node.text
                )
                
                context.append({
                    "chunk_id": chunk_id,
                    "content": chunk_node.text,
                    "type": "partial_support",
                    "reason": "Document chunk with relevant information to the question topic",
                    "keywords": chunk_keywords,
                    "DocId": doc_id
                })
                chunk_id_counter += 1
                added_support_chunks += 1
        
        # 3. æ·»åŠ å¹²æ‰°æ€§chunksï¼ˆconfusedå’Œirrelevantï¼‰
        # ä»å‰©ä½™chunksä¸­é€‰æ‹©ä¸€äº›ä½œä¸ºå¹²æ‰°é¡¹
        remaining_chunks = chunk_nodes[5:8]  # é€‰æ‹©ä¸­é—´çš„ä¸€äº›chunkä½œä¸ºå¹²æ‰°
        
        for i, chunk_node in enumerate(remaining_chunks):
            chunk_id = f"chunk_{chunk_id_counter}"
            
            # äº¤æ›¿æ·»åŠ confusedå’Œirrelevantç±»å‹
            if i % 2 == 0:
                chunk_type = "confused"
                reason = "Contains similar terms but doesn't directly answer the question"
            else:
                chunk_type = "irrelevant"
                reason = "Contains unrelated information to the question"
            
            context.append({
                "chunk_id": chunk_id,
                "content": chunk_node.text,
                "type": chunk_type,
                "reason": reason,
                "keywords": [],  # å¹²æ‰°chunké€šå¸¸ä¸æä¾›å…³é”®è¯
                "DocId": doc_id
            })
            chunk_id_counter += 1
        
        return context
    
    def _extract_keywords_from_text(self, question: str, text: str) -> List[str]:
        """
        ğŸ†• ä»æ–‡æœ¬ä¸­æå–ä¸é—®é¢˜ç›¸å…³çš„å…³é”®è¯
        
        Args:
            question: é—®é¢˜æ–‡æœ¬
            text: è¦æå–å…³é”®è¯çš„æ–‡æœ¬
            
        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        try:
            # ç®€åŒ–çš„å…³é”®è¯æå–é€»è¾‘
            # å¯ä»¥é›†æˆåˆ°QAç”Ÿæˆå™¨ä¸­ä½¿ç”¨æ›´å¤æ‚çš„æ–¹æ³•
            
            # ç§»é™¤å¸¸è§åœç”¨è¯
            stop_words = set(['çš„', 'æ˜¯', 'åœ¨', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œä¸”', 'è¿™', 'é‚£', 'ä¸€ä¸ª', 'æˆ‘ä»¬', 'ä»–ä»¬'])
            
            # ä»é—®é¢˜ä¸­æå–å…³é”®è¯ä½œä¸ºå‚è€ƒ
            question_words = set()
            for word in question.replace('ï¼Ÿ', '').replace('?', '').split():
                if len(word) > 1 and word not in stop_words:
                    question_words.add(word)
            
            # ä»æ–‡æœ¬ä¸­æ‰¾åˆ°ä¸é—®é¢˜ç›¸å…³çš„è¯æ±‡
            keywords = []
            for word in text.split():
                if (len(word) > 1 and 
                    word not in stop_words and 
                    (word in question_words or any(qw in word for qw in question_words))):
                    keywords.append(word)
            
            # å»é‡å¹¶é™åˆ¶æ•°é‡
            keywords = list(set(keywords))[:5]
            
            return keywords
            
        except Exception as e:
            logger.debug(f"æå–å…³é”®è¯å¤±è´¥: {e}")
            return []
    
    def _is_chunk_relevant_to_question(self, question: str, chunk_text: str) -> bool:
        """
        ğŸ†• åˆ¤æ–­chunkæ˜¯å¦ä¸é—®é¢˜ç›¸å…³
        
        Args:
            question: é—®é¢˜æ–‡æœ¬
            chunk_text: chunkæ–‡æœ¬
            
        Returns:
            æ˜¯å¦ç›¸å…³
        """
        try:
            # ç®€åŒ–çš„ç›¸å…³æ€§åˆ¤æ–­
            question_words = set(question.lower().split())
            chunk_words = set(chunk_text.lower().split())
            
            # è®¡ç®—è¯æ±‡é‡å åº¦
            overlap = len(question_words.intersection(chunk_words))
            relevance_score = overlap / max(len(question_words), 1)
            
            # è®¾ç½®é˜ˆå€¼
            return relevance_score > 0.1
            
        except Exception as e:
            logger.debug(f"ç›¸å…³æ€§åˆ¤æ–­å¤±è´¥: {e}")
            return False
    
    def load_single_file_as_document(self, file_path: Path) -> List[Document]:
        """
        ğŸ†• åŠ è½½å•ä¸ªæ–‡ä»¶ä¸ºDocumentå¯¹è±¡
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            Documentåˆ—è¡¨
        """
        logger.info(f"ğŸ“„ åŠ è½½å•ä¸ªæ–‡ä»¶: {file_path}")
        
        if not file_path.exists():
            raise ValueError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        try:
            # ä½¿ç”¨FileProcessoræå–æ–‡æœ¬
            text_content = FileProcessor.extract_text(str(file_path))
            
            if not text_content or len(text_content.strip()) < 50:
                raise ValueError(f"æ–‡ä»¶å†…å®¹å¤ªå°‘æˆ–ä¸ºç©º: {len(text_content)} å­—ç¬¦")
            
            # åˆ›å»ºDocumentå¯¹è±¡
            doc = Document(
                text=text_content,
                metadata={
                    "file_name": file_path.name,
                    "file_path": str(file_path),
                    "file_type": file_path.suffix.lower(),
                    "file_size": file_path.stat().st_size,
                    "text_length": len(text_content),
                    "source_dataset": "single_file_input",
                    "processing_timestamp": datetime.now().isoformat()
                }
            )
            
            logger.info(f"âœ… æˆåŠŸåŠ è½½æ–‡ä»¶: {file_path.name}")
            logger.info(f"   æ–‡ä»¶å¤§å°: {file_path.stat().st_size / 1024:.1f} KB")
            logger.info(f"   æ–‡æœ¬é•¿åº¦: {len(text_content)} å­—ç¬¦")
            logger.info(f"   æ–‡æœ¬é¢„è§ˆ: {text_content[:200]}...")
            
            return [doc]
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            raise e
    
    def test_single_file(self, file_path: Path = None) -> Dict[str, Any]:
        """
        ğŸ†• æµ‹è¯•å•ä¸ªæ–‡ä»¶ - æ”¯æŒPDF/TXT/MD
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨åˆå§‹åŒ–æ—¶çš„input_file
            
        Returns:
            æµ‹è¯•ç»“æœå­—å…¸
        """
        target_file = file_path or self.input_file
        if not target_file:
            raise ValueError("æœªæŒ‡å®šè¾“å…¥æ–‡ä»¶")
        
        file_name = target_file.stem  # æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        logger.info(f"ğŸ§ª å¼€å§‹æµ‹è¯•æ–‡ä»¶: {target_file}")
        logger.info("=" * 60)
        
        start_time = time.time()
        test_result = {
            "input_file": str(target_file),
            "file_name": file_name,
            "status": "unknown",
            "start_time": datetime.now().isoformat(),
            "documents_count": 0,
            "processing_time": 0.0,
            "pipeline_results": {},
            "pipeline_stats": {},
            "qa_results": {},
            "error_message": None
        }
        
        try:
            # 1. åŠ è½½æ–‡ä»¶
            logger.info("ğŸ“„ æ­¥éª¤1: åŠ è½½æ–‡ä»¶...")
            documents = self.load_single_file_as_document(target_file)
            test_result["documents_count"] = len(documents)
            
            if not documents:
                raise ValueError(f"æ–‡ä»¶ {target_file} åŠ è½½å¤±è´¥")
            
            # è®°å½•æ–‡æ¡£ä¿¡æ¯
            test_result["document_details"] = [
                {
                    "index": i,
                    "text_length": len(doc.text),
                    "file_type": doc.metadata.get("file_type", "unknown"),
                    "file_size": doc.metadata.get("file_size", 0),
                    "text_preview": doc.text[:200] + "..." if len(doc.text) > 200 else doc.text
                }
                for i, doc in enumerate(documents)
            ]
            
            logger.info(f"   âœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
            
            # 2. é‡ç½®PipelineçŠ¶æ€
            self.pipeline.reset_pipeline()
            
            # 3. è¿è¡ŒPipeline
            logger.info("ğŸ”¬ æ­¥éª¤2: è¿è¡Œæ¦‚å¿µæå–å’Œè¯æ®æå–Pipeline...")
            pipeline_start = time.time()
            
            pipeline_results = self.pipeline.run_pipeline(documents)
            
            pipeline_time = time.time() - pipeline_start
            logger.info(f"   âœ… Pipelineè¿è¡Œå®Œæˆï¼Œè€—æ—¶: {pipeline_time:.2f}ç§’")
            
            # 4. è·å–ç»Ÿè®¡ä¿¡æ¯
            pipeline_stats = self.pipeline.get_pipeline_statistics()
            
            # 5. ğŸ†• QAç”Ÿæˆæ­¥éª¤
            qa_pairs = []
            training_data = []
            if self.enable_qa_generation:
                logger.info("â“ æ­¥éª¤3: ç”Ÿæˆé—®ç­”å¯¹...")
                
                # æå–Evidenceæ–‡æœ¬
                evidence_texts = self.extract_evidence_texts(pipeline_results)
                
                if evidence_texts:
                    # ç”Ÿæˆé—®ç­”å¯¹
                    qa_pairs = self.generate_qa_pairs(evidence_texts, file_name)
                    
                    # åˆ›å»ºè®­ç»ƒæ•°æ®
                    if qa_pairs:
                        training_data = self.create_training_data(qa_pairs, file_name, pipeline_results)
                else:
                    logger.warning("âš ï¸ æ²¡æœ‰æå–åˆ°Evidenceï¼Œè·³è¿‡QAç”Ÿæˆ")
            
            # 6. è®°å½•ç»“æœ
            test_result.update({
                "status": "success",
                "processing_time": time.time() - start_time,
                "pipeline_processing_time": pipeline_time,
                "pipeline_results": {
                    "chunk_count": len(pipeline_results["chunk_nodes"]),
                    "concept_count": len(pipeline_results["concept_nodes"]),
                    "evidence_count": len(pipeline_results["evidence_nodes"]),
                    "concept_to_chunks_mapping": len(pipeline_results.get("concept_to_chunks", {}))
                },
                "pipeline_stats": pipeline_stats,
                # ğŸ†• QAç»“æœ
                "qa_results": {
                    "qa_pairs_count": len(qa_pairs),
                    "training_data_count": len(training_data),
                    "qa_generation_enabled": self.enable_qa_generation,
                    "questions_config": self.questions_per_type if self.enable_qa_generation else None
                }
            })
            
            # 7. ä¿å­˜è¯¦ç»†ç»“æœ
            self._save_file_results(file_name, pipeline_results, test_result, qa_pairs, training_data)
            
            logger.info(f"âœ… æ–‡ä»¶ {target_file.name} æµ‹è¯•æˆåŠŸ")
            logger.info(f"   ğŸ“Š Pipelineç»“æœ: {test_result['pipeline_results']}")
            if self.enable_qa_generation:
                logger.info(f"   ğŸ“Š QAç»“æœ: {test_result['qa_results']}")
            
            self.test_stats["successful_tests"] += 1
            
        except Exception as e:
            error_msg = f"æµ‹è¯•å¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            
            test_result.update({
                "status": "failed",
                "processing_time": time.time() - start_time,
                "error_message": error_msg,
                "traceback": traceback.format_exc()
            })
            
            self.test_stats["failed_tests"] += 1
        
        test_result["end_time"] = datetime.now().isoformat()
        self.test_stats["total_questions"] += 1
        self.test_stats["total_documents"] += test_result["documents_count"]
        self.test_stats["total_processing_time"] += test_result["processing_time"]
        
        return test_result
    
    def test_single_question(self, question_folder: str) -> Dict[str, Any]:
        """
        æµ‹è¯•å•ä¸ªé—®é¢˜ - ä¿®å¤ç‰ˆ + QAç”Ÿæˆ
        
        Args:
            question_folder: é—®é¢˜æ–‡ä»¶å¤¹åç§°
            
        Returns:
            æµ‹è¯•ç»“æœå­—å…¸
        """
        logger.info(f"ğŸ§ª å¼€å§‹æµ‹è¯•é—®é¢˜: {question_folder}")
        logger.info("=" * 60)
        
        start_time = time.time()
        test_result = {
            "question_folder": question_folder,
            "status": "unknown",
            "start_time": datetime.now().isoformat(),
            "documents_count": 0,
            "processing_time": 0.0,
            "pipeline_results": {},
            "pipeline_stats": {},
            "qa_results": {},  # ğŸ†• QAç”Ÿæˆç»“æœ
            "error_message": None
        }
        
        try:
            # 1. åŠ è½½æ–‡æ¡£
            logger.info("ğŸ“„ æ­¥éª¤1: åŠ è½½æ–‡æ¡£...")
            documents = self.load_question_documents(question_folder)
            test_result["documents_count"] = len(documents)
            
            if not documents:
                raise ValueError(f"é—®é¢˜ {question_folder} æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ–‡æ¡£")
            
            # è®°å½•æ–‡æ¡£ä¿¡æ¯
            test_result["document_details"] = [
                {
                    "index": i,
                    "text_length": len(doc.text),
                    "source_file": doc.metadata.get("file_name", "unknown"),
                    "text_preview": doc.text[:200] + "..." if len(doc.text) > 200 else doc.text
                }
                for i, doc in enumerate(documents)
            ]
            
            logger.info(f"   âœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
            
            # 2. é‡ç½®PipelineçŠ¶æ€
            self.pipeline.reset_pipeline()
            
            # 3. è¿è¡ŒPipeline
            logger.info("ğŸ”¬ æ­¥éª¤2: è¿è¡Œæ¦‚å¿µæå–å’Œè¯æ®æå–Pipeline...")
            pipeline_start = time.time()
            
            pipeline_results = self.pipeline.run_pipeline(documents)
            
            pipeline_time = time.time() - pipeline_start
            logger.info(f"   âœ… Pipelineè¿è¡Œå®Œæˆï¼Œè€—æ—¶: {pipeline_time:.2f}ç§’")
            
            # 4. è·å–ç»Ÿè®¡ä¿¡æ¯
            pipeline_stats = self.pipeline.get_pipeline_statistics()
            
            # 5. ğŸ†• QAç”Ÿæˆæ­¥éª¤
            qa_pairs = []
            training_data = []
            if self.enable_qa_generation:
                logger.info("â“ æ­¥éª¤3: ç”Ÿæˆé—®ç­”å¯¹...")
                
                # æå–Evidenceæ–‡æœ¬
                evidence_texts = self.extract_evidence_texts(pipeline_results)
                
                if evidence_texts:
                    # ç”Ÿæˆé—®ç­”å¯¹
                    qa_pairs = self.generate_qa_pairs(evidence_texts, question_folder)
                    
                    # åˆ›å»ºè®­ç»ƒæ•°æ®
                    if qa_pairs:
                        training_data = self.create_training_data(qa_pairs, question_folder, pipeline_results)
                else:
                    logger.warning("âš ï¸ æ²¡æœ‰æå–åˆ°Evidenceï¼Œè·³è¿‡QAç”Ÿæˆ")
            
            # 6. è®°å½•ç»“æœ
            test_result.update({
                "status": "success",
                "processing_time": time.time() - start_time,
                "pipeline_processing_time": pipeline_time,
                "pipeline_results": {
                    "chunk_count": len(pipeline_results["chunk_nodes"]),
                    "concept_count": len(pipeline_results["concept_nodes"]),
                    "evidence_count": len(pipeline_results["evidence_nodes"]),
                    "concept_to_chunks_mapping": len(pipeline_results.get("concept_to_chunks", {}))
                },
                "pipeline_stats": pipeline_stats,
                # ğŸ†• QAç»“æœ
                "qa_results": {
                    "qa_pairs_count": len(qa_pairs),
                    "training_data_count": len(training_data),
                    "qa_generation_enabled": self.enable_qa_generation,
                    "questions_config": self.questions_per_type if self.enable_qa_generation else None
                }
            })
            
            # 7. ä¿å­˜è¯¦ç»†ç»“æœ
            self._save_question_results(question_folder, pipeline_results, test_result, qa_pairs, training_data)
            
            logger.info(f"âœ… é—®é¢˜ {question_folder} æµ‹è¯•æˆåŠŸ")
            logger.info(f"   ğŸ“Š Pipelineç»“æœ: {test_result['pipeline_results']}")
            if self.enable_qa_generation:
                logger.info(f"   ğŸ“Š QAç»“æœ: {test_result['qa_results']}")
            
            self.test_stats["successful_tests"] += 1
            
        except Exception as e:
            error_msg = f"æµ‹è¯•å¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            
            test_result.update({
                "status": "failed",
                "processing_time": time.time() - start_time,
                "error_message": error_msg,
                "traceback": traceback.format_exc()
            })
            
            self.test_stats["failed_tests"] += 1
        
        test_result["end_time"] = datetime.now().isoformat()
        self.test_stats["total_questions"] += 1
        self.test_stats["total_documents"] += test_result["documents_count"]
        self.test_stats["total_processing_time"] += test_result["processing_time"]
        
        return test_result
    
    def test_multiple_questions(self, question_list: List[str]) -> Dict[str, Any]:
        """
        æµ‹è¯•å¤šä¸ªé—®é¢˜
        
        Args:
            question_list: é—®é¢˜æ–‡ä»¶å¤¹åç§°åˆ—è¡¨
            
        Returns:
            æ‰¹é‡æµ‹è¯•ç»“æœ
        """
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡æµ‹è¯• {len(question_list)} ä¸ªé—®é¢˜")
        logger.info("=" * 80)
        
        batch_start_time = time.time()
        batch_results = {
            "batch_info": {
                "start_time": datetime.now().isoformat(),
                "question_count": len(question_list),
                "questions": question_list,
                "qa_generation_enabled": self.enable_qa_generation,
                "questions_config": self.questions_per_type if self.enable_qa_generation else None
            },
            "individual_results": [],
            "batch_summary": {},
            "pipeline_config": self.pipeline.get_pipeline_statistics()["config_summary"]
        }
        
        # é€ä¸ªæµ‹è¯•é—®é¢˜
        for i, question_folder in enumerate(question_list, 1):
            logger.info(f"\n{'='*20} æµ‹è¯•è¿›åº¦: {i}/{len(question_list)} {'='*20}")
            
            result = self.test_single_question(question_folder)
            batch_results["individual_results"].append(result)
            
            # æ¯3ä¸ªé—®é¢˜ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
            if i % 3 == 0 or i == len(question_list):
                self._save_batch_intermediate_results(batch_results, i)
        
        # è®¡ç®—æ‰¹é‡ç»Ÿè®¡
        batch_time = time.time() - batch_start_time
        self.test_stats["average_processing_time"] = (
            self.test_stats["total_processing_time"] / max(self.test_stats["total_questions"], 1)
        )
        
        batch_results["batch_summary"] = {
            "end_time": datetime.now().isoformat(),
            "total_batch_time": batch_time,
            "statistics": self.test_stats.copy(),
            "success_rate": (
                self.test_stats["successful_tests"] / max(self.test_stats["total_questions"], 1) * 100
            )
        }
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        self._save_batch_final_results(batch_results)
        
        logger.info(f"\nğŸ‰ æ‰¹é‡æµ‹è¯•å®Œæˆ!")
        logger.info(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        logger.info(f"   - æ€»é—®é¢˜æ•°: {self.test_stats['total_questions']}")
        logger.info(f"   - æˆåŠŸ: {self.test_stats['successful_tests']}")
        logger.info(f"   - å¤±è´¥: {self.test_stats['failed_tests']}")
        logger.info(f"   - æˆåŠŸç‡: {batch_results['batch_summary']['success_rate']:.1f}%")
        logger.info(f"   - æ€»å¤„ç†æ—¶é—´: {batch_time:.2f}ç§’")
        logger.info(f"   - å¹³å‡å¤„ç†æ—¶é—´: {self.test_stats['average_processing_time']:.2f}ç§’/é—®é¢˜")
        
        # ğŸ†• QAç”Ÿæˆç»Ÿè®¡
        if self.enable_qa_generation:
            logger.info(f"   - ç”Ÿæˆé—®ç­”å¯¹: {self.test_stats['total_qa_pairs']} ä¸ª")
            logger.info(f"   - QAç”Ÿæˆæ—¶é—´: {self.test_stats['qa_generation_time']:.2f}ç§’")
            logger.info(f"   - QAç”Ÿæˆé”™è¯¯: {self.test_stats['qa_generation_errors']} æ¬¡")
        
        return batch_results
    
    def run_quick_test(self) -> Dict[str, Any]:
        """
        å¿«é€Ÿæµ‹è¯• - é€‰æ‹©ä¸€ä¸ªä»£è¡¨æ€§é—®é¢˜
        """
        representative_questions = [
            "question_150",  # ä¹¦ç±ç›¸å…³ (æ–‡å­¦å†å²) - æ–‡æ¡£è¾ƒå°ï¼Œæµ‹è¯•å‹å¥½
        ]
        
        # æ£€æŸ¥é—®é¢˜æ˜¯å¦å­˜åœ¨
        available_questions = self.get_available_questions()
        test_questions = [q for q in representative_questions if q in available_questions]
        
        if not test_questions:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»£è¡¨æ€§é—®é¢˜ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨é—®é¢˜")
            test_questions = available_questions[:1]  # ä¿®æ”¹ä¸ºåªå–ç¬¬ä¸€ä¸ª
        
        logger.info(f"ğŸ¯ å¿«é€Ÿæµ‹è¯•é€‰æ‹©çš„é—®é¢˜: {test_questions}")
        return self.test_multiple_questions(test_questions)
    
    def run_full_test(self) -> Dict[str, Any]:
        """
        å®Œæ•´æµ‹è¯• - æµ‹è¯•æ‰€æœ‰é—®é¢˜
        """
        all_questions = self.get_available_questions()
        logger.info(f"ğŸ”¥ å®Œæ•´æµ‹è¯•ï¼Œå°†æµ‹è¯•æ‰€æœ‰ {len(all_questions)} ä¸ªé—®é¢˜")
        return self.test_multiple_questions(all_questions)
    
    def _save_question_results(self, question_folder: str, pipeline_results: Dict, test_result: Dict, 
                              qa_pairs: List[Dict[str, Any]], training_data: List[Dict[str, Any]]):
        """ä¿å­˜å•ä¸ªé—®é¢˜çš„è¯¦ç»†ç»“æœ - åŒ…å«QAæ•°æ®"""
        question_output_dir = self.output_dir / question_folder
        question_output_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜pipelineç»“æœ
        pipeline_file = question_output_dir / "pipeline_results.json"
        self.pipeline.save_results(pipeline_results, str(pipeline_file))
        
        # ä¿å­˜æµ‹è¯•å…ƒä¿¡æ¯
        test_info_file = question_output_dir / "test_info.json"
        with open(test_info_file, 'w', encoding='utf-8') as f:
            json.dump(test_result, f, ensure_ascii=False, indent=2)
        
        # ğŸ†• ä¿å­˜é—®ç­”å¯¹
        if qa_pairs:
            qa_file = question_output_dir / "qa_pairs.json"
            with open(qa_file, 'w', encoding='utf-8') as f:
                json.dump(qa_pairs, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ é—®ç­”å¯¹å·²ä¿å­˜: {qa_file}")
        
        # ğŸ†• ä¿å­˜è®­ç»ƒæ•°æ®
        if training_data:
            training_file = question_output_dir / "training_data.jsonl"
            with open(training_file, 'w', encoding='utf-8') as f:
                for item in training_data:
                    f.write(json.dumps(item, ensure_ascii=False) + "\n")
            logger.info(f"ğŸ’¾ è®­ç»ƒæ•°æ®å·²ä¿å­˜: {training_file}")
    
    def _save_batch_intermediate_results(self, batch_results: Dict, current_index: int):
        """ä¿å­˜æ‰¹é‡æµ‹è¯•çš„ä¸­é—´ç»“æœ"""
        intermediate_file = self.output_dir / f"batch_intermediate_{current_index}.json"
        with open(intermediate_file, 'w', encoding='utf-8') as f:
            json.dump(batch_results, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ å·²ä¿å­˜ä¸­é—´ç»“æœ: {intermediate_file}")
    
    def _save_batch_final_results(self, batch_results: Dict):
        """ä¿å­˜æ‰¹é‡æµ‹è¯•çš„æœ€ç»ˆç»“æœ - åŒ…å«QAç»Ÿè®¡"""
        final_file = self.output_dir / "batch_final_results.json"
        with open(final_file, 'w', encoding='utf-8') as f:
            json.dump(batch_results, f, ensure_ascii=False, indent=2)
        
        # ğŸ†• åˆå¹¶æ‰€æœ‰è®­ç»ƒæ•°æ®
        if self.enable_qa_generation:
            self._combine_all_training_data()
        
        # ç”Ÿæˆç®€åŒ–çš„ç»Ÿè®¡æŠ¥å‘Š
        summary_file = self.output_dir / "test_summary.txt"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("SimpleQA Historyæ•°æ®é›† Pipelineæµ‹è¯•æŠ¥å‘Š (ä¿®å¤ç‰ˆ + QAç”Ÿæˆ)\n")
            f.write("=" * 65 + "\n\n")
            f.write(f"æµ‹è¯•æ—¶é—´: {batch_results['batch_info']['start_time']}\n")
            f.write(f"æµ‹è¯•é—®é¢˜æ•°: {batch_results['batch_info']['question_count']}\n")
            f.write(f"æˆåŠŸ: {self.test_stats['successful_tests']}\n")
            f.write(f"å¤±è´¥: {self.test_stats['failed_tests']}\n")
            f.write(f"æˆåŠŸç‡: {batch_results['batch_summary']['success_rate']:.1f}%\n")
            f.write(f"æ€»å¤„ç†æ—¶é—´: {batch_results['batch_summary']['total_batch_time']:.2f}ç§’\n")
            f.write(f"å¹³å‡å¤„ç†æ—¶é—´: {self.test_stats['average_processing_time']:.2f}ç§’/é—®é¢˜\n")
            f.write(f"æ€»æ–‡æ¡£æ•°: {self.test_stats['total_documents']}\n")
            
            # ğŸ†• QAç”Ÿæˆç»Ÿè®¡
            if self.enable_qa_generation:
                f.write(f"\nQAç”Ÿæˆç»Ÿè®¡:\n")
                f.write(f"å¯ç”¨çŠ¶æ€: æ˜¯\n")
                f.write(f"ç”Ÿæˆé—®ç­”å¯¹æ€»æ•°: {self.test_stats['total_qa_pairs']}\n")
                f.write(f"QAç”Ÿæˆæ€»æ—¶é—´: {self.test_stats['qa_generation_time']:.2f}ç§’\n")
                f.write(f"QAç”Ÿæˆé”™è¯¯æ•°: {self.test_stats['qa_generation_errors']}\n")
                f.write(f"é—®é¢˜ç±»å‹é…ç½®: {self.questions_per_type}\n")
            else:
                f.write(f"\nQAç”Ÿæˆç»Ÿè®¡: æœªå¯ç”¨\n")
            
            f.write("\nè¯¦ç»†ç»“æœ:\n")
            f.write("-" * 50 + "\n")
            for result in batch_results["individual_results"]:
                status_emoji = "âœ…" if result["status"] == "success" else "âŒ"
                f.write(f"{status_emoji} {result['question_folder']}: "
                       f"{result['documents_count']}æ–‡æ¡£, "
                       f"{result['processing_time']:.2f}ç§’")
                
                if result["status"] == "success":
                    pipeline_res = result["pipeline_results"]
                    qa_res = result.get("qa_results", {})
                    f.write(f"\n   ğŸ“Š Pipeline: Chunks:{pipeline_res['chunk_count']}, "
                           f"Concepts:{pipeline_res['concept_count']}, "
                           f"Evidence:{pipeline_res['evidence_count']}")
                    if qa_res.get("qa_generation_enabled"):
                        f.write(f"\n   ğŸ“Š QAç”Ÿæˆ: {qa_res['qa_pairs_count']} ä¸ªé—®ç­”å¯¹")
                    f.write(f"\n")
                elif result.get("error_message"):
                    f.write(f"\n   âŒ é”™è¯¯: {result['error_message']}\n")
        
        logger.info(f"ğŸ’¾ å·²ä¿å­˜æœ€ç»ˆç»“æœ: {final_file}")
        logger.info(f"ğŸ“‹ å·²ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š: {summary_file}")
    
    def _combine_all_training_data(self):
        """ğŸ†• åˆå¹¶æ‰€æœ‰é—®é¢˜çš„è®­ç»ƒæ•°æ®"""
        combined_file = self.output_dir / "combined_training_data.jsonl"
        total_count = 0
        
        with open(combined_file, 'w', encoding='utf-8') as outfile:
            for question_dir in self.output_dir.iterdir():
                if question_dir.is_dir() and question_dir.name.startswith("question_"):
                    training_file = question_dir / "training_data.jsonl"
                    if training_file.exists():
                        with open(training_file, 'r', encoding='utf-8') as infile:
                            for line in infile:
                                outfile.write(line)
                                total_count += 1
        
        logger.info(f"ğŸ’¾ åˆå¹¶è®­ç»ƒæ•°æ®å®Œæˆ: {combined_file} ({total_count} æ¡è®°å½•)")
    
    def _save_file_results(self, file_name: str, pipeline_results: Dict, test_result: Dict, 
                          qa_pairs: List[Dict[str, Any]], training_data: List[Dict[str, Any]]):
        """ğŸ†• ä¿å­˜å•ä¸ªæ–‡ä»¶çš„è¯¦ç»†ç»“æœ"""
        file_output_dir = self.output_dir / f"file_{file_name}"
        file_output_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜pipelineç»“æœ
        pipeline_file = file_output_dir / "pipeline_results.json"
        self.pipeline.save_results(pipeline_results, str(pipeline_file))
        
        # ä¿å­˜æµ‹è¯•å…ƒä¿¡æ¯
        test_info_file = file_output_dir / "test_info.json"
        with open(test_info_file, 'w', encoding='utf-8') as f:
            json.dump(test_result, f, ensure_ascii=False, indent=2)
        
        # ğŸ†• ä¿å­˜é—®ç­”å¯¹
        if qa_pairs:
            qa_file = file_output_dir / "qa_pairs.json"
            with open(qa_file, 'w', encoding='utf-8') as f:
                json.dump(qa_pairs, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ é—®ç­”å¯¹å·²ä¿å­˜: {qa_file}")
        
        # ğŸ†• ä¿å­˜è®­ç»ƒæ•°æ®
        if training_data:
            training_file = file_output_dir / "training_data.jsonl"
            with open(training_file, 'w', encoding='utf-8') as f:
                for item in training_data:
                    f.write(json.dumps(item, ensure_ascii=False) + "\n")
            logger.info(f"ğŸ’¾ è®­ç»ƒæ•°æ®å·²ä¿å­˜: {training_file}")
            
            # ğŸ†• ä¹Ÿä¿å­˜ä¸ºå®Œæ•´çš„è®­ç»ƒæ•°æ®é›†
            complete_training_file = self.output_dir / f"{file_name}_training_data.jsonl"
            with open(complete_training_file, 'w', encoding='utf-8') as f:
                for item in training_data:
                    f.write(json.dumps(item, ensure_ascii=False) + "\n")
            logger.info(f"ğŸ’¾ å®Œæ•´è®­ç»ƒæ•°æ®å·²ä¿å­˜: {complete_training_file}")

def main():
    """ä¸»å‡½æ•° - æä¾›äº¤äº’å¼æµ‹è¯•é€‰æ‹©"""
    print("ğŸ§ª SimpleQA Historyæ•°æ®é›† Pipelineæµ‹è¯•å·¥å…· (ä¿®å¤ç‰ˆ + QAç”Ÿæˆ + PDFæ”¯æŒ)")
    print("=" * 70)
    
    # ğŸ†• è¯¢é—®è¾“å…¥æ¨¡å¼
    print("é€‰æ‹©è¾“å…¥æ¨¡å¼:")
    print("1. å•ä¸ªæ–‡ä»¶å¤„ç† (PDF/TXT/MD)")
    print("2. æ‰¹é‡æ•°æ®é›†å¤„ç† (simpleqa_histroy_md)")
    
    input_mode = input("\nè¯·é€‰æ‹© (1-2): ").strip()
    
    input_file = None
    if input_mode == "1":
        # å•æ–‡ä»¶æ¨¡å¼
        file_path = input("è¯·è¾“å…¥æ–‡ä»¶è·¯å¾„ (ä¾‹: attention is all you need.pdf): ").strip()
        if not file_path:
            print("âŒ æœªæä¾›æ–‡ä»¶è·¯å¾„")
            return
        
        input_file = Path(file_path)
        if not input_file.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
            return
        
        print(f"ğŸ“„ å°†å¤„ç†æ–‡ä»¶: {input_file}")
    
    # è¯¢é—®æ˜¯å¦å¯ç”¨QAç”Ÿæˆ
    enable_qa = input("æ˜¯å¦å¯ç”¨é—®ç­”ç”ŸæˆåŠŸèƒ½? (y/N): ").strip().lower() == 'y'
    
    qa_config = {}
    if enable_qa:
        print("\né…ç½®é—®ç­”ç”Ÿæˆå‚æ•°:")
        print("1. ä½¿ç”¨é»˜è®¤é…ç½® (æ¯ç§ç±»å‹1-2ä¸ªé—®é¢˜)")
        print("2. è‡ªå®šä¹‰é…ç½®")
        
        config_choice = input("è¯·é€‰æ‹© (1-2): ").strip()
        
        if config_choice == "2":
            print("\nè¯·è¾“å…¥æ¯ç§ç±»å‹çš„é—®é¢˜æ•°é‡:")
            qa_config = {
                "remember": int(input("Rememberç±»å‹ (é»˜è®¤2): ") or "2"),
                "understand": int(input("Understandç±»å‹ (é»˜è®¤2): ") or "2"),
                "apply": int(input("Applyç±»å‹ (é»˜è®¤1): ") or "1"),
                "analyze": int(input("Analyzeç±»å‹ (é»˜è®¤1): ") or "1"),
                "evaluate": int(input("Evaluateç±»å‹ (é»˜è®¤1): ") or "1"),
                "create": int(input("Createç±»å‹ (é»˜è®¤1): ") or "1")
            }
        
        qa_model = input("QAç”Ÿæˆæ¨¡å‹ (é»˜è®¤gpt-4o-mini): ").strip() or "gpt-4o-mini"
    else:
        qa_model = "gpt-4o-mini"
    
    # åˆå§‹åŒ–æµ‹è¯•å¥—ä»¶
    test_suite = SimpleQATestSuiteWithQAGeneration(
        enable_qa_generation=enable_qa,
        qa_model_name=qa_model,
        questions_per_type=qa_config if qa_config else None,
        input_file=str(input_file) if input_file else None  # ğŸ†•
    )
    
    try:
        if input_mode == "1":
            # ğŸ†• å•æ–‡ä»¶å¤„ç†
            print(f"\nğŸ“„ å¼€å§‹å¤„ç†æ–‡ä»¶: {input_file.name}")
            result = test_suite.test_single_file()
            
            print(f"\nç»“æœ: {result['status']}")
            if result['status'] == 'success':
                print(f"   å¤„ç†æ—¶é—´: {result['processing_time']:.2f}ç§’")
                print(f"   Pipelineç»“æœ: {result['pipeline_results']}")
                if enable_qa:
                    print(f"   QAç»“æœ: {result['qa_results']}")
                print(f"\nğŸ‰ å¤„ç†å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: {test_suite.output_dir}")
                if enable_qa:
                    print("ğŸ“‹ è®­ç»ƒæ•°æ®æ–‡ä»¶:")
                    print(f"   - ä¸»æ–‡ä»¶: {test_suite.output_dir}/{input_file.stem}_training_data.jsonl")
                    print(f"   - è¯¦ç»†ç›®å½•: {test_suite.output_dir}/file_{input_file.stem}/")
            else:
                print(f"   é”™è¯¯: {result.get('error_message', 'æœªçŸ¥é”™è¯¯')}")
                
        else:
            # åŸæœ‰çš„æ‰¹é‡å¤„ç†é€»è¾‘
            # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
            available_questions = test_suite.get_available_questions()
            if not available_questions:
                print("âŒ é”™è¯¯: æœªæ‰¾åˆ°simpleqa_histroy_mdæ•°æ®é›†")
                return
            
            print(f"ğŸ“Š å‘ç° {len(available_questions)} ä¸ªé—®é¢˜å¯ä¾›æµ‹è¯•")
            print(f"é—®é¢˜èŒƒå›´: {available_questions[0]} åˆ° {available_questions[-1]}")
            
            # åŸæœ‰çš„æµ‹è¯•é€‰é¡¹é€»è¾‘...
            print("\né€‰æ‹©æµ‹è¯•æ¨¡å¼:")
            print("1. å¿«é€Ÿæµ‹è¯• (1ä¸ªä»£è¡¨æ€§é—®é¢˜)")
            print("2. è‡ªå®šä¹‰æµ‹è¯• (æŒ‡å®šé—®é¢˜)")
            print("3. å®Œæ•´æµ‹è¯• (æ‰€æœ‰é—®é¢˜)")
            print("4. å•ä¸ªé—®é¢˜æµ‹è¯•")
            
            choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-4): ").strip()
            
            if choice == "1":
                print("\nğŸ¯ å¼€å§‹å¿«é€Ÿæµ‹è¯•...")
                results = test_suite.run_quick_test()
            # ... å…¶ä»–é€‰é¡¹çš„å¤„ç†é€»è¾‘ä¿æŒä¸å˜
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸ å¤„ç†è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        logger.error(f"ä¸»ç¨‹åºé”™è¯¯: {traceback.format_exc()}")

if __name__ == "__main__":
    main() 