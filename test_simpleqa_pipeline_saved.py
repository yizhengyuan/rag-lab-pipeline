"""
SimpleQA Historyæ•°æ®é›† Pipelineæµ‹è¯•è„šæœ¬ - ä¿®å¤ç‰ˆ
==============================================

ä¿®å¤å†…å®¹ï¼š
- ä¿®å¤æ–‡æ¡£åŠ è½½é—®é¢˜
- æ”¹ä¸ºæ‰‹åŠ¨è¯»å–markdownæ–‡ä»¶
- æ·»åŠ æ›´è¯¦ç»†çš„é”™è¯¯å¤„ç†
"""

import os
import json
import time
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
from pathlib import Path
import traceback

# å¯¼å…¥pipelineç›¸å…³æ¨¡å—
from pipeline_new import ImprovedConceptBasedPipeline
from llama_index.core import Document

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('simpleqa_test_fixed.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class SimpleQATestSuiteFixed:
    """SimpleQAæ•°æ®é›†æµ‹è¯•å¥—ä»¶ - ä¿®å¤ç‰ˆ"""
    
    def __init__(self, 
                 data_dir: str = "simpleqa_histroy_md",
                 output_dir: str = "simpleqa_test_results_fixed",
                 config_path: str = None):
        """
        åˆå§‹åŒ–æµ‹è¯•å¥—ä»¶
        
        Args:
            data_dir: SimpleQAæ•°æ®é›†ç›®å½•
            output_dir: æµ‹è¯•ç»“æœè¾“å‡ºç›®å½•
            config_path: Pipelineé…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.config_path = config_path
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–pipeline
        if config_path:
            self.pipeline = ImprovedConceptBasedPipeline(config_path=config_path)
        else:
            # ä½¿ç”¨é»˜è®¤é…ç½®
            self.pipeline = ImprovedConceptBasedPipeline()
        
        # æµ‹è¯•ç»Ÿè®¡
        self.test_stats = {
            "total_questions": 0,
            "successful_tests": 0,
            "failed_tests": 0,
            "total_documents": 0,
            "total_processing_time": 0.0,
            "average_processing_time": 0.0
        }
        
        logger.info(f"åˆå§‹åŒ–SimpleQAæµ‹è¯•å¥—ä»¶ (ä¿®å¤ç‰ˆ)")
        logger.info(f"æ•°æ®ç›®å½•: {self.data_dir}")
        logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def get_available_questions(self) -> List[str]:
        """è·å–æ‰€æœ‰å¯ç”¨çš„é—®é¢˜æ–‡ä»¶å¤¹"""
        questions = []
        if self.data_dir.exists():
            for item in self.data_dir.iterdir():
                if item.is_dir() and item.name.startswith("question_"):
                    questions.append(item.name)
        return sorted(questions)
    
    def load_question_documents(self, question_folder: str) -> List[Document]:
        """
        åŠ è½½æŒ‡å®šé—®é¢˜æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡æ¡£ - ä¿®å¤ç‰ˆ
        
        Args:
            question_folder: é—®é¢˜æ–‡ä»¶å¤¹åç§° (å¦‚ 'question_100')
            
        Returns:
            Documentåˆ—è¡¨
        """
        question_path = self.data_dir / question_folder
        if not question_path.exists():
            raise ValueError(f"é—®é¢˜æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {question_path}")
        
        documents = []
        
        # æ‰‹åŠ¨éå†å¹¶åŠ è½½markdownæ–‡ä»¶
        md_files = list(question_path.glob("*.md"))
        logger.info(f"åœ¨ {question_folder} ä¸­æ‰¾åˆ° {len(md_files)} ä¸ªmarkdownæ–‡ä»¶")
        
        for md_file in md_files:
            try:
                # ç›´æ¥è¯»å–æ–‡ä»¶å†…å®¹
                with open(md_file, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read().strip()
                
                # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©º
                if not content:
                    logger.warning(f"æ–‡ä»¶ {md_file.name} å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
                    continue
                
                # åˆ›å»ºDocumentå¯¹è±¡
                doc = Document(
                    text=content,
                    metadata={
                        "file_name": md_file.name,
                        "file_path": str(md_file),
                        "question_folder": question_folder,
                        "source_dataset": "simpleqa_history",
                        "file_size": len(content)
                    }
                )
                documents.append(doc)
                logger.debug(f"âœ… æˆåŠŸåŠ è½½æ–‡ä»¶: {md_file.name} ({len(content)} å­—ç¬¦)")
                
            except Exception as e:
                logger.warning(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {md_file.name}: {e}")
                continue
        
        if not documents:
            logger.warning(f"âš ï¸ é—®é¢˜ {question_folder} æœªåŠ è½½åˆ°ä»»ä½•æœ‰æ•ˆæ–‡æ¡£")
        else:
            total_chars = sum(len(doc.text) for doc in documents)
            logger.info(f"âœ… é—®é¢˜ {question_folder}: æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£ï¼Œæ€»è®¡ {total_chars} å­—ç¬¦")
        
        return documents
    
    def test_single_question(self, question_folder: str) -> Dict[str, Any]:
        """
        æµ‹è¯•å•ä¸ªé—®é¢˜
        
        Args:
            question_folder: é—®é¢˜æ–‡ä»¶å¤¹åç§°
            
        Returns:
            æµ‹è¯•ç»“æœå­—å…¸
        """
        logger.info(f"ğŸ§ª å¼€å§‹æµ‹è¯•é—®é¢˜: {question_folder}")
        logger.info("=" * 60)
        
        start_time = time.time()
        test_result = {
            "question_folder": question_folder,
            "status": "unknown",
            "start_time": datetime.now().isoformat(),
            "documents_count": 0,
            "processing_time": 0.0,
            "pipeline_results": {},
            "pipeline_stats": {},
            "error_message": None
        }
        
        try:
            # 1. åŠ è½½æ–‡æ¡£
            logger.info("ğŸ“„ æ­¥éª¤1: åŠ è½½æ–‡æ¡£...")
            documents = self.load_question_documents(question_folder)
            test_result["documents_count"] = len(documents)
            
            if not documents:
                raise ValueError(f"é—®é¢˜ {question_folder} æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ–‡æ¡£")
            
            # è®°å½•æ–‡æ¡£ä¿¡æ¯
            test_result["document_details"] = [
                {
                    "index": i,
                    "text_length": len(doc.text),
                    "source_file": doc.metadata.get("file_name", "unknown"),
                    "text_preview": doc.text[:200] + "..." if len(doc.text) > 200 else doc.text
                }
                for i, doc in enumerate(documents)
            ]
            
            logger.info(f"   âœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
            
            # 2. é‡ç½®PipelineçŠ¶æ€
            self.pipeline.reset_pipeline()
            
            # 3. è¿è¡ŒPipeline
            logger.info("ğŸ”¬ æ­¥éª¤2: è¿è¡ŒPipeline...")
            pipeline_start = time.time()
            
            pipeline_results = self.pipeline.run_pipeline(documents)
            
            pipeline_time = time.time() - pipeline_start
            logger.info(f"   âœ… Pipelineè¿è¡Œå®Œæˆï¼Œè€—æ—¶: {pipeline_time:.2f}ç§’")
            
            # 4. è·å–ç»Ÿè®¡ä¿¡æ¯
            pipeline_stats = self.pipeline.get_pipeline_statistics()
            
            # 5. è®°å½•ç»“æœ
            test_result.update({
                "status": "success",
                "processing_time": time.time() - start_time,
                "pipeline_processing_time": pipeline_time,
                "pipeline_results": {
                    "chunk_count": len(pipeline_results["chunk_nodes"]),
                    "concept_count": len(pipeline_results["concept_nodes"]),
                    "evidence_count": len(pipeline_results["evidence_nodes"]),
                    "concept_to_chunks_mapping": len(pipeline_results.get("concept_to_chunks", {}))
                },
                "pipeline_stats": pipeline_stats
            })
            
            # 6. ä¿å­˜è¯¦ç»†ç»“æœ
            self._save_question_results(question_folder, pipeline_results, test_result)
            
            logger.info(f"âœ… é—®é¢˜ {question_folder} æµ‹è¯•æˆåŠŸ")
            logger.info(f"   ğŸ“Š ç»“æœ: {test_result['pipeline_results']}")
            
            self.test_stats["successful_tests"] += 1
            
        except Exception as e:
            error_msg = f"æµ‹è¯•å¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            
            test_result.update({
                "status": "failed",
                "processing_time": time.time() - start_time,
                "error_message": error_msg,
                "traceback": traceback.format_exc()
            })
            
            self.test_stats["failed_tests"] += 1
        
        test_result["end_time"] = datetime.now().isoformat()
        self.test_stats["total_questions"] += 1
        self.test_stats["total_documents"] += test_result["documents_count"]
        self.test_stats["total_processing_time"] += test_result["processing_time"]
        
        return test_result
    
    def test_multiple_questions(self, question_list: List[str]) -> Dict[str, Any]:
        """
        æµ‹è¯•å¤šä¸ªé—®é¢˜
        
        Args:
            question_list: é—®é¢˜æ–‡ä»¶å¤¹åç§°åˆ—è¡¨
            
        Returns:
            æ‰¹é‡æµ‹è¯•ç»“æœ
        """
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡æµ‹è¯• {len(question_list)} ä¸ªé—®é¢˜")
        logger.info("=" * 80)
        
        batch_start_time = time.time()
        batch_results = {
            "batch_info": {
                "start_time": datetime.now().isoformat(),
                "question_count": len(question_list),
                "questions": question_list
            },
            "individual_results": [],
            "batch_summary": {},
            "pipeline_config": self.pipeline.get_pipeline_statistics()["config_summary"]
        }
        
        # é€ä¸ªæµ‹è¯•é—®é¢˜
        for i, question_folder in enumerate(question_list, 1):
            logger.info(f"\n{'='*20} æµ‹è¯•è¿›åº¦: {i}/{len(question_list)} {'='*20}")
            
            result = self.test_single_question(question_folder)
            batch_results["individual_results"].append(result)
            
            # æ¯3ä¸ªé—®é¢˜ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
            if i % 3 == 0 or i == len(question_list):
                self._save_batch_intermediate_results(batch_results, i)
        
        # è®¡ç®—æ‰¹é‡ç»Ÿè®¡
        batch_time = time.time() - batch_start_time
        self.test_stats["average_processing_time"] = (
            self.test_stats["total_processing_time"] / max(self.test_stats["total_questions"], 1)
        )
        
        batch_results["batch_summary"] = {
            "end_time": datetime.now().isoformat(),
            "total_batch_time": batch_time,
            "statistics": self.test_stats.copy(),
            "success_rate": (
                self.test_stats["successful_tests"] / max(self.test_stats["total_questions"], 1) * 100
            )
        }
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        self._save_batch_final_results(batch_results)
        
        logger.info(f"\nğŸ‰ æ‰¹é‡æµ‹è¯•å®Œæˆ!")
        logger.info(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        logger.info(f"   - æ€»é—®é¢˜æ•°: {self.test_stats['total_questions']}")
        logger.info(f"   - æˆåŠŸ: {self.test_stats['successful_tests']}")
        logger.info(f"   - å¤±è´¥: {self.test_stats['failed_tests']}")
        logger.info(f"   - æˆåŠŸç‡: {batch_results['batch_summary']['success_rate']:.1f}%")
        logger.info(f"   - æ€»å¤„ç†æ—¶é—´: {batch_time:.2f}ç§’")
        logger.info(f"   - å¹³å‡å¤„ç†æ—¶é—´: {self.test_stats['average_processing_time']:.2f}ç§’/é—®é¢˜")
        
        return batch_results
    
    def run_quick_test(self) -> Dict[str, Any]:
        """
        å¿«é€Ÿæµ‹è¯• - é€‰æ‹©ä¸€ä¸ªä»£è¡¨æ€§é—®é¢˜
        """
        representative_questions = [
            "question_150",  # ä¹¦ç±ç›¸å…³ (æ–‡å­¦å†å²) - æ–‡æ¡£è¾ƒå°ï¼Œæµ‹è¯•å‹å¥½
        ]
        
        # æ£€æŸ¥é—®é¢˜æ˜¯å¦å­˜åœ¨
        available_questions = self.get_available_questions()
        test_questions = [q for q in representative_questions if q in available_questions]
        
        if not test_questions:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»£è¡¨æ€§é—®é¢˜ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨é—®é¢˜")
            test_questions = available_questions[:1]  # ä¿®æ”¹ä¸ºåªå–ç¬¬ä¸€ä¸ª
        
        logger.info(f"ğŸ¯ å¿«é€Ÿæµ‹è¯•é€‰æ‹©çš„é—®é¢˜: {test_questions}")
        return self.test_multiple_questions(test_questions)
    
    def run_full_test(self) -> Dict[str, Any]:
        """
        å®Œæ•´æµ‹è¯• - æµ‹è¯•æ‰€æœ‰é—®é¢˜
        """
        all_questions = self.get_available_questions()
        logger.info(f"ğŸ”¥ å®Œæ•´æµ‹è¯•ï¼Œå°†æµ‹è¯•æ‰€æœ‰ {len(all_questions)} ä¸ªé—®é¢˜")
        return self.test_multiple_questions(all_questions)
    
    def _save_question_results(self, question_folder: str, pipeline_results: Dict, test_result: Dict):
        """ä¿å­˜å•ä¸ªé—®é¢˜çš„è¯¦ç»†ç»“æœ"""
        question_output_dir = self.output_dir / question_folder
        question_output_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜pipelineç»“æœ
        pipeline_file = question_output_dir / "pipeline_results.json"
        self.pipeline.save_results(pipeline_results, str(pipeline_file))
        
        # ä¿å­˜æµ‹è¯•å…ƒä¿¡æ¯
        test_info_file = question_output_dir / "test_info.json"
        with open(test_info_file, 'w', encoding='utf-8') as f:
            json.dump(test_result, f, ensure_ascii=False, indent=2)
    
    def _save_batch_intermediate_results(self, batch_results: Dict, current_index: int):
        """ä¿å­˜æ‰¹é‡æµ‹è¯•çš„ä¸­é—´ç»“æœ"""
        intermediate_file = self.output_dir / f"batch_intermediate_{current_index}.json"
        with open(intermediate_file, 'w', encoding='utf-8') as f:
            json.dump(batch_results, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ å·²ä¿å­˜ä¸­é—´ç»“æœ: {intermediate_file}")
    
    def _save_batch_final_results(self, batch_results: Dict):
        """ä¿å­˜æ‰¹é‡æµ‹è¯•çš„æœ€ç»ˆç»“æœ"""
        final_file = self.output_dir / "batch_final_results.json"
        with open(final_file, 'w', encoding='utf-8') as f:
            json.dump(batch_results, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆç®€åŒ–çš„ç»Ÿè®¡æŠ¥å‘Š
        summary_file = self.output_dir / "test_summary.txt"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("SimpleQA Historyæ•°æ®é›† Pipelineæµ‹è¯•æŠ¥å‘Š (ä¿®å¤ç‰ˆ)\n")
            f.write("=" * 55 + "\n\n")
            f.write(f"æµ‹è¯•æ—¶é—´: {batch_results['batch_info']['start_time']}\n")
            f.write(f"æµ‹è¯•é—®é¢˜æ•°: {batch_results['batch_info']['question_count']}\n")
            f.write(f"æˆåŠŸ: {self.test_stats['successful_tests']}\n")
            f.write(f"å¤±è´¥: {self.test_stats['failed_tests']}\n")
            f.write(f"æˆåŠŸç‡: {batch_results['batch_summary']['success_rate']:.1f}%\n")
            f.write(f"æ€»å¤„ç†æ—¶é—´: {batch_results['batch_summary']['total_batch_time']:.2f}ç§’\n")
            f.write(f"å¹³å‡å¤„ç†æ—¶é—´: {self.test_stats['average_processing_time']:.2f}ç§’/é—®é¢˜\n")
            f.write(f"æ€»æ–‡æ¡£æ•°: {self.test_stats['total_documents']}\n")
            
            f.write("\nè¯¦ç»†ç»“æœ:\n")
            f.write("-" * 40 + "\n")
            for result in batch_results["individual_results"]:
                status_emoji = "âœ…" if result["status"] == "success" else "âŒ"
                f.write(f"{status_emoji} {result['question_folder']}: "
                       f"{result['documents_count']}æ–‡æ¡£, "
                       f"{result['processing_time']:.2f}ç§’\n")
                if result["status"] == "success":
                    pipeline_res = result["pipeline_results"]
                    f.write(f"   ğŸ“Š Chunks: {pipeline_res['chunk_count']}, "
                           f"Concepts: {pipeline_res['concept_count']}, "
                           f"Evidence: {pipeline_res['evidence_count']}\n")
                elif result.get("error_message"):
                    f.write(f"   âŒ é”™è¯¯: {result['error_message']}\n")
        
        logger.info(f"ğŸ’¾ å·²ä¿å­˜æœ€ç»ˆç»“æœ: {final_file}")
        logger.info(f"ğŸ“‹ å·²ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š: {summary_file}")

def main():
    """ä¸»å‡½æ•° - æä¾›äº¤äº’å¼æµ‹è¯•é€‰æ‹©"""
    print("ğŸ§ª SimpleQA Historyæ•°æ®é›† Pipelineæµ‹è¯•å·¥å…· (ä¿®å¤ç‰ˆ)")
    print("=" * 55)
    
    # åˆå§‹åŒ–æµ‹è¯•å¥—ä»¶
    test_suite = SimpleQATestSuiteFixed()
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
    available_questions = test_suite.get_available_questions()
    if not available_questions:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°simpleqa_histroy_mdæ•°æ®é›†")
        return
    
    print(f"ğŸ“Š å‘ç° {len(available_questions)} ä¸ªé—®é¢˜å¯ä¾›æµ‹è¯•")
    print(f"é—®é¢˜èŒƒå›´: {available_questions[0]} åˆ° {available_questions[-1]}")
    
    # æµ‹è¯•é€‰é¡¹
    print("\né€‰æ‹©æµ‹è¯•æ¨¡å¼:")
    print("1. å¿«é€Ÿæµ‹è¯• (1ä¸ªä»£è¡¨æ€§é—®é¢˜)")
    print("2. è‡ªå®šä¹‰æµ‹è¯• (æŒ‡å®šé—®é¢˜)")
    print("3. å®Œæ•´æµ‹è¯• (æ‰€æœ‰é—®é¢˜)")
    print("4. å•ä¸ªé—®é¢˜æµ‹è¯•")
    
    choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-4): ").strip()
    
    try:
        if choice == "1":
            print("\nğŸ¯ å¼€å§‹å¿«é€Ÿæµ‹è¯•...")
            results = test_suite.run_quick_test()
            
        elif choice == "2":
            print(f"\nå¯ç”¨é—®é¢˜: {available_questions[:10]}... (æ˜¾ç¤ºå‰10ä¸ª)")
            questions_input = input("è¯·è¾“å…¥é—®é¢˜æ–‡ä»¶å¤¹åç§° (ç”¨é€—å·åˆ†éš”): ").strip()
            question_list = [q.strip() for q in questions_input.split(",") if q.strip()]
            
            if question_list:
                print(f"\nğŸ”§ è‡ªå®šä¹‰æµ‹è¯•: {question_list}")
                results = test_suite.test_multiple_questions(question_list)
            else:
                print("âŒ æœªæä¾›æœ‰æ•ˆçš„é—®é¢˜åˆ—è¡¨")
                return
                
        elif choice == "3":
            confirm = input("\nâš ï¸  å®Œæ•´æµ‹è¯•å°†éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œç¡®è®¤ç»§ç»­? (y/N): ").strip().lower()
            if confirm == 'y':
                print("\nğŸ”¥ å¼€å§‹å®Œæ•´æµ‹è¯•...")
                results = test_suite.run_full_test()
            else:
                print("å–æ¶ˆå®Œæ•´æµ‹è¯•")
                return
                
        elif choice == "4":
            print(f"\nå¯ç”¨é—®é¢˜: {available_questions[:10]}... (æ˜¾ç¤ºå‰10ä¸ª)")
            question = input("è¯·è¾“å…¥å•ä¸ªé—®é¢˜æ–‡ä»¶å¤¹åç§°: ").strip()
            
            if question in available_questions:
                print(f"\nğŸ” å•ä¸ªé—®é¢˜æµ‹è¯•: {question}")
                result = test_suite.test_single_question(question)
                print(f"\nç»“æœ: {result['status']}")
                if result['status'] == 'success':
                    print(f"   å¤„ç†æ—¶é—´: {result['processing_time']:.2f}ç§’")
                    print(f"   Pipelineç»“æœ: {result['pipeline_results']}")
                else:
                    print(f"   é”™è¯¯: {result.get('error_message', 'æœªçŸ¥é”™è¯¯')}")
            else:
                print(f"âŒ é—®é¢˜ {question} ä¸å­˜åœ¨")
                return
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
            return
            
        print(f"\nğŸ‰ æµ‹è¯•å®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: {test_suite.output_dir}")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        logger.error(f"ä¸»ç¨‹åºé”™è¯¯: {traceback.format_exc()}")

if __name__ == "__main__":
    main() 