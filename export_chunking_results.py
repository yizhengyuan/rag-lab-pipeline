"""
ç›´æ¥å¯¼å‡º chunking.py çš„çœŸå®å¤„ç†ç»“æœ
====================================

è¿™ä¸ªè„šæœ¬ç›´æ¥ä½¿ç”¨æ‚¨çš„ï¼š
- core/chunking.py ä¸­çš„ SemanticChunker
- config/config.yml ä¸­çš„é…ç½®
- å®Œå…¨ç›¸åŒçš„åˆå§‹åŒ–å’Œå¤„ç†æµç¨‹

ç„¶åå°†ç»“æœå¯¼å‡ºä¸ºå•ç‹¬çš„æ–‡æœ¬æ–‡ä»¶
"""

import os
import logging
from datetime import datetime
from llama_index.core import SimpleDirectoryReader

# ç›´æ¥å¯¼å…¥æ‚¨çš„æ¨¡å—
from core.chunking import SemanticChunker  
from config import load_config_from_yaml

logger = logging.getLogger(__name__)

def export_real_chunking_results(pdf_path: str, config_path: str = "config/config.yml", output_dir: str = "./chunking_export"):
    """
    ç›´æ¥å¯¼å‡º chunking.py çš„çœŸå®å¤„ç†ç»“æœ
    
    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„  
        output_dir: è¾“å‡ºç›®å½•
    """
    
    file_name = os.path.basename(pdf_path)
    base_name = os.path.splitext(file_name)[0]
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(os.path.join(output_dir, "chunks"), exist_ok=True)
    
    logger.info(f"ğŸ”„ å¼€å§‹ä½¿ç”¨ chunking.py å¤„ç†: {pdf_path}")
    logger.info(f"âš™ï¸ é…ç½®æ–‡ä»¶: {config_path}")
    logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    logger.info("=" * 80)
    
    try:
        # 1. åŠ è½½æ‚¨çš„é…ç½®
        logger.info("âš™ï¸ æ­¥éª¤1: åŠ è½½é…ç½®...")
        config = load_config_from_yaml(config_path)
        logger.info("   âœ… é…ç½®åŠ è½½å®Œæˆ")
        
        # 2. åˆå§‹åŒ–æ‚¨çš„ SemanticChunker
        logger.info("ğŸ§  æ­¥éª¤2: åˆå§‹åŒ– SemanticChunker...")
        chunker = SemanticChunker(config)
        logger.info("   âœ… SemanticChunker åˆå§‹åŒ–å®Œæˆ")
        
        # 3. åŠ è½½PDFæ–‡æ¡£
        logger.info("ğŸ“„ æ­¥éª¤3: åŠ è½½PDFæ–‡æ¡£...")
        reader = SimpleDirectoryReader(input_files=[pdf_path])
        documents = reader.load_data()
        full_text = "\n\n".join([doc.text for doc in documents])
        logger.info(f"   âœ… PDFåŠ è½½å®Œæˆ: {len(full_text)} å­—ç¬¦")
        
        # 4. ä½¿ç”¨æ‚¨çš„ chunking.py è¿›è¡Œå¤„ç†
        logger.info("âœ‚ï¸ æ­¥éª¤4: æ‰§è¡ŒçœŸå®çš„è¯­ä¹‰åˆ†å—å’Œæ¦‚å¿µæå–...")
        chunk_nodes = chunker.chunk_and_extract_concepts(documents)
        logger.info(f"   âœ… å¤„ç†å®Œæˆ: {len(chunk_nodes)} ä¸ªchunk")
        
        # 5. è·å–ç»Ÿè®¡ä¿¡æ¯
        logger.info("ğŸ“Š æ­¥éª¤5: è·å–ç»Ÿè®¡ä¿¡æ¯...")
        stats = chunker.get_chunking_statistics()
        logger.info(f"   âœ… ç»Ÿè®¡å®Œæˆ: æ€»æ¦‚å¿µæ•° {stats.get('total_concepts', 0)}")
        
        # 6. å¯¼å‡ºä¸ºå•ç‹¬çš„æ–‡æœ¬æ–‡ä»¶
        logger.info("ğŸ’¾ æ­¥éª¤6: å¯¼å‡ºchunkä¸ºå•ç‹¬æ–‡ä»¶...")
        chunk_files = export_chunks_to_individual_files(chunk_nodes, base_name, output_dir)
        logger.info(f"   âœ… å·²å¯¼å‡º {len(chunk_files)} ä¸ªæ–‡ä»¶")
        
        # 7. åŒæ—¶ä½¿ç”¨åŸæœ‰çš„å¯¼å‡ºæ–¹æ³•ç”ŸæˆJSON
        logger.info("ğŸ“„ æ­¥éª¤7: ç”ŸæˆJSONæ ¼å¼çš„å®Œæ•´å¯¼å‡º...")
        json_export_path = os.path.join(output_dir, f"{base_name}_chunks_complete.json")
        chunker.export_chunks_with_concepts(json_export_path)
        logger.info(f"   âœ… JSONå¯¼å‡ºå®Œæˆ: {json_export_path}")
        
        # 8. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        logger.info("ğŸ“‹ æ­¥éª¤8: ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š...")
        report_path = generate_detailed_report(
            pdf_path, config_path, stats, chunk_files, json_export_path, base_name, output_dir
        )
        logger.info(f"   âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ: {report_path}")
        
        print(f"\nâœ… chunking.py çœŸå®å¤„ç†ç»“æœå¯¼å‡ºå®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ“Š è¯¦ç»†æŠ¥å‘Š: {report_path}")
        print(f"ğŸ“„ JSONå®Œæ•´å¯¼å‡º: {json_export_path}")
        print(f"ğŸ“ å•ç‹¬chunkæ–‡ä»¶: {output_dir}/chunks/")
        print(f"\nğŸ“ˆ å¤„ç†ç»Ÿè®¡:")
        print(f"   - æ€»chunkæ•°: {len(chunk_nodes)}")
        print(f"   - æ€»æ¦‚å¿µæ•°: {stats.get('total_concepts', 0)}")
        print(f"   - å¹³å‡chunké•¿åº¦: {stats.get('avg_chunk_length', 0):.0f} å­—ç¬¦")
        print(f"   - å¹³å‡æ¯å—æ¦‚å¿µæ•°: {stats.get('avg_concepts_per_chunk', 0):.1f}")
        
        return {
            "success": True,
            "chunk_files": chunk_files,
            "json_export": json_export_path,
            "report": report_path,
            "stats": stats
        }
        
    except Exception as e:
        logger.error(f"âŒ å¤„ç†å¤±è´¥: {str(e)}")
        print(f"\nâŒ å¤„ç†å¤±è´¥: {str(e)}")
        return {"success": False, "error": str(e)}

def export_chunks_to_individual_files(chunk_nodes, base_name: str, output_dir: str):
    """å°†chunk_nodeså¯¼å‡ºä¸ºå•ç‹¬çš„æ–‡æœ¬æ–‡ä»¶"""
    chunk_files = []
    chunks_dir = os.path.join(output_dir, "chunks")
    
    for i, node in enumerate(chunk_nodes):
        # è·å–chunkä¿¡æ¯
        chunk_id = node.metadata.get("chunk_id", f"chunk_{i}")
        concepts = node.metadata.get("concepts", [])
        
        # åˆ›å»ºæ–‡ä»¶å
        chunk_filename = f"{base_name}_{chunk_id}.txt"
        chunk_path = os.path.join(chunks_dir, chunk_filename)
        
        # å†™å…¥æ–‡ä»¶
        with open(chunk_path, 'w', encoding='utf-8') as f:
            # å¤´éƒ¨ä¿¡æ¯
            f.write(f"============================================================\n")
            f.write(f"chunking.py çœŸå®å¤„ç†ç»“æœ\n")
            f.write(f"============================================================\n\n")
            f.write(f"Chunk ID: {chunk_id}\n")
            f.write(f"Chunk Index: {i}\n")
            f.write(f"Text Length: {len(node.text)} characters\n")
            f.write(f"Word Count: {len(node.text.split())} words\n")
            f.write(f"Concept Count: {len(concepts)}\n")
            f.write(f"Node ID: {getattr(node, 'node_id', 'N/A')}\n")
            f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Source: core/chunking.py SemanticChunker\n")
            f.write(f"============================================================\n\n")
            
            # æå–çš„æ¦‚å¿µ
            if concepts:
                f.write(f"ğŸ§  æå–çš„æ¦‚å¿µ (ç”±chunking.pyç”Ÿæˆ):\n")
                for j, concept in enumerate(concepts, 1):
                    f.write(f"  {j:2d}. {concept}\n")
                f.write(f"\n")
            else:
                f.write(f"ğŸ§  æå–çš„æ¦‚å¿µ: (æ— æ¦‚å¿µè¢«æå–)\n\n")
            
            f.write(f"============================================================\n\n")
            
            # å®Œæ•´æ–‡æœ¬å†…å®¹
            f.write(f"ğŸ“„ Chunk å®Œæ•´æ–‡æœ¬å†…å®¹:\n\n")
            f.write(node.text)
            
            # å¦‚æœæœ‰é¢å¤–çš„metadataï¼Œä¹Ÿè®°å½•ä¸‹æ¥
            f.write(f"\n\n============================================================\n")
            f.write(f"ğŸ”§ æŠ€æœ¯ä¿¡æ¯:\n\n")
            for key, value in node.metadata.items():
                if key not in ['concepts', 'chunk_id', 'chunk_length', 'concept_count']:
                    f.write(f"  {key}: {value}\n")
        
        chunk_files.append(chunk_path)
        logger.info(f"   ğŸ’¾ å·²ä¿å­˜: {chunk_filename} ({len(concepts)} ä¸ªæ¦‚å¿µ)")
    
    return chunk_files

def generate_detailed_report(pdf_path, config_path, stats, chunk_files, json_export_path, base_name, output_dir):
    """ç”Ÿæˆè¯¦ç»†çš„å¤„ç†æŠ¥å‘Š"""
    report_path = os.path.join(output_dir, f"{base_name}_chunking_report.md")
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(f"# chunking.py çœŸå®å¤„ç†ç»“æœæŠ¥å‘Š\n\n")
        f.write(f"**å¤„ç†æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write(f"## ğŸ“„ è¾“å…¥ä¿¡æ¯\n\n")
        f.write(f"- **PDFæ–‡ä»¶**: `{pdf_path}`\n")
        f.write(f"- **é…ç½®æ–‡ä»¶**: `{config_path}`\n")
        f.write(f"- **å¤„ç†å™¨**: `core/chunking.py` ä¸­çš„ `SemanticChunker`\n\n")
        
        f.write(f"## ğŸ“Š å¤„ç†ç»Ÿè®¡\n\n")
        f.write(f"- **æ€»chunkæ•°**: {stats.get('total_chunks', 0)}\n")
        f.write(f"- **æ€»æ¦‚å¿µæ•°**: {stats.get('total_concepts', 0)}\n")  
        f.write(f"- **å¹³å‡chunké•¿åº¦**: {stats.get('avg_chunk_length', 0):.0f} å­—ç¬¦\n")
        f.write(f"- **æœ€å°chunké•¿åº¦**: {stats.get('min_chunk_length', 0)} å­—ç¬¦\n")
        f.write(f"- **æœ€å¤§chunké•¿åº¦**: {stats.get('max_chunk_length', 0)} å­—ç¬¦\n")
        f.write(f"- **å¹³å‡æ¯å—æ¦‚å¿µæ•°**: {stats.get('avg_concepts_per_chunk', 0):.1f}\n")
        f.write(f"- **æœ€å°‘æ¦‚å¿µæ•°**: {stats.get('min_concepts_per_chunk', 0)}\n")
        f.write(f"- **æœ€å¤šæ¦‚å¿µæ•°**: {stats.get('max_concepts_per_chunk', 0)}\n\n")
        
        f.write(f"## ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶\n\n")
        f.write(f"### å•ç‹¬çš„chunkæ–‡ä»¶ ({len(chunk_files)}ä¸ª)\n\n")
        for i, chunk_file in enumerate(chunk_files, 1):
            filename = os.path.basename(chunk_file)
            f.write(f"{i:3d}. `{filename}`\n")
        
        f.write(f"\n### å®Œæ•´JSONå¯¼å‡º\n\n")
        f.write(f"- `{os.path.basename(json_export_path)}` - å®Œæ•´çš„JSONæ ¼å¼å¯¼å‡º\n\n")
        
        f.write(f"## ğŸ”§ æŠ€æœ¯è¯´æ˜\n\n")
        f.write(f"æ­¤ç»“æœæ˜¯é€šè¿‡ç›´æ¥è°ƒç”¨æ‚¨çš„ `core/chunking.py` ç”Ÿæˆçš„ï¼Œç¡®ä¿100%ä¸æ‚¨çš„pipelineä¸€è‡´ï¼š\n\n")
        f.write(f"1. **ä½¿ç”¨æ‚¨çš„é…ç½®**: ä» `{config_path}` åŠ è½½æ‰€æœ‰å‚æ•°\n")
        f.write(f"2. **ä½¿ç”¨æ‚¨çš„chunker**: `SemanticChunker` ç±»çš„ `chunk_and_extract_concepts` æ–¹æ³•\n")
        f.write(f"3. **ç›¸åŒçš„åˆå§‹åŒ–**: å®Œå…¨ç›¸åŒçš„é…ç½®å’ŒLLMè®¾ç½®\n")
        f.write(f"4. **çœŸå®çš„æ¦‚å¿µæå–**: æ¯ä¸ªchunkçš„æ¦‚å¿µéƒ½æ˜¯ç”±æ‚¨çš„LLMå®é™…ç”Ÿæˆçš„\n\n")
        
        f.write(f"## ğŸ“– æŸ¥çœ‹è¯´æ˜\n\n")
        f.write(f"æ¯ä¸ªchunkæ–‡ä»¶ (`chunks/` ç›®å½•) åŒ…å«ï¼š\n")
        f.write(f"- ChunkåŸºæœ¬ä¿¡æ¯å’Œç»Ÿè®¡\n")
        f.write(f"- ğŸ§  ç”±æ‚¨çš„LLMå®é™…æå–çš„æ¦‚å¿µ\n")
        f.write(f"- ğŸ“„ å®Œæ•´çš„chunkæ–‡æœ¬å†…å®¹\n")
        f.write(f"- ğŸ”§ æŠ€æœ¯å…ƒæ•°æ®\n\n")
        
        f.write(f"è¿™äº›å°±æ˜¯æ‚¨çš„ `chunking.py` çš„çœŸå®è¾“å‡ºç»“æœï¼\n")
    
    return report_path

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="å¯¼å‡º chunking.py çš„çœŸå®å¤„ç†ç»“æœ")
    parser.add_argument("--pdf", required=True, help="PDFæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--config", default="config/config.yml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", default="./chunking_export", help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    # æ‰§è¡Œå¯¼å‡º
    result = export_real_chunking_results(args.pdf, args.config, args.output)
    
    if not result.get("success"):
        exit(1)

if __name__ == "__main__":
    main()