#!/usr/bin/env python3
"""
æ­¥éª¤7: é—®ç­”ç”Ÿæˆ - å¢å¼ºç‰ˆ
==========================

åŠŸèƒ½ï¼š
1. ä»æ­¥éª¤6çš„ç»“æœä¸­è¯»å–è¯æ®æ•°æ®
2. åŸºäºæ¯ä¸ªè¯æ®ç”Ÿæˆé«˜è´¨é‡é—®ç­”å¯¹
3. æ”¯æŒå¤šç§é—®ç­”ç±»å‹å’Œéš¾åº¦çº§åˆ«
4. ä¿å­˜åˆ°åŒä¸€ä¸ªå®éªŒæ–‡ä»¶å¤¹

ç”¨æ³•: 
- python step7.py <step6è¾“å‡ºæ–‡ä»¶.txt>
- python step7.py <å®éªŒæ–‡ä»¶å¤¹è·¯å¾„>

æ–°åŠŸèƒ½ï¼š
- âœ… è‡ªåŠ¨è¯†åˆ«å¹¶ä½¿ç”¨step6çš„å®éªŒæ–‡ä»¶å¤¹
- âœ… æ™ºèƒ½é—®ç­”ç”Ÿæˆï¼Œæ”¯æŒå¤šç§è®¤çŸ¥å±‚æ¬¡
- âœ… é—®ç­”è´¨é‡è¯„ä¼°å’Œåˆ†ç±»
- âœ… ç»Ÿä¸€çš„å®éªŒç®¡ç†
- âœ… æ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼
"""

import sys
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Set
from collections import defaultdict
import logging

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
sys.path.append(str(Path(__file__).parent))
from llama_index.core import Document
from llama_index.core.schema import TextNode

# å¯¼å…¥æ ¸å¿ƒå¤„ç†æ¨¡å—
from config.settings import load_config_from_yaml

# ğŸ”§ ä¿®æ­£ï¼šå¯¼å…¥QAç”Ÿæˆå™¨
from data_generate_0526 import SimpleDataGenerator as QAGenerator

# å¯¼å…¥å®éªŒç®¡ç†å™¨
from utils.experiment_manager import ExperimentManager
from utils.helpers import FileHelper

logger = logging.getLogger(__name__)

def load_step6_result(step6_file_or_dir: str) -> tuple:
    """
    ä»æ­¥éª¤6çš„è¾“å‡ºæ–‡ä»¶æˆ–å®éªŒæ–‡ä»¶å¤¹ä¸­åŠ è½½ç»“æœ
    
    Args:
        step6_file_or_dir: æ­¥éª¤6çš„è¾“å‡ºæ–‡ä»¶æˆ–å®éªŒæ–‡ä»¶å¤¹è·¯å¾„
        
    Returns:
        tuple: (step6_result, experiment_manager)
    """
    step6_path = Path(step6_file_or_dir)
    
    if step6_path.is_file():
        # æƒ…å†µ1ï¼šç›´æ¥æŒ‡å®šäº†step6çš„è¾“å‡ºæ–‡ä»¶
        if step6_path.name.startswith("step6") and step6_path.suffix == ".txt":
            experiment_dir = step6_path.parent
            experiment_manager = ExperimentManager.load_experiment(str(experiment_dir))
            
            # ä»txtæ–‡ä»¶åŠ è½½ç»“æœ
            step6_result = load_result_from_txt(str(step6_path))
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {step6_path}")
            
    elif step6_path.is_dir():
        # æƒ…å†µ2ï¼šç›´æ¥æŒ‡å®šäº†å®éªŒæ–‡ä»¶å¤¹
        experiment_manager = ExperimentManager.load_experiment(str(step6_path))
        
        # æŸ¥æ‰¾step6çš„è¾“å‡ºæ–‡ä»¶
        step6_txt_path = experiment_manager.get_step_output_path(6, "txt")
        if not step6_txt_path.exists():
            raise FileNotFoundError(f"å®éªŒæ–‡ä»¶å¤¹ä¸­æ‰¾ä¸åˆ°step6è¾“å‡ºæ–‡ä»¶: {step6_txt_path}")
        
        step6_result = load_result_from_txt(str(step6_txt_path))
        
    else:
        raise FileNotFoundError(f"è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {step6_path}")
    
    return step6_result, experiment_manager

def load_result_from_txt(input_file: str) -> Dict[str, Any]:
    """ä»txtæ–‡ä»¶ä¸­åŠ è½½ç»“æœæ•°æ®"""
    with open(input_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    start_marker = "# JSON_DATA_START\n"
    end_marker = "\n# JSON_DATA_END"
    
    start_idx = content.find(start_marker)
    end_idx = content.find(end_marker)
    
    if start_idx == -1 or end_idx == -1:
        raise ValueError("æ— æ³•ä»txtæ–‡ä»¶ä¸­è§£ææ•°æ®")
    
    json_str = content[start_idx + len(start_marker):end_idx]
    return json.loads(json_str)

def extract_evidence_nodes_from_step6(step6_result: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    ä»æ­¥éª¤6ç»“æœä¸­æå–è¯æ®èŠ‚ç‚¹
    
    Args:
        step6_result: æ­¥éª¤6çš„ç»“æœ
        
    Returns:
        List[Dict[str, Any]]: è¯æ®èŠ‚ç‚¹åˆ—è¡¨
    """
    print("ğŸ“Š ä»æ­¥éª¤6ç»“æœä¸­æå–è¯æ®èŠ‚ç‚¹...")
    
    evidence_nodes = step6_result.get("evidence_nodes", [])
    
    if not evidence_nodes:
        raise ValueError("æ­¥éª¤6ç»“æœä¸­æ²¡æœ‰æ‰¾åˆ°è¯æ®æ•°æ®")
    
    print(f"   - è¯æ®èŠ‚ç‚¹æ•°: {len(evidence_nodes)}")
    
    # éªŒè¯è¯æ®èŠ‚ç‚¹æ ¼å¼
    valid_evidences = []
    for evidence in evidence_nodes:
        if isinstance(evidence, dict) and "evidence_text" in evidence:
            valid_evidences.append(evidence)
        else:
            print(f"âš ï¸ è·³è¿‡æ— æ•ˆçš„è¯æ®èŠ‚ç‚¹: {type(evidence)}")
    
    print(f"   - æœ‰æ•ˆè¯æ®èŠ‚ç‚¹æ•°: {len(valid_evidences)}")
    
    return valid_evidences

def generate_qa_pairs_from_evidence(evidence_nodes: List[Dict[str, Any]], 
                                  config) -> Dict[str, Any]:
    """
    ä»è¯æ®ä¸­ç”Ÿæˆé—®ç­”å¯¹
    
    Args:
        evidence_nodes: è¯æ®èŠ‚ç‚¹åˆ—è¡¨
        config: é…ç½®å¯¹è±¡
        
    Returns:
        Dict[str, Any]: é—®ç­”ç”Ÿæˆç»“æœ
    """
    print("â“ å¼€å§‹é—®ç­”ç”Ÿæˆ...")
    
    # ğŸ”§ åˆå§‹åŒ–QAç”Ÿæˆå™¨ï¼Œä½¿ç”¨é…ç½®ä¸­çš„å‚æ•°
    questions_per_type = config.get("qa_generation.questions_per_type", {
        "remember": 2,
        "understand": 2,
        "apply": 1,
        "analyze": 1,
        "evaluate": 1,
        "create": 1
    })
    
    qa_generator = QAGenerator(
        model_name=config.get("openai.model", "gpt-4o-mini"),
        questions_per_type=questions_per_type
    )
    
    all_qa_pairs = []
    qa_stats = defaultdict(int)
    failed_evidences = 0
    
    for i, evidence_node in enumerate(evidence_nodes):
        try:
            evidence_text = evidence_node.get("evidence_text", "")
            concept_text = evidence_node.get("concept_text", "æœªçŸ¥æ¦‚å¿µ")
            evidence_id = evidence_node.get("evidence_id", f"evidence_{i}")
            
            print(f"   ç”Ÿæˆè¯æ® {i+1}/{len(evidence_nodes)} çš„é—®ç­”: {concept_text}")
            
            if not evidence_text or len(evidence_text.strip()) < 20:
                print(f"   âš ï¸ è·³è¿‡è¯æ® {i+1}: æ–‡æœ¬å¤ªçŸ­æˆ–ä¸ºç©º")
                failed_evidences += 1
                continue
            
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨å­—å…¸æ•°æ®ç”Ÿæˆé—®ç­”å¯¹
            qa_pairs = qa_generator.generate_qa_pairs_from_text(evidence_text)
            
            # ä¸ºæ¯ä¸ªé—®ç­”å¯¹æ·»åŠ å…ƒæ•°æ®
            for qa_pair in qa_pairs:
                qa_pair["evidence_source"] = evidence_id
                qa_pair["evidence_concept"] = concept_text
                qa_pair["evidence_relevance"] = evidence_node.get("relevance_score", 0.0)
                qa_pair["evidence_type"] = evidence_node.get("evidence_type", "general")
                qa_pair["generation_timestamp"] = datetime.now().isoformat()
                
                # ç»Ÿè®¡QAç±»å‹
                qa_type = qa_pair.get("type", "unknown")
                qa_stats[qa_type] += 1
            
            all_qa_pairs.extend(qa_pairs)
            print(f"   âœ… ç”Ÿæˆäº† {len(qa_pairs)} ä¸ªé—®ç­”å¯¹")
            
        except Exception as e:
            print(f"   âŒ è¯æ® {i+1} ç”Ÿæˆå¤±è´¥: {e}")
            failed_evidences += 1
            continue
    
    print(f"   âœ… é—®ç­”ç”Ÿæˆå®Œæˆ:")
    print(f"      - æ€»é—®ç­”å¯¹æ•°: {len(all_qa_pairs)}")
    print(f"      - æˆåŠŸå¤„ç†è¯æ®: {len(evidence_nodes) - failed_evidences}/{len(evidence_nodes)}")
    print(f"      - å¤±è´¥è¯æ®æ•°: {failed_evidences}")
    
    return {
        "qa_pairs": all_qa_pairs,
        "qa_stats": dict(qa_stats),
        "total_qa_pairs": len(all_qa_pairs),
        "processed_evidences": len(evidence_nodes) - failed_evidences,
        "failed_evidences": failed_evidences
    }

def analyze_qa_quality(qa_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    åˆ†æé—®ç­”è´¨é‡
    
    Args:
        qa_data: é—®ç­”æ•°æ®
        
    Returns:
        Dict[str, Any]: è´¨é‡åˆ†æç»“æœ
    """
    print("ğŸ“ˆ åˆ†æé—®ç­”è´¨é‡...")
    
    qa_pairs = qa_data["qa_pairs"]
    
    if not qa_pairs:
        return {
            "overall_quality_score": 0.0,
            "difficulty_distribution": {},
            "type_distribution": {},
            "avg_question_length": 0.0,
            "avg_answer_length": 0.0
        }
    
    # éš¾åº¦åˆ†å¸ƒ
    difficulty_counts = defaultdict(int)
    type_counts = defaultdict(int)
    question_lengths = []
    answer_lengths = []
    
    for qa_pair in qa_pairs:
        difficulty = qa_pair.get("difficulty", "unknown")
        qa_type = qa_pair.get("type", "unknown")
        question = qa_pair.get("question", "")
        answer = qa_pair.get("answer", "")
        
        difficulty_counts[difficulty] += 1
        type_counts[qa_type] += 1
        question_lengths.append(len(question))
        answer_lengths.append(len(answer))
    
    # è®¡ç®—å¹³å‡é•¿åº¦
    avg_question_length = sum(question_lengths) / len(question_lengths) if question_lengths else 0
    avg_answer_length = sum(answer_lengths) / len(answer_lengths) if answer_lengths else 0
    
    # è®¡ç®—è´¨é‡åˆ†æ•°ï¼ˆåŸºäºå¤šæ ·æ€§å’Œå®Œæ•´æ€§ï¼‰
    type_diversity = len(type_counts) / 6.0  # æœ€å¤š6ç§ç±»å‹
    difficulty_diversity = len(difficulty_counts) / 3.0  # 3ç§éš¾åº¦
    completeness_score = len(qa_pairs) / max(len(qa_pairs), 10)  # ç›¸å¯¹äºé¢„æœŸæ•°é‡
    
    overall_quality_score = (type_diversity + difficulty_diversity + min(completeness_score, 1.0)) / 3.0
    
    return {
        "overall_quality_score": overall_quality_score,
        "difficulty_distribution": dict(difficulty_counts),
        "type_distribution": dict(type_counts),
        "avg_question_length": avg_question_length,
        "avg_answer_length": avg_answer_length,
        "type_diversity": type_diversity,
        "difficulty_diversity": difficulty_diversity
    }

def process_step7_qa_generation(step6_result: Dict[str, Any], 
                               config_path: str = "config.yml") -> Dict[str, Any]:
    """
    æ‰§è¡Œæ­¥éª¤7çš„é—®ç­”ç”Ÿæˆå¤„ç†
    
    Args:
        step6_result: æ­¥éª¤6çš„ç»“æœ
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        Dict[str, Any]: æ­¥éª¤7çš„å¤„ç†ç»“æœ
    """
    start_time = time.time()
    
    try:
        # 1. åŠ è½½é…ç½®
        print("ğŸ“‹ åŠ è½½é…ç½®...")
        config = load_config_from_yaml(config_path)
        
        # 2. æå–è¯æ®èŠ‚ç‚¹
        print("ğŸ“Š æå–è¯æ®èŠ‚ç‚¹...")
        evidence_nodes = extract_evidence_nodes_from_step6(step6_result)
        
        if not evidence_nodes:
            return {
                "success": True,
                "skipped": True,
                "reason": "æ²¡æœ‰å¯ç”¨çš„è¯æ®æ•°æ®",
                "processing_time": time.time() - start_time,
                "timestamp": datetime.now().isoformat()
            }
        
        # 3. ç”Ÿæˆé—®ç­”å¯¹
        print("â“ ç”Ÿæˆé—®ç­”å¯¹...")
        qa_data = generate_qa_pairs_from_evidence(evidence_nodes, config)
        
        if qa_data["total_qa_pairs"] == 0:
            return {
                "success": True,
                "skipped": True,
                "reason": "æœªèƒ½ç”Ÿæˆä»»ä½•é—®ç­”å¯¹",
                "processing_time": time.time() - start_time,
                "timestamp": datetime.now().isoformat()
            }
        
        # 4. åˆ†æé—®ç­”è´¨é‡
        print("ğŸ“ˆ åˆ†æé—®ç­”è´¨é‡...")
        quality_analysis = analyze_qa_quality(qa_data)
        
        processing_time = time.time() - start_time
        
        # 5. æ„å»ºç»“æœ
        result = {
            "success": True,
            "step": 7,
            "step_name": "é—®ç­”ç”Ÿæˆ",
            "qa_pairs": qa_data["qa_pairs"],
            "statistics": {
                "total_qa_pairs": qa_data["total_qa_pairs"],
                "processed_evidences": qa_data["processed_evidences"],
                "failed_evidences": qa_data["failed_evidences"],
                "qa_type_distribution": qa_data["qa_stats"]
            },
            "quality_analysis": quality_analysis,
            "processing_time": processing_time,
            "timestamp": datetime.now().isoformat(),
            "config_used": {
                "questions_per_type": config.get("qa_generation.questions_per_type", {}),
                "evidence_count": len(evidence_nodes)
            }
        }
        
        return result
        
    except Exception as e:
        error_msg = f"æ­¥éª¤7å¤„ç†å¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        
        result = {
            "success": False,
            "step": 7,
            "step_name": "é—®ç­”ç”Ÿæˆ",
            "error": error_msg,
            "processing_time": time.time() - start_time,
            "timestamp": datetime.now().isoformat()
        }
        
        return result

def main():
    if len(sys.argv) != 2:
        print("ç”¨æ³•: python step7.py <step6è¾“å‡ºæ–‡ä»¶æˆ–å®éªŒæ–‡ä»¶å¤¹>")
        print("ç¤ºä¾‹:")
        print("  python step7.py experiments/20241204_143052_attention_paper/step6_evaluation.txt")
        print("  python step7.py experiments/20241204_143052_attention_paper/")
        print("\næ–°åŠŸèƒ½:")
        print("âœ… è‡ªåŠ¨è¯†åˆ«å®éªŒæ–‡ä»¶å¤¹")
        print("âœ… æ™ºèƒ½é—®ç­”ç”Ÿæˆï¼Œæ”¯æŒå¤šç§è®¤çŸ¥å±‚æ¬¡")
        print("âœ… é—®ç­”è´¨é‡è¯„ä¼°å’Œåˆ†ç±»")
        print("âœ… ç»Ÿä¸€çš„å®éªŒç®¡ç†")
        sys.exit(1)
    
    input_path = sys.argv[1]
    
    print(f"ğŸš€ æ­¥éª¤7: é—®ç­”ç”Ÿæˆ (å¢å¼ºç‰ˆ)")
    print(f"ğŸ“„ è¾“å…¥: {input_path}")
    print("="*60)
    
    try:
        # 1. åŠ è½½æ­¥éª¤6ç»“æœå’Œå®éªŒç®¡ç†å™¨
        print("ğŸ“‚ åŠ è½½æ­¥éª¤6ç»“æœ...")
        step6_result, experiment_manager = load_step6_result(input_path)
        
        if not step6_result.get("success"):
            print("âŒ æ­¥éª¤6æœªæˆåŠŸå®Œæˆï¼Œæ— æ³•ç»§ç»­")
            sys.exit(1)
        
        print(f"âœ… å·²åŠ è½½å®éªŒ: {experiment_manager.experiment_name}")
        print(f"ğŸ“ å®éªŒç›®å½•: {experiment_manager.experiment_dir}")
        print()
        
        # 2. æ‰§è¡Œæ­¥éª¤7å¤„ç†
        print("ğŸ”„ å¼€å§‹æ­¥éª¤7å¤„ç†...")
        result = process_step7_qa_generation(step6_result)
        
        # 3. ä¿å­˜ç»“æœåˆ°å®éªŒæ–‡ä»¶å¤¹
        print("ğŸ’¾ ä¿å­˜å®éªŒç»“æœ...")
        saved_files = experiment_manager.save_step_result(
            step_num=7,
            result=result,
            save_formats=['txt', 'json']
        )
        
        if result.get("success"):
            if result.get("skipped"):
                print(f"\nâ­ï¸ æ­¥éª¤7å·²è·³è¿‡!")
                print(f"ğŸ“ å®éªŒç›®å½•: {experiment_manager.experiment_dir}")
                print(f"âš ï¸ è·³è¿‡åŸå› : {result.get('reason')}")
            else:
                print(f"\nâœ… æ­¥éª¤7å®Œæˆ!")
                print(f"ğŸ“ å®éªŒç›®å½•: {experiment_manager.experiment_dir}")
                print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶:")
                for format_type, file_path in saved_files.items():
                    print(f"   - {format_type.upper()}: {file_path}")
                
                # æ˜¾ç¤ºå¤„ç†ç»“æœæ‘˜è¦
                stats = result.get("statistics", {})
                quality = result.get("quality_analysis", {})
                
                print(f"\nğŸ“Š é—®ç­”ç”Ÿæˆç»“æœæ‘˜è¦:")
                print(f"   - æ€»é—®ç­”å¯¹æ•°: {stats.get('total_qa_pairs', 0)}")
                print(f"   - å¤„ç†è¯æ®æ•°: {stats.get('processed_evidences', 0)}")
                print(f"   - å¤±è´¥è¯æ®æ•°: {stats.get('failed_evidences', 0)}")
                print(f"   - æ•´ä½“è´¨é‡åˆ†æ•°: {quality.get('overall_quality_score', 0):.3f}")
                print(f"   - å¹³å‡é—®é¢˜é•¿åº¦: {quality.get('avg_question_length', 0):.1f} å­—ç¬¦")
                print(f"   - å¹³å‡ç­”æ¡ˆé•¿åº¦: {quality.get('avg_answer_length', 0):.1f} å­—ç¬¦")
                print(f"   - å¤„ç†æ—¶é—´: {result.get('processing_time', 0):.2f} ç§’")
                
                # æ˜¾ç¤ºé—®ç­”ç±»å‹åˆ†å¸ƒ
                qa_type_dist = stats.get("qa_type_distribution", {})
                if qa_type_dist:
                    print(f"\nğŸ“ˆ é—®ç­”ç±»å‹åˆ†å¸ƒ:")
                    for qa_type, count in qa_type_dist.items():
                        print(f"   - {qa_type}: {count} ä¸ª")
                
                # æ˜¾ç¤ºéš¾åº¦åˆ†å¸ƒ
                difficulty_dist = quality.get("difficulty_distribution", {})
                if difficulty_dist:
                    print(f"\nğŸ“Š éš¾åº¦åˆ†å¸ƒ:")
                    for difficulty, count in difficulty_dist.items():
                        print(f"   - {difficulty}: {count} ä¸ª")
                
                # æ˜¾ç¤ºé—®ç­”å¯¹ç¤ºä¾‹
                qa_pairs = result.get("qa_pairs", [])
                if qa_pairs:
                    print(f"\nğŸŒŸ é—®ç­”å¯¹ç¤ºä¾‹ (å‰3ä¸ª):")
                    for i, qa_pair in enumerate(qa_pairs[:3], 1):
                        print(f"   {i}. ç±»å‹: {qa_pair.get('type', 'æœªçŸ¥')}, éš¾åº¦: {qa_pair.get('difficulty', 'æœªçŸ¥')}")
                        print(f"      æ¦‚å¿µ: {qa_pair.get('evidence_concept', 'æœªçŸ¥')}")
                        print(f"      é—®é¢˜: {qa_pair.get('question', 'æœªçŸ¥')[:80]}...")
                        print(f"      ç­”æ¡ˆ: {qa_pair.get('answer', 'æœªçŸ¥')[:100]}...")
                        print()
                
                if len(qa_pairs) > 3:
                    print(f"   ... è¿˜æœ‰ {len(qa_pairs) - 3} ä¸ªé—®ç­”å¯¹")
            
            # æ˜¾ç¤ºå®éªŒçŠ¶æ€
            summary = experiment_manager.get_experiment_summary()
            print(f"\nğŸ§ª å®éªŒçŠ¶æ€:")
            print(f"   - å®éªŒID: {summary['experiment_id']}")
            print(f"   - å·²å®Œæˆæ­¥éª¤: {summary['steps_completed']}/{summary['total_steps']}")
            print(f"   - å½“å‰çŠ¶æ€: {summary['status']}")
            
            # æç¤ºåç»­æ­¥éª¤
            if not result.get("skipped"):
                print(f"\nğŸ‰ æµæ°´çº¿å¤„ç†å®Œæˆ!")
                print(f"   æŸ¥çœ‹ç»“æœ: cat {saved_files['txt']}")
                print(f"   æŸ¥çœ‹é—®ç­”: grep -A 10 'é—®ç­”å¯¹ç¤ºä¾‹' {saved_files['txt']}")
                print(f"   å®éªŒç›®å½•: ls {experiment_manager.experiment_dir}")
                
        else:
            print(f"âŒ æ­¥éª¤7å¤±è´¥: {result.get('error')}")
            
            # å³ä½¿å¤±è´¥ä¹Ÿä¿å­˜é”™è¯¯ä¿¡æ¯
            experiment_manager.save_step_result(
                step_num=7,
                result=result,
                save_formats=['txt']
            )
            sys.exit(1)
            
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        # å¦‚æœæœ‰å®éªŒç®¡ç†å™¨ï¼Œä¿å­˜é”™è¯¯ä¿¡æ¯
        if 'experiment_manager' in locals():
            error_result = {
                "step": 7,
                "step_name": "é—®ç­”ç”Ÿæˆ",
                "success": False,
                "error": str(e),
                "traceback": traceback.format_exc(),
                "processing_time": 0.0,
                "timestamp": datetime.now().isoformat()
            }
            
            try:
                experiment_manager.save_step_result(7, error_result, ['txt'])
                print(f"ğŸ“„ é”™è¯¯ä¿¡æ¯å·²ä¿å­˜åˆ°å®éªŒç›®å½•: {experiment_manager.experiment_dir}")
            except:
                pass
        
        sys.exit(1)

if __name__ == "__main__":
    main()