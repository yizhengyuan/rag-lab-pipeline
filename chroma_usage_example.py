"""
Chroma å‘é‡æ•°æ®åº“è¯¦ç»†ä½¿ç”¨ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•åœ¨æ¦‚å¿µæå–ç®¡é“ä¸­ä½¿ç”¨ Chroma è¿›è¡Œæœ¬åœ°å‘é‡å­˜å‚¨

æœ¬æ–‡ä»¶æ¼”ç¤ºäº†ï¼š
1. åŸºç¡€é…ç½®å’Œåˆå§‹åŒ–
2. æ–‡æ¡£å¤„ç†å’Œåˆ†å—
3. å‘é‡å­˜å‚¨å’Œæ£€ç´¢
4. ç¼“å­˜æœºåˆ¶çš„ä½¿ç”¨
5. æ•°æ®ç®¡ç†å’Œç»´æŠ¤
"""

import os
import sys
import logging
from pathlib import Path
from typing import List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from core.chunking import SemanticChunker, EmbeddingCache
from core.vector_store import VectorStoreManager
from utils.config_loader import ConfigLoader
from llama_index.core import Document

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def demo_basic_usage():
    """æ¼”ç¤ºåŸºæœ¬çš„ Chroma ä½¿ç”¨æµç¨‹"""
    
    print("=" * 60)
    print("ğŸ¯ Chroma åŸºæœ¬ä½¿ç”¨æµç¨‹æ¼”ç¤º")
    print("=" * 60)
    
    # 1. åŠ è½½é…ç½®
    print("\nğŸ“‹ æ­¥éª¤ 1: åŠ è½½é…ç½®")
    config = ConfigLoader.load_config()
    
    # æ£€æŸ¥é…ç½®
    vector_type = config.get('vector_store.type')
    persist_dir = config.get('vector_store.persist_directory')
    collection_name = config.get('vector_store.collection_name')
    
    print(f"   âœ… å‘é‡æ•°æ®åº“ç±»å‹: {vector_type}")
    print(f"   ğŸ“‚ å­˜å‚¨ç›®å½•: {persist_dir}")
    print(f"   ğŸ·ï¸  é›†åˆåç§°: {collection_name}")
    
    # 2. åˆå§‹åŒ–ç»„ä»¶
    print("\nğŸ”§ æ­¥éª¤ 2: åˆå§‹åŒ–ç»„ä»¶")
    
    # åˆå§‹åŒ–è¯­ä¹‰åˆ†å—å™¨ï¼ˆè‡ªåŠ¨å¯ç”¨embeddingç¼“å­˜ï¼‰
    chunker = SemanticChunker(config)
    print("   âœ… SemanticChunker åˆå§‹åŒ–å®Œæˆ")
    
    # åˆå§‹åŒ–å‘é‡å­˜å‚¨ç®¡ç†å™¨ï¼ˆè‡ªåŠ¨è¿æ¥Chromaï¼‰
    vector_manager = VectorStoreManager(config)
    print("   âœ… VectorStoreManager åˆå§‹åŒ–å®Œæˆ")
    
    # 3. å‡†å¤‡æµ‹è¯•æ–‡æ¡£
    print("\nğŸ“ æ­¥éª¤ 3: å‡†å¤‡æµ‹è¯•æ–‡æ¡£")
    documents = [
        Document(
            text="Transformeræ¶æ„æ˜¯ç°ä»£è‡ªç„¶è¯­è¨€å¤„ç†çš„åŸºç¡€ï¼Œé€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶å®ç°äº†å¹¶è¡Œè®¡ç®—ã€‚",
            metadata={"source": "transformer_paper", "type": "technical"}
        ),
        Document(
            text="BERTæ¨¡å‹åŸºäºTransformerçš„ç¼–ç å™¨ç»“æ„ï¼Œé€šè¿‡æ©ç è¯­è¨€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒã€‚",
            metadata={"source": "bert_paper", "type": "technical"}
        ),
        Document(
            text="GPTç³»åˆ—æ¨¡å‹é‡‡ç”¨Transformerçš„è§£ç å™¨ç»“æ„ï¼Œä¸“æ³¨äºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚",
            metadata={"source": "gpt_paper", "type": "technical"}
        )
    ]
    print(f"   ğŸ“„ å‡†å¤‡äº† {len(documents)} ä¸ªæµ‹è¯•æ–‡æ¡£")
    
    # 4. ç¬¬ä¸€æ¬¡å¤„ç† - ä¼šè°ƒç”¨APIå¹¶ç¼“å­˜
    print("\nğŸ”„ æ­¥éª¤ 4: ç¬¬ä¸€æ¬¡å¤„ç†ï¼ˆä¼šè°ƒç”¨OpenAI APIï¼‰")
    
    chunk_nodes = chunker.chunk_and_extract_concepts(documents)
    print(f"   âœ… è¯­ä¹‰åˆ†å—å®Œæˆ: {len(chunk_nodes)} ä¸ªchunks")
    
    # æ˜¾ç¤ºæ¦‚å¿µæå–ç»“æœ
    for i, node in enumerate(chunk_nodes):
        concepts = chunker.get_concepts_from_node(node)
        text_preview = node.text[:50] + "..." if len(node.text) > 50 else node.text
        print(f"   ğŸ“ Chunk {i+1}: {len(concepts)} ä¸ªæ¦‚å¿µ")
        print(f"      ğŸ“ æ–‡æœ¬: {text_preview}")
        print(f"      ğŸ·ï¸  æ¦‚å¿µ: {concepts}")
    
    # 5. åˆ›å»ºå‘é‡ç´¢å¼•å¹¶å­˜å‚¨åˆ°Chroma
    print("\nğŸ—ƒï¸  æ­¥éª¤ 5: åˆ›å»ºå‘é‡ç´¢å¼•å¹¶å­˜å‚¨")
    
    chunk_index = vector_manager.create_chunk_index(chunk_nodes, persist=True)
    print("   âœ… å‘é‡ç´¢å¼•å·²åˆ›å»ºå¹¶å­˜å‚¨åˆ°Chroma")
    
    # 6. éªŒè¯å­˜å‚¨
    print("\nğŸ” æ­¥éª¤ 6: éªŒè¯æ•°æ®å­˜å‚¨")
    
    # æ£€æŸ¥Chromaæ•°æ®åº“ç›®å½•
    if os.path.exists(persist_dir):
        db_files = list(Path(persist_dir).glob("*"))
        print(f"   ğŸ“ Chromaæ•°æ®åº“æ–‡ä»¶: {len(db_files)} ä¸ª")
        for file in db_files[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
            print(f"      ğŸ“„ {file.name}")
    
    # æ£€æŸ¥ç´¢å¼•ä¿¡æ¯
    index_info = vector_manager.get_index_info()
    for index_type, info in index_info['indexes'].items():
        if info['exists']:
            node_count = info['metadata'].get('node_count', 0)
            print(f"   ğŸ“Š {index_type} ç´¢å¼•: {node_count} ä¸ªèŠ‚ç‚¹")
    
    # 7. æµ‹è¯•å‘é‡æ£€ç´¢
    print("\nğŸ” æ­¥éª¤ 7: æµ‹è¯•å‘é‡æ£€ç´¢")
    
    query_engine = chunk_index.as_query_engine(
        similarity_top_k=3,
        verbose=True
    )
    
    test_query = "Transformeræ¨¡å‹çš„æ ¸å¿ƒæœºåˆ¶æ˜¯ä»€ä¹ˆï¼Ÿ"
    print(f"   â“ æŸ¥è¯¢: {test_query}")
    
    try:
        response = query_engine.query(test_query)
        print(f"   âœ… æ£€ç´¢æˆåŠŸ")
        print(f"   ğŸ“ å›ç­”: {response.response[:100]}...")
        
        # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„æºæ–‡æ¡£
        if hasattr(response, 'source_nodes'):
            print(f"   ğŸ“š æ£€ç´¢åˆ° {len(response.source_nodes)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
    except Exception as e:
        print(f"   âš ï¸  æ£€ç´¢æµ‹è¯•å¤±è´¥: {e}")
    
    return chunker, vector_manager, chunk_nodes

def demo_caching_mechanism():
    """æ¼”ç¤ºç¼“å­˜æœºåˆ¶çš„å·¥ä½œåŸç†"""
    
    print("\n" + "=" * 60)
    print("ğŸ’¾ ç¼“å­˜æœºåˆ¶æ¼”ç¤º")
    print("=" * 60)
    
    config = ConfigLoader.load_config()
    chunker = SemanticChunker(config)
    
    # å‡†å¤‡ç›¸åŒçš„æ–‡æ¡£
    documents = [
        Document(text="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£ï¼Œç”¨äºæ¼”ç¤ºç¼“å­˜æœºåˆ¶çš„å·¥ä½œåŸç†ã€‚"),
        Document(text="ç¬¬äºŒä¸ªæ–‡æ¡£åŒ…å«ä¸åŒçš„å†…å®¹ï¼Œä½†åŒæ ·ä¼šè¢«ç¼“å­˜ã€‚")
    ]
    
    print("\nğŸ”„ ç¬¬ä¸€æ¬¡å¤„ç†ï¼ˆä¼šè°ƒç”¨APIï¼‰...")
    import time
    start_time = time.time()
    
    chunk_nodes_1 = chunker.chunk_and_extract_concepts(documents)
    first_time = time.time() - start_time
    
    print(f"   â±ï¸  ç¬¬ä¸€æ¬¡å¤„ç†è€—æ—¶: {first_time:.2f} ç§’")
    
    # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
    if chunker.embedding_cache:
        stats = chunker.embedding_cache.get_cache_stats()
        print(f"   ğŸ’¾ ç¼“å­˜æ¡ç›®: {stats['total_entries']}")
        print(f"   ğŸ’½ ç¼“å­˜å¤§å°: {stats['estimated_size_mb']:.2f} MB")
    
    print("\nğŸ”„ ç¬¬äºŒæ¬¡å¤„ç†ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰...")
    
    # é‡ç½®åˆ†å—å™¨çŠ¶æ€ä½†ä¿ç•™ç¼“å­˜
    chunker.chunk_nodes = []
    chunker.chunk_index = None
    
    start_time = time.time()
    chunk_nodes_2 = chunker.chunk_and_extract_concepts(documents)
    second_time = time.time() - start_time
    
    print(f"   â±ï¸  ç¬¬äºŒæ¬¡å¤„ç†è€—æ—¶: {second_time:.2f} ç§’")
    print(f"   ğŸš€ é€Ÿåº¦æå‡: {first_time/second_time:.1f}x")
    
    # éªŒè¯ç»“æœä¸€è‡´æ€§
    if len(chunk_nodes_1) == len(chunk_nodes_2):
        print("   âœ… ä¸¤æ¬¡å¤„ç†ç»“æœä¸€è‡´")
    else:
        print("   âš ï¸  ç»“æœä¸ä¸€è‡´ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜")

def demo_data_management():
    """æ¼”ç¤ºæ•°æ®ç®¡ç†åŠŸèƒ½"""
    
    print("\n" + "=" * 60)
    print("ğŸ› ï¸  æ•°æ®ç®¡ç†åŠŸèƒ½æ¼”ç¤º")
    print("=" * 60)
    
    config = ConfigLoader.load_config()
    vector_manager = VectorStoreManager(config)
    
    # 1. æŸ¥çœ‹å­˜å‚¨ä¿¡æ¯
    print("\nğŸ“Š å½“å‰å­˜å‚¨çŠ¶æ€:")
    
    try:
        storage_info = vector_manager.get_storage_size()
        total_size = storage_info.get('total', {}).get('size_mb', 0)
        print(f"   ğŸ’½ æ€»å­˜å‚¨å¤§å°: {total_size:.2f} MB")
        
        for index_type, info in storage_info.items():
            if index_type != 'total' and isinstance(info, dict):
                size_mb = info.get('size_mb', 0)
                if size_mb > 0:
                    print(f"   ğŸ“ {index_type}: {size_mb:.2f} MB")
    except Exception as e:
        print(f"   âš ï¸  è·å–å­˜å‚¨ä¿¡æ¯å¤±è´¥: {e}")
    
    # 2. ç´¢å¼•ä¿¡æ¯
    print("\nğŸ“‹ ç´¢å¼•ä¿¡æ¯:")
    index_info = vector_manager.get_index_info()
    
    print(f"   ğŸ—ƒï¸  æ•°æ®åº“ç±»å‹: {index_info['store_type']}")
    print(f"   ğŸ“‚ å­˜å‚¨ç›®å½•: {index_info['persist_directory']}")
    
    for index_type, info in index_info['indexes'].items():
        exists = info['exists']
        persisted = info['persisted']
        node_count = info['metadata'].get('node_count', 0)
        
        status = "âœ…" if exists else "âŒ"
        persist_status = "ğŸ’¾" if persisted else "âŒ"
        
        print(f"   {status} {index_type}: å†…å­˜ä¸­={exists}, æŒä¹…åŒ–={persisted}, èŠ‚ç‚¹æ•°={node_count}")
    
    # 3. å¤‡ä»½æ¼”ç¤º
    print("\nğŸ’¾ å¤‡ä»½åŠŸèƒ½æ¼”ç¤º:")
    
    backup_dir = "./demo_backup"
    try:
        success = vector_manager.backup_indexes(backup_dir)
        if success:
            print(f"   âœ… å¤‡ä»½åˆ›å»ºæˆåŠŸ: {backup_dir}")
            
            # æ˜¾ç¤ºå¤‡ä»½å†…å®¹
            backup_path = Path(backup_dir)
            if backup_path.exists():
                backup_folders = [d for d in backup_path.iterdir() if d.is_dir()]
                if backup_folders:
                    latest_backup = max(backup_folders, key=lambda x: x.stat().st_mtime)
                    print(f"   ğŸ“ æœ€æ–°å¤‡ä»½: {latest_backup.name}")
        else:
            print("   âŒ å¤‡ä»½åˆ›å»ºå¤±è´¥")
    except Exception as e:
        print(f"   âš ï¸  å¤‡ä»½æ“ä½œå¤±è´¥: {e}")

def demo_advanced_usage():
    """æ¼”ç¤ºé«˜çº§ä½¿ç”¨åœºæ™¯"""
    
    print("\n" + "=" * 60)
    print("ğŸš€ é«˜çº§ä½¿ç”¨åœºæ™¯æ¼”ç¤º")
    print("=" * 60)
    
    config = ConfigLoader.load_config()
    vector_manager = VectorStoreManager(config)
    
    # 1. å¤šç±»å‹æ•°æ®å­˜å‚¨
    print("\nğŸ—‚ï¸  å¤šç±»å‹æ•°æ®å­˜å‚¨:")
    
    from core.nodes import ConceptNode
    from llama_index.core.schema import TextNode
    
    # åˆ›å»ºä¸åŒç±»å‹çš„èŠ‚ç‚¹
    concept_nodes = [
        ConceptNode(text="æ·±åº¦å­¦ä¹ ", concept_type="æŠ€æœ¯æ¦‚å¿µ"),
        ConceptNode(text="ç¥ç»ç½‘ç»œ", concept_type="æŠ€æœ¯æ¦‚å¿µ"),
        ConceptNode(text="Transformer", concept_type="æ¨¡å‹æ¶æ„")
    ]
    
    evidence_nodes = [
        TextNode(
            text="Transformeræ¨¡å‹åœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡",
            metadata={"type": "evidence", "concept": "Transformer", "source": "research"}
        )
    ]
    
    try:
        # åˆ†åˆ«å­˜å‚¨ä¸åŒç±»å‹çš„æ•°æ®
        concept_index = vector_manager.create_concept_index(concept_nodes)
        evidence_index = vector_manager.create_evidence_index(evidence_nodes)
        
        print("   âœ… æ¦‚å¿µç´¢å¼•åˆ›å»ºæˆåŠŸ")
        print("   âœ… è¯æ®ç´¢å¼•åˆ›å»ºæˆåŠŸ")
        
        # æ˜¾ç¤ºå„ç´¢å¼•çš„èŠ‚ç‚¹æ•°
        info = vector_manager.get_index_info()
        for index_type, index_info in info['indexes'].items():
            if index_info['exists']:
                count = index_info['metadata'].get('node_count', 0)
                print(f"   ğŸ“Š {index_type} ç´¢å¼•: {count} ä¸ªèŠ‚ç‚¹")
    
    except Exception as e:
        print(f"   âš ï¸  å¤šç±»å‹å­˜å‚¨å¤±è´¥: {e}")
    
    # 2. ç¼“å­˜æ¸…ç†æ¼”ç¤º
    print("\nğŸ§¹ ç¼“å­˜æ¸…ç†:")
    
    chunker = SemanticChunker(config)
    if chunker.embedding_cache:
        initial_stats = chunker.embedding_cache.get_cache_stats()
        print(f"   ğŸ“Š æ¸…ç†å‰: {initial_stats['total_entries']} ä¸ªç¼“å­˜æ¡ç›®")
        
        # æ¸…ç†è¿‡æœŸç¼“å­˜
        chunker.embedding_cache.clear_expired()
        
        final_stats = chunker.embedding_cache.get_cache_stats()
        print(f"   ğŸ“Š æ¸…ç†å: {final_stats['total_entries']} ä¸ªç¼“å­˜æ¡ç›®")
        
        cleaned = initial_stats['total_entries'] - final_stats['total_entries']
        if cleaned > 0:
            print(f"   ğŸ—‘ï¸  æ¸…ç†äº† {cleaned} ä¸ªè¿‡æœŸç¼“å­˜")
        else:
            print("   âœ… æ²¡æœ‰è¿‡æœŸç¼“å­˜éœ€è¦æ¸…ç†")

def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    
    print("ğŸ¯ Chroma å‘é‡æ•°æ®åº“å®Œæ•´ä½¿ç”¨æ¼”ç¤º")
    print("=" * 70)
    
    try:
        # åŸºæœ¬ä½¿ç”¨æµç¨‹
        chunker, vector_manager, chunk_nodes = demo_basic_usage()
        
        # ç¼“å­˜æœºåˆ¶æ¼”ç¤º
        demo_caching_mechanism()
        
        # æ•°æ®ç®¡ç†æ¼”ç¤º  
        demo_data_management()
        
        # é«˜çº§ä½¿ç”¨æ¼”ç¤º
        demo_advanced_usage()
        
        print("\n" + "=" * 70)
        print("âœ… æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ’¡ å…³é”®è¦ç‚¹:")
        print("   ğŸ”¥ é¦–æ¬¡å¤„ç†æ—¶ä¼šè°ƒç”¨APIå¹¶ç¼“å­˜ç»“æœ")
        print("   âš¡ åç»­ç›¸åŒå†…å®¹ç›´æ¥ä½¿ç”¨ç¼“å­˜ï¼Œé€Ÿåº¦æå‡æ•°åå€")
        print("   ğŸ’¾ æ‰€æœ‰å‘é‡æ•°æ®è‡ªåŠ¨æŒä¹…åŒ–åˆ°æœ¬åœ°Chromaæ•°æ®åº“")
        print("   ğŸ—‚ï¸  æ”¯æŒå¤šç§æ•°æ®ç±»å‹çš„åˆ†ç¦»å­˜å‚¨")
        print("   ğŸ› ï¸  æä¾›å®Œæ•´çš„æ•°æ®ç®¡ç†å’Œå¤‡ä»½åŠŸèƒ½")
        
        print(f"\nğŸ“‚ ç”Ÿæˆçš„æ–‡ä»¶å’Œç›®å½•:")
        print(f"   ğŸ—ƒï¸  å‘é‡æ•°æ®åº“: ./vector_db/")
        print(f"   ğŸ’¾ Embeddingç¼“å­˜: ./embedding_cache/")
        print(f"   ğŸ“¦ å¤‡ä»½æ–‡ä»¶: ./demo_backup/")
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 