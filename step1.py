#!/usr/bin/env python3
"""
æ­¥éª¤1: æ–‡ä»¶åŠ è½½ + å‘é‡åŒ–å­˜å‚¨ - å¢å¼ºç‰ˆ
==========================================

åŠŸèƒ½ï¼š
1. åŠ è½½å„ç§æ ¼å¼çš„æ–‡æ¡£æ–‡ä»¶
2. è¿›è¡Œè¯­ä¹‰åˆ†å—å’Œæ¦‚å¿µæå–
3. å‘é‡åŒ–å¹¶å­˜å‚¨åˆ°æœ¬åœ°Chromaæ•°æ®åº“
4. ç”Ÿæˆè¯¦ç»†çš„å‘é‡åŒ–æŠ¥å‘Š

ç”¨æ³•: python step1.py sourceDoc.txt

æ–°åŠŸèƒ½ï¼š
- âœ… è‡ªåŠ¨åˆ›å»ºåŸºäºæ—¶é—´æˆ³çš„å®éªŒæ–‡ä»¶å¤¹
- âœ… ç»Ÿä¸€çš„è¾“å‡ºæ–‡ä»¶ç®¡ç†
- âœ… æ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼ï¼ˆtxt, json, mdï¼‰
- âœ… å®éªŒå…ƒæ•°æ®è‡ªåŠ¨è®°å½•
"""

import sys
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List

# å¯¼å…¥ç°æœ‰çš„å¤„ç†å™¨ç±»
sys.path.append(str(Path(__file__).parent))
from pipeline0604 import FileProcessor
from llama_index.core import Document

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from core.chunking import SemanticChunker
from core.vector_store import VectorStoreManager
from config.settings import load_config_from_yaml

# ğŸ†• å¯¼å…¥å®éªŒç®¡ç†å™¨
from utils.experiment_manager import create_experiment_manager

def convert_result_to_serializable(result):
    """å°†ç»“æœè½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼"""
    serializable_result = {}
    
    for key, value in result.items():
        if key == "document":
            # ç‰¹æ®Šå¤„ç†Documentå¯¹è±¡
            if hasattr(value, 'text') and hasattr(value, 'metadata'):
                serializable_result[key] = {
                    "text": value.text,
                    "metadata": dict(value.metadata)
                }
            elif isinstance(value, dict):
                serializable_result[key] = value
            else:
                serializable_result[key] = str(value)
        elif key == "chunk_nodes":
            # å¤„ç†æ–‡æœ¬èŠ‚ç‚¹åˆ—è¡¨
            serializable_result[key] = []
            for node in value:
                # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®è§£ææ¦‚å¿µJSONå­—ç¬¦ä¸²
                concepts_data = node.metadata.get("concepts", "[]")
                if isinstance(concepts_data, str):
                    try:
                        concepts = json.loads(concepts_data)
                    except json.JSONDecodeError:
                        concepts = []
                else:
                    concepts = concepts_data if isinstance(concepts_data, list) else []
                
                node_data = {
                    "node_id": node.node_id,
                    "text": node.text,
                    "text_length": len(node.text),
                    "concepts": concepts,  # ä½¿ç”¨è§£æåçš„æ¦‚å¿µåˆ—è¡¨
                    "metadata": dict(node.metadata)
                }
                serializable_result[key].append(node_data)
        elif key == "vector_info":
            # å‘é‡ä¿¡æ¯å·²ç»æ˜¯å¯åºåˆ—åŒ–çš„
            serializable_result[key] = value
        else:
            # å…¶ä»–é”®å€¼ç›´æ¥å¤åˆ¶
            serializable_result[key] = value
    
    return serializable_result

def save_result_as_txt(result, output_file):
    """ä¿å­˜ç»“æœä¸ºtxtæ ¼å¼ï¼ŒåŒ…å«å‘é‡åŒ–ä¿¡æ¯ - å…¼å®¹æ€§å‡½æ•°"""
    
    # ğŸ”§ è‡ªåŠ¨åºåˆ—åŒ–åŸå§‹ç»“æœ
    if 'chunk_nodes' in result and result['chunk_nodes']:
        first_chunk = result['chunk_nodes'][0]
        if hasattr(first_chunk, 'text'):  # æ˜¯TextNodeå¯¹è±¡
            result = convert_result_to_serializable(result)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("æ­¥éª¤1: æ–‡ä»¶åŠ è½½ + å‘é‡åŒ–å­˜å‚¨ - å¤„ç†ç»“æœ\n")
        f.write("="*80 + "\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"å¤„ç†çŠ¶æ€: {'âœ… æˆåŠŸ' if result.get('success') else 'âŒ å¤±è´¥'}\n")
        
        if result.get("success"):
            # æ–‡æ¡£åŸºæœ¬ä¿¡æ¯
            doc_data = result.get("document", {})
            if hasattr(doc_data, 'text') and hasattr(doc_data, 'metadata'):
                # Documentå¯¹è±¡
                metadata = doc_data.metadata
                text_content = doc_data.text
            elif isinstance(doc_data, dict):
                metadata = doc_data.get("metadata", {})
                text_content = doc_data.get("text", "")
            else:
                metadata = {}
                text_content = str(doc_data) if doc_data else ""
            
            f.write(f"\nğŸ“Š æ–‡æ¡£åŸºæœ¬ä¿¡æ¯:\n")
            f.write(f"- æ–‡ä»¶å: {metadata.get('file_name', 'æœªçŸ¥')}\n")
            f.write(f"- æ–‡ä»¶ç±»å‹: {metadata.get('file_type', 'æœªçŸ¥')}\n")
            f.write(f"- æ–‡ä»¶å¤§å°: {metadata.get('file_size', 0) / 1024:.2f} KB\n")
            f.write(f"- æ–‡æœ¬é•¿åº¦: {len(text_content):,} å­—ç¬¦\n")
            f.write(f"- å¤„ç†æ—¶é—´: {result.get('processing_time', 0):.2f} ç§’\n")
            
            # åˆ†å—ä¿¡æ¯
            chunks = result.get("chunk_nodes", [])
            f.write(f"\nğŸ“„ æ–‡æ¡£åˆ†å—ä¿¡æ¯:\n")
            f.write(f"- æ€»åˆ†å—æ•°: {len(chunks)}\n")
            
            if chunks:
                # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®å¤„ç†æ¦‚å¿µæ•°æ®
                chunk_lengths = []
                concept_counts = []
                all_concepts = set()
                
                for chunk in chunks:
                    if hasattr(chunk, 'text'):
                        # TextNodeå¯¹è±¡ - éœ€è¦è§£æJSONå­—ç¬¦ä¸²
                        chunk_lengths.append(len(chunk.text))
                        concepts_data = chunk.metadata.get("concepts", "[]")
                        if isinstance(concepts_data, str):
                            try:
                                concepts = json.loads(concepts_data)
                            except json.JSONDecodeError:
                                concepts = []
                        else:
                            concepts = concepts_data if isinstance(concepts_data, list) else []
                        concept_counts.append(len(concepts))
                        all_concepts.update(concepts)
                    else:
                        # å­—å…¸å¯¹è±¡ï¼ˆå‘åå…¼å®¹ï¼‰
                        chunk_lengths.append(chunk.get('text_length', 0))
                        concepts = chunk.get('concepts', [])
                        if isinstance(concepts, list):
                            concept_counts.append(len(concepts))
                            all_concepts.update(concepts)
                        else:
                            concept_counts.append(0)
                
                if chunk_lengths:
                    f.write(f"- å¹³å‡åˆ†å—é•¿åº¦: {sum(chunk_lengths) / len(chunk_lengths):.1f} å­—ç¬¦\n")
                    f.write(f"- æœ€çŸ­åˆ†å—: {min(chunk_lengths)} å­—ç¬¦\n")
                    f.write(f"- æœ€é•¿åˆ†å—: {max(chunk_lengths)} å­—ç¬¦\n")
                
                f.write(f"- æ€»æ¦‚å¿µæ•°: {sum(concept_counts)}\n")
                f.write(f"- å”¯ä¸€æ¦‚å¿µæ•°: {len(all_concepts)}\n")
                if concept_counts:
                    f.write(f"- å¹³å‡æ¯åˆ†å—æ¦‚å¿µæ•°: {sum(concept_counts) / len(concept_counts):.1f}\n")
            
            # å‘é‡åŒ–ä¿¡æ¯
            vector_info = result.get("vector_info", {})
            f.write(f"\nğŸ—ƒï¸  å‘é‡åŒ–å­˜å‚¨ä¿¡æ¯:\n")
            f.write(f"- å‘é‡æ•°æ®åº“ç±»å‹: {vector_info.get('store_type', 'æœªçŸ¥')}\n")
            f.write(f"- å­˜å‚¨ç›®å½•: {vector_info.get('persist_directory', 'æœªçŸ¥')}\n")
            f.write(f"- é›†åˆåç§°: {vector_info.get('collection_name', 'æœªçŸ¥')}\n")
            f.write(f"- å‘é‡ç»´åº¦: {vector_info.get('dimension', 'æœªçŸ¥')}\n")
            f.write(f"- å‘é‡åŒ–èŠ‚ç‚¹æ•°: {vector_info.get('vectorized_nodes', 0)}\n")
            f.write(f"- å­˜å‚¨å¤§å°: {vector_info.get('storage_size_mb', 0):.2f} MB\n")
            f.write(f"- å‘é‡åŒ–æ—¶é—´: {vector_info.get('vectorization_time', 0):.2f} ç§’\n")
            
            # ç¼“å­˜ä¿¡æ¯
            cache_info = vector_info.get('cache_info', {})
            if cache_info:
                f.write(f"\nğŸ’¾ ç¼“å­˜ä¿¡æ¯:\n")
                f.write(f"- ç¼“å­˜æ¡ç›®æ•°: {cache_info.get('total_entries', 0)}\n")
                f.write(f"- ç¼“å­˜å¤§å°: {cache_info.get('estimated_size_mb', 0):.2f} MB\n")
                f.write(f"- ç¼“å­˜å‘½ä¸­ç‡: {cache_info.get('hit_rate', 0):.1%}\n")
            
            # æ˜¾ç¤ºåˆ†å—è¯¦æƒ…ï¼ˆå‰10ä¸ªï¼‰
            f.write(f"\nğŸ“ åˆ†å—è¯¦ç»†ä¿¡æ¯ (å‰10ä¸ª):\n")
            f.write("-" * 60 + "\n")
            
            for i, chunk in enumerate(chunks[:10], 1):
                if hasattr(chunk, 'text'):
                    # TextNodeå¯¹è±¡ - éœ€è¦è§£æJSONå­—ç¬¦ä¸²
                    concepts_data = chunk.metadata.get('concepts', "[]")
                    if isinstance(concepts_data, str):
                        try:
                            concepts = json.loads(concepts_data)
                        except json.JSONDecodeError:
                            concepts = []
                    else:
                        concepts = concepts_data if isinstance(concepts_data, list) else []
                    text_preview = chunk.text[:100]
                    node_id = chunk.node_id
                    text_length = len(chunk.text)
                else:
                    # å­—å…¸å¯¹è±¡ï¼ˆå‘åå…¼å®¹ï¼‰
                    concepts = chunk.get('concepts', [])
                    if not isinstance(concepts, list):
                        concepts = []
                    text_preview = chunk.get('text', '')[:100]
                    node_id = chunk.get('node_id', 'æœªçŸ¥')
                    text_length = chunk.get('text_length', 0)
                
                f.write(f"\nåˆ†å— {i}:\n")
                f.write(f"  ID: {node_id}\n")
                f.write(f"  é•¿åº¦: {text_length} å­—ç¬¦\n")
                f.write(f"  æ¦‚å¿µæ•°: {len(concepts)}\n")
                f.write(f"  æ¦‚å¿µ: {concepts}\n")
                f.write(f"  å†…å®¹é¢„è§ˆ: {text_preview}...\n")
                f.write("-" * 40 + "\n")
            
            if len(chunks) > 10:
                f.write(f"\n... è¿˜æœ‰ {len(chunks) - 10} ä¸ªåˆ†å—\n")
            
            # æ˜¾ç¤ºæ‰€æœ‰æ¦‚å¿µ
            all_concepts = set()
            for chunk in chunks:
                if hasattr(chunk, 'metadata'):
                    # TextNodeå¯¹è±¡ - éœ€è¦è§£æJSONå­—ç¬¦ä¸²
                    concepts_data = chunk.metadata.get('concepts', "[]")
                    if isinstance(concepts_data, str):
                        try:
                            concepts = json.loads(concepts_data)
                        except json.JSONDecodeError:
                            concepts = []
                    else:
                        concepts = concepts_data if isinstance(concepts_data, list) else []
                    all_concepts.update(concepts)
                else:
                    # å­—å…¸å¯¹è±¡
                    concepts = chunk.get('concepts', [])
                    if isinstance(concepts, list):
                        all_concepts.update(concepts)
            
            f.write(f"\nğŸ§  æå–çš„æ¦‚å¿µåˆ—è¡¨ ({len(all_concepts)} ä¸ª):\n")
            f.write("-" * 60 + "\n")
            for i, concept in enumerate(sorted(all_concepts), 1):
                f.write(f"{i:3d}. {concept}\n")
            
            # æ–‡æœ¬å†…å®¹é¢„è§ˆ
            f.write(f"\nğŸ“„ æ–‡æœ¬å†…å®¹é¢„è§ˆ (å‰500å­—ç¬¦):\n")
            f.write("-" * 50 + "\n")
            f.write(text_content[:500])
            f.write("\n" + "-" * 50 + "\n")
            
            if len(text_content) > 500:
                f.write(f"\nğŸ“„ æ–‡æœ¬å†…å®¹é¢„è§ˆ (å500å­—ç¬¦):\n")
                f.write("-" * 50 + "\n")
                f.write(text_content[-500:])
                f.write("\n" + "-" * 50 + "\n")
                
        else:
            f.write(f"\nâŒ é”™è¯¯ä¿¡æ¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}\n")
        
        # åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ æœºå™¨å¯è¯»çš„æ•°æ®
        try:
            json_result = convert_result_to_serializable(result)
            
            f.write(f"\n" + "="*80 + "\n")
            f.write("# æœºå™¨å¯è¯»æ•°æ® (è¯·å‹¿æ‰‹åŠ¨ä¿®æ”¹)\n")
            f.write("# JSON_DATA_START\n")
            f.write(json.dumps(json_result, ensure_ascii=False, indent=2))
            f.write("\n# JSON_DATA_END\n")
            
        except Exception as json_error:
            f.write(f"\nâš ï¸ JSONåºåˆ—åŒ–å¤±è´¥: {json_error}\n")

def load_result_from_txt(input_file):
    """ä»txtæ–‡ä»¶ä¸­åŠ è½½ç»“æœæ•°æ®"""
    with open(input_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    start_marker = "# JSON_DATA_START\n"
    end_marker = "\n# JSON_DATA_END"
    
    start_idx = content.find(start_marker)
    end_idx = content.find(end_marker)
    
    if start_idx == -1 or end_idx == -1:
        raise ValueError("æ— æ³•ä»txtæ–‡ä»¶ä¸­è§£ææ•°æ®")
    
    json_str = content[start_idx + len(start_marker):end_idx]
    return json.loads(json_str)

def process_file_with_vectorization(input_file: str, config_path: str = "config.yml") -> Dict[str, Any]:
    """
    å¤„ç†æ–‡ä»¶å¹¶è¿›è¡Œå‘é‡åŒ–å­˜å‚¨
    
    Args:
        input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        Dict[str, Any]: å¤„ç†ç»“æœ
    """
    start_time = time.time()
    
    try:
        # 1. åŠ è½½é…ç½®
        print("ğŸ“‹ åŠ è½½é…ç½®...")
        config = load_config_from_yaml(config_path)
        
        # 2. æ–‡ä»¶åŠ è½½
        print(f"ğŸ“„ åŠ è½½æ–‡ä»¶: {input_file}")
        text_content = FileProcessor.extract_text(input_file)
        
        # åˆ›å»ºDocumentå¯¹è±¡
        input_path = Path(input_file)
        document = Document(
            text=text_content,
            metadata={
                "file_name": input_path.name,
                "file_path": str(input_path),
                "file_type": input_path.suffix.lower(),
                "file_size": input_path.stat().st_size,
                "text_length": len(text_content),
                "processing_timestamp": datetime.now().isoformat(),
                "vectorized": True  # æ ‡è®°ä¸ºå·²å‘é‡åŒ–
            }
        )
        
        print(f"âœ… æ–‡ä»¶åŠ è½½æˆåŠŸ: {len(text_content):,} å­—ç¬¦")
        
        # 3. åˆå§‹åŒ–å‘é‡åŒ–ç»„ä»¶
        print("ğŸ”§ åˆå§‹åŒ–å‘é‡åŒ–ç»„ä»¶...")
        chunker = SemanticChunker(config)
        vector_manager = VectorStoreManager(config)
        
        # 4. è¯­ä¹‰åˆ†å—å’Œæ¦‚å¿µæå–
        print("âœ‚ï¸ æ‰§è¡Œè¯­ä¹‰åˆ†å—å’Œæ¦‚å¿µæå–...")
        chunk_start_time = time.time()
        
        chunk_nodes = chunker.chunk_and_extract_concepts([document])
        
        chunk_time = time.time() - chunk_start_time
        print(f"âœ… åˆ†å—å®Œæˆ: {len(chunk_nodes)} ä¸ªchunksï¼Œè€—æ—¶ {chunk_time:.2f} ç§’")
        
        # 5. å‘é‡åŒ–å­˜å‚¨
        print("ğŸ—ƒï¸  å¼€å§‹å‘é‡åŒ–å­˜å‚¨...")
        vector_start_time = time.time()
        
        # åˆ›å»ºå‘é‡ç´¢å¼•å¹¶å­˜å‚¨åˆ°Chroma
        chunk_index = vector_manager.create_chunk_index(chunk_nodes, persist=True)
        
        vector_time = time.time() - vector_start_time
        print(f"âœ… å‘é‡åŒ–å­˜å‚¨å®Œæˆï¼Œè€—æ—¶ {vector_time:.2f} ç§’")
        
        # 6. æ”¶é›†å‘é‡åŒ–ä¿¡æ¯
        index_info = vector_manager.get_index_info()
        storage_info = vector_manager.get_storage_size()
        
        # è·å–ç¼“å­˜ä¿¡æ¯
        cache_info = {}
        if chunker.embedding_cache:
            cache_stats = chunker.embedding_cache.get_cache_stats()
            cache_info = cache_stats
        
        vector_info = {
            "store_type": index_info['store_type'],
            "persist_directory": index_info['persist_directory'],
            "collection_name": index_info['collection_name'],
            "dimension": index_info['dimension'],
            "vectorized_nodes": len(chunk_nodes),
            "storage_size_mb": storage_info.get('total', {}).get('size_mb', 0),
            "vectorization_time": vector_time,
            "cache_info": cache_info,
            "index_metadata": index_info['indexes'].get('chunks', {}).get('metadata', {}),
        }
        
        processing_time = time.time() - start_time
        
        # 7. æ„å»ºç»“æœ
        result = {
            "success": True,
            "step": 1,
            "step_name": "æ–‡ä»¶åŠ è½½ä¸å‘é‡åŒ–å­˜å‚¨",
            "document": document,
            "chunk_nodes": chunk_nodes,
            "vector_info": vector_info,
            "statistics": {
                "total_chunks": len(chunk_nodes),
                "total_concepts": sum(len(node.metadata.get("concepts", [])) for node in chunk_nodes),
                "unique_concepts": len(set().union(*[node.metadata.get("concepts", []) for node in chunk_nodes])),
                "avg_chunk_length": sum(len(node.text) for node in chunk_nodes) / len(chunk_nodes) if chunk_nodes else 0,
                "chunk_time": chunk_time,
                "vector_time": vector_time,
            },
            "processing_time": processing_time,
            "timestamp": datetime.now().isoformat()
        }
        
        return result
        
    except Exception as e:
        error_msg = f"æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        
        result = {
            "success": False,
            "step": 1,
            "step_name": "æ–‡ä»¶åŠ è½½ä¸å‘é‡åŒ–å­˜å‚¨",
            "error": error_msg,
            "processing_time": time.time() - start_time,
            "timestamp": datetime.now().isoformat()
        }
        
        return result

def main():
    if len(sys.argv) != 2:
        print("ç”¨æ³•: python step1.py <è¾“å…¥æ–‡ä»¶>")
        print("ç¤ºä¾‹: python step1.py 'attention is all you need.pdf'")
        print("\næ–°åŠŸèƒ½:")
        print("âœ… è‡ªåŠ¨åˆ›å»ºå®éªŒæ–‡ä»¶å¤¹ï¼ˆåŸºäºæ—¶é—´æˆ³ï¼‰")
        print("âœ… ç»Ÿä¸€çš„æ–‡ä»¶è¾“å‡ºç®¡ç†")
        print("âœ… æ”¯æŒå¤šç§æ ¼å¼ï¼ˆtxt, json, mdï¼‰")
        print("âœ… å®éªŒå…ƒæ•°æ®è‡ªåŠ¨è®°å½•")
        sys.exit(1)
    
    input_file = sys.argv[1]
    
    print(f"ğŸš€ æ­¥éª¤1: æ–‡ä»¶åŠ è½½ + å‘é‡åŒ–å­˜å‚¨ (æ–°ç‰ˆ)")
    print(f"ğŸ“„ è¾“å…¥æ–‡ä»¶: {input_file}")
    print("="*60)
    
    try:
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not Path(input_file).exists():
            print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
            sys.exit(1)
        
        # ğŸ†• åˆ›å»ºå®éªŒç®¡ç†å™¨
        print("ğŸ§ª åˆ›å»ºå®éªŒç¯å¢ƒ...")
        experiment_manager = create_experiment_manager(input_file)
        
        print(f"âœ… å®éªŒç¯å¢ƒåˆ›å»ºå®Œæˆ:")
        print(f"   å®éªŒID: {experiment_manager.experiment_name}")
        print(f"   å®éªŒç›®å½•: {experiment_manager.experiment_dir}")
        print()
        
        # å¤„ç†æ–‡ä»¶å¹¶å‘é‡åŒ–
        print("ğŸ”„ å¼€å§‹å¤„ç†æ–‡ä»¶...")
        result = process_file_with_vectorization(input_file)
        
        # ğŸ†• ä½¿ç”¨å®éªŒç®¡ç†å™¨ä¿å­˜ç»“æœ
        print("ğŸ’¾ ä¿å­˜å®éªŒç»“æœ...")
        saved_files = experiment_manager.save_step_result(
            step_num=1,
            result=result,
            save_formats=['txt', 'json']  # å¯ä»¥é€‰æ‹©ä¿å­˜çš„æ ¼å¼
        )
        
        if result.get("success"):
            print(f"\nâœ… æ­¥éª¤1å®Œæˆ!")
            print(f"ğŸ“ å®éªŒç›®å½•: {experiment_manager.experiment_dir}")
            print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶:")
            for format_type, file_path in saved_files.items():
                print(f"   - {format_type.upper()}: {file_path}")
            
            # æ˜¾ç¤ºç®€è¦ç»Ÿè®¡
            stats = result.get("statistics", {})
            vector_info = result.get("vector_info", {})
            
            print(f"\nğŸ“Š å¤„ç†ç»“æœæ‘˜è¦:")
            print(f"   - æ–‡æ¡£åˆ†å—æ•°: {stats.get('total_chunks', 0)}")
            print(f"   - æå–æ¦‚å¿µæ•°: {stats.get('total_concepts', 0)} (å”¯ä¸€: {stats.get('unique_concepts', 0)})")
            print(f"   - å‘é‡åŒ–èŠ‚ç‚¹æ•°: {vector_info.get('vectorized_nodes', 0)}")
            print(f"   - å­˜å‚¨å¤§å°: {vector_info.get('storage_size_mb', 0):.2f} MB")
            print(f"   - æ€»å¤„ç†æ—¶é—´: {result.get('processing_time', 0):.2f} ç§’")
            print(f"   - å‘é‡æ•°æ®åº“: {vector_info.get('store_type', 'æœªçŸ¥')} @ {vector_info.get('persist_directory', 'æœªçŸ¥')}")
            
            # æ˜¾ç¤ºç¼“å­˜ä¿¡æ¯
            cache_info = vector_info.get('cache_info', {})
            if cache_info:
                print(f"   - ç¼“å­˜çŠ¶æ€: {cache_info.get('total_entries', 0)} æ¡è®°å½•, {cache_info.get('estimated_size_mb', 0):.1f} MB")
            
            # ğŸ†• æ˜¾ç¤ºå®éªŒä¿¡æ¯
            summary = experiment_manager.get_experiment_summary()
            print(f"\nğŸ§ª å®éªŒä¿¡æ¯:")
            print(f"   - å®éªŒID: {summary['experiment_id']}")
            print(f"   - å·²å®Œæˆæ­¥éª¤: {summary['steps_completed']}/{summary['total_steps']}")
            print(f"   - å®éªŒçŠ¶æ€: {summary['status']}")
            
            # ğŸ†• æç¤ºåç»­æ­¥éª¤
            print(f"\nğŸ“‹ åç»­æ­¥éª¤:")
            print(f"   è¿è¡Œä¸‹ä¸€æ­¥: python step2.py {saved_files['txt']}")
            print(f"   æŸ¥çœ‹ç»“æœ: cat {saved_files['txt']}")
            print(f"   å®éªŒç›®å½•: ls {experiment_manager.experiment_dir}")
                
        else:
            print(f"âŒ æ­¥éª¤1å¤±è´¥: {result.get('error')}")
            
            # å³ä½¿å¤±è´¥ä¹Ÿä¿å­˜é”™è¯¯ä¿¡æ¯
            experiment_manager.save_step_result(
                step_num=1,
                result=result,
                save_formats=['txt']
            )
            sys.exit(1)
            
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        # ğŸ†• ä¿å­˜é”™è¯¯ä¿¡æ¯åˆ°å®éªŒç›®å½•
        if 'experiment_manager' in locals():
            error_result = {
                "step": 1,
                "step_name": "æ–‡ä»¶åŠ è½½ä¸å‘é‡åŒ–å­˜å‚¨",
                "success": False,
                "error": str(e),
                "traceback": traceback.format_exc(),
                "processing_time": 0.0,
                "timestamp": datetime.now().isoformat()
            }
            
            try:
                experiment_manager.save_step_result(1, error_result, ['txt'])
                print(f"ğŸ“„ é”™è¯¯ä¿¡æ¯å·²ä¿å­˜åˆ°å®éªŒç›®å½•: {experiment_manager.experiment_dir}")
            except:
                pass
        
        sys.exit(1)

if __name__ == "__main__":
    main()