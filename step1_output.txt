================================================================================
步骤1: 文件加载 + 向量化存储 - 处理结果
================================================================================
生成时间: 2025-06-05 20:25:02
处理状态: ✅ 成功

📊 文档基本信息:
- 文件名: attention is all you need.pdf
- 文件类型: .pdf
- 文件大小: 2163.32 KB
- 文本长度: 39,483 字符
- 处理时间: 79.68 秒

📄 文档分块信息:
- 总分块数: 21
- 平均分块长度: 1910.0 字符
- 最短分块: 124 字符
- 最长分块: 8151 字符
- 总概念数: 105
- 唯一概念数: 79
- 平均每分块概念数: 5.0

🗃️  向量化存储信息:
- 向量数据库类型: chroma
- 存储目录: ./vector_db
- 集合名称: concept_pipeline
- 向量维度: 1536
- 向量化节点数: 21
- 存储大小: 0.00 MB
- 向量化时间: 5.88 秒

💾 缓存信息:
- 缓存条目数: 0
- 缓存大小: 0.00 MB
- 缓存命中率: 0.0%

📝 分块详细信息 (前10个):
------------------------------------------------------------

分块 1:
  ID: f5b4ecc9-c113-429f-be4a-ff7e3a8bd3c7
  长度: 1758 字符
  概念数: 5
  概念: ['google', 'all', 'requiring', 'eight', 'reproduce']
  内容预览: Provided proper attribution is provided, Google hereby grants permission to
reproduce the tables and...
----------------------------------------

分块 2:
  ID: 8c62aecc-c04e-4580-a240-cbebd15b20e4
  长度: 924 字符
  概念数: 5
  概念: ['attention', 'greatly', 'google', 'massively', 'listing']
  内容预览: Listing order is random. Jakob proposed replacing RNNs with self-attention and started
the effort to...
----------------------------------------

分块 3:
  ID: 7c90436f-2185-47fb-a94e-25dfab524e6c
  长度: 603 字符
  概念数: 5
  概念: ['google', 'modeling', 'efforts', 'boundaries', 'since']
  内容预览: ‡Work performed while at Google Research.
31st Conference on Neural Information Processing Systems (...
----------------------------------------

分块 4:
  ID: f6847ef6-8242-4a0b-bfe4-a9f0aad0c98a
  长度: 8151 字符
  概念数: 5
  概念: ['matrices', 'yielding', 'which', 'allows', 'right']
  内容预览: Recurrent models typically factor computation along the symbol positions of the input and output
seq...
----------------------------------------

分块 5:
  ID: 71c81327-f67d-4d44-81b2-f5456b002d24
  长度: 2519 字符
  概念数: 5
  概念: ['matrices', 'which', 'allows', 'memory', 'all']
  内容预览: These are concatenated and once again projected, resulting in the final values, as
depicted in Figur...
----------------------------------------

分块 6:
  ID: 7bf4c9c2-29ef-4728-aa4b-1de673157191
  长度: 5456 字符
  概念数: 5
  概念: ['vectors', 'geometric', 'bottoms', 'third', 'pairs']
  内容预览: The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality
dff=...
----------------------------------------

分块 7:
  ID: d9379558-e957-46cb-b759-be886ffe30db
  长度: 2852 字符
  概念数: 5
  概念: ['describes', 'line', 'types', 'pairs', 'which']
  内容预览: We inspect attention distributions
from our models and present and discuss examples in the appendix....
----------------------------------------

分块 8:
  ID: 5d122527-c1a9-46ad-9cc1-7ccd0e5e2640
  长度: 598 字符
  概念数: 5
  概念: ['outperforms', 'during', 'line', 'than', 'employed']
  内容预览: For the base model, we use a rate of
Pdrop= 0.1.
Label Smoothing During training, we employed label ...
----------------------------------------

分块 9:
  ID: fc0d7be7-fec2-4215-a5fd-02b8f0bd75b0
  长度: 1746 字符
  概念数: 5
  概念: ['which', 'rate', 'all', 'importance', 'published']
  内容预览: Training took 3.5days on 8P100 GPUs. Even our base model
surpasses all previously published models a...
----------------------------------------

分块 10:
  ID: 9a49211d-4b7f-4494-a73f-9423a87caaba
  长度: 212 字符
  概念数: 5
  概念: ['not', 'translation', 'listed', 'according', 'our']
  内容预览: All metrics are on the English-to-German translation development set, newstest2013. Listed
perplexit...
----------------------------------------

... 还有 11 个分块

🧠 提取的概念列表 (79 个):
------------------------------------------------------------
  1. according
  2. active
  3. advances
  4. alexander
  5. all
  6. allows
  7. architecture
  8. arthur
  9. arxiv
 10. attention
 11. beatrice
 12. bottoms
 13. boundaries
 14. caglar
 15. deep
 16. denis
 17. dependencies
 18. depthwise
 19. describes
 20. during
 21. efficiently
 22. efforts
 23. eight
 24. employed
 25. full
 26. gap
 27. geometric
 28. google
 29. gradient
 30. greatly
 31. here
 32. hochreiter
 33. importance
 34. investigate
 35. jimmy
 36. krzysztof
 37. line
 38. listed
 39. listing
 40. massively
 41. matrices
 42. memory
 43. modeling
 44. mohammad
 45. noah
 46. not
 47. our
 48. outperforms
 49. pairs
 50. penn
 51. positional
 52. preprint
 53. proceedings
 54. process
 55. published
 56. rate
 57. reproduce
 58. requiring
 59. right
 60. rnn
 61. separable
 62. since
 63. structured
 64. than
 65. third
 66. translation
 67. types
 68. unlisted
 69. van
 70. vectors
 71. wenliang
 72. which
 73. words
 74. yielding
 75. yoon
 76. yoshua
 77. zbigniew
 78. łukasz
 79. ϵlstrain

📄 文本内容预览 (前500字符):
--------------------------------------------------
Provided proper attribution is provided, Google hereby grants permission to
reproduce the tables and figures in this paper solely for use in journalistic or
scholarly works.
Attention Is All You Need
Ashish Vaswani∗
Google Brain
avaswani@google.comNoam Shazeer∗
Google Brain
noam@google.comNiki Parmar∗
Google Research
nikip@google.comJakob Uszkoreit∗
Google Research
usz@google.com
Llion Jones∗
Google Research
llion@google.comAidan N. Gomez∗ †
University of Toronto
aidan@cs.toronto.eduŁukasz Kaise
--------------------------------------------------

📄 文本内容预览 (后500字符):
--------------------------------------------------
fect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the
sentence. We give two such examples above, from two different heads from the encoder self-attention
at layer 5 of 6. The heads clearly learned to perform different tasks.
15
--------------------------------------------------

⚠️ JSON序列化失败: 'dict' object has no attribute 'metadata'
