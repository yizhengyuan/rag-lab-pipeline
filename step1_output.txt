================================================================================
æ­¥éª¤1: æ–‡ä»¶åŠ è½½ + å‘é‡åŒ–å­˜å‚¨ - å¤„ç†ç»“æœ
================================================================================
ç”Ÿæˆæ—¶é—´: 2025-06-05 20:25:02
å¤„ç†çŠ¶æ€: âœ… æˆåŠŸ

ğŸ“Š æ–‡æ¡£åŸºæœ¬ä¿¡æ¯:
- æ–‡ä»¶å: attention is all you need.pdf
- æ–‡ä»¶ç±»å‹: .pdf
- æ–‡ä»¶å¤§å°: 2163.32 KB
- æ–‡æœ¬é•¿åº¦: 39,483 å­—ç¬¦
- å¤„ç†æ—¶é—´: 79.68 ç§’

ğŸ“„ æ–‡æ¡£åˆ†å—ä¿¡æ¯:
- æ€»åˆ†å—æ•°: 21
- å¹³å‡åˆ†å—é•¿åº¦: 1910.0 å­—ç¬¦
- æœ€çŸ­åˆ†å—: 124 å­—ç¬¦
- æœ€é•¿åˆ†å—: 8151 å­—ç¬¦
- æ€»æ¦‚å¿µæ•°: 105
- å”¯ä¸€æ¦‚å¿µæ•°: 79
- å¹³å‡æ¯åˆ†å—æ¦‚å¿µæ•°: 5.0

ğŸ—ƒï¸  å‘é‡åŒ–å­˜å‚¨ä¿¡æ¯:
- å‘é‡æ•°æ®åº“ç±»å‹: chroma
- å­˜å‚¨ç›®å½•: ./vector_db
- é›†åˆåç§°: concept_pipeline
- å‘é‡ç»´åº¦: 1536
- å‘é‡åŒ–èŠ‚ç‚¹æ•°: 21
- å­˜å‚¨å¤§å°: 0.00 MB
- å‘é‡åŒ–æ—¶é—´: 5.88 ç§’

ğŸ’¾ ç¼“å­˜ä¿¡æ¯:
- ç¼“å­˜æ¡ç›®æ•°: 0
- ç¼“å­˜å¤§å°: 0.00 MB
- ç¼“å­˜å‘½ä¸­ç‡: 0.0%

ğŸ“ åˆ†å—è¯¦ç»†ä¿¡æ¯ (å‰10ä¸ª):
------------------------------------------------------------

åˆ†å— 1:
  ID: f5b4ecc9-c113-429f-be4a-ff7e3a8bd3c7
  é•¿åº¦: 1758 å­—ç¬¦
  æ¦‚å¿µæ•°: 5
  æ¦‚å¿µ: ['google', 'all', 'requiring', 'eight', 'reproduce']
  å†…å®¹é¢„è§ˆ: Provided proper attribution is provided, Google hereby grants permission to
reproduce the tables and...
----------------------------------------

åˆ†å— 2:
  ID: 8c62aecc-c04e-4580-a240-cbebd15b20e4
  é•¿åº¦: 924 å­—ç¬¦
  æ¦‚å¿µæ•°: 5
  æ¦‚å¿µ: ['attention', 'greatly', 'google', 'massively', 'listing']
  å†…å®¹é¢„è§ˆ: Listing order is random. Jakob proposed replacing RNNs with self-attention and started
the effort to...
----------------------------------------

åˆ†å— 3:
  ID: 7c90436f-2185-47fb-a94e-25dfab524e6c
  é•¿åº¦: 603 å­—ç¬¦
  æ¦‚å¿µæ•°: 5
  æ¦‚å¿µ: ['google', 'modeling', 'efforts', 'boundaries', 'since']
  å†…å®¹é¢„è§ˆ: â€¡Work performed while at Google Research.
31st Conference on Neural Information Processing Systems (...
----------------------------------------

åˆ†å— 4:
  ID: f6847ef6-8242-4a0b-bfe4-a9f0aad0c98a
  é•¿åº¦: 8151 å­—ç¬¦
  æ¦‚å¿µæ•°: 5
  æ¦‚å¿µ: ['matrices', 'yielding', 'which', 'allows', 'right']
  å†…å®¹é¢„è§ˆ: Recurrent models typically factor computation along the symbol positions of the input and output
seq...
----------------------------------------

åˆ†å— 5:
  ID: 71c81327-f67d-4d44-81b2-f5456b002d24
  é•¿åº¦: 2519 å­—ç¬¦
  æ¦‚å¿µæ•°: 5
  æ¦‚å¿µ: ['matrices', 'which', 'allows', 'memory', 'all']
  å†…å®¹é¢„è§ˆ: These are concatenated and once again projected, resulting in the final values, as
depicted in Figur...
----------------------------------------

åˆ†å— 6:
  ID: 7bf4c9c2-29ef-4728-aa4b-1de673157191
  é•¿åº¦: 5456 å­—ç¬¦
  æ¦‚å¿µæ•°: 5
  æ¦‚å¿µ: ['vectors', 'geometric', 'bottoms', 'third', 'pairs']
  å†…å®¹é¢„è§ˆ: The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality
dff=...
----------------------------------------

åˆ†å— 7:
  ID: d9379558-e957-46cb-b759-be886ffe30db
  é•¿åº¦: 2852 å­—ç¬¦
  æ¦‚å¿µæ•°: 5
  æ¦‚å¿µ: ['describes', 'line', 'types', 'pairs', 'which']
  å†…å®¹é¢„è§ˆ: We inspect attention distributions
from our models and present and discuss examples in the appendix....
----------------------------------------

åˆ†å— 8:
  ID: 5d122527-c1a9-46ad-9cc1-7ccd0e5e2640
  é•¿åº¦: 598 å­—ç¬¦
  æ¦‚å¿µæ•°: 5
  æ¦‚å¿µ: ['outperforms', 'during', 'line', 'than', 'employed']
  å†…å®¹é¢„è§ˆ: For the base model, we use a rate of
Pdrop= 0.1.
Label Smoothing During training, we employed label ...
----------------------------------------

åˆ†å— 9:
  ID: fc0d7be7-fec2-4215-a5fd-02b8f0bd75b0
  é•¿åº¦: 1746 å­—ç¬¦
  æ¦‚å¿µæ•°: 5
  æ¦‚å¿µ: ['which', 'rate', 'all', 'importance', 'published']
  å†…å®¹é¢„è§ˆ: Training took 3.5days on 8P100 GPUs. Even our base model
surpasses all previously published models a...
----------------------------------------

åˆ†å— 10:
  ID: 9a49211d-4b7f-4494-a73f-9423a87caaba
  é•¿åº¦: 212 å­—ç¬¦
  æ¦‚å¿µæ•°: 5
  æ¦‚å¿µ: ['not', 'translation', 'listed', 'according', 'our']
  å†…å®¹é¢„è§ˆ: All metrics are on the English-to-German translation development set, newstest2013. Listed
perplexit...
----------------------------------------

... è¿˜æœ‰ 11 ä¸ªåˆ†å—

ğŸ§  æå–çš„æ¦‚å¿µåˆ—è¡¨ (79 ä¸ª):
------------------------------------------------------------
  1. according
  2. active
  3. advances
  4. alexander
  5. all
  6. allows
  7. architecture
  8. arthur
  9. arxiv
 10. attention
 11. beatrice
 12. bottoms
 13. boundaries
 14. caglar
 15. deep
 16. denis
 17. dependencies
 18. depthwise
 19. describes
 20. during
 21. efficiently
 22. efforts
 23. eight
 24. employed
 25. full
 26. gap
 27. geometric
 28. google
 29. gradient
 30. greatly
 31. here
 32. hochreiter
 33. importance
 34. investigate
 35. jimmy
 36. krzysztof
 37. line
 38. listed
 39. listing
 40. massively
 41. matrices
 42. memory
 43. modeling
 44. mohammad
 45. noah
 46. not
 47. our
 48. outperforms
 49. pairs
 50. penn
 51. positional
 52. preprint
 53. proceedings
 54. process
 55. published
 56. rate
 57. reproduce
 58. requiring
 59. right
 60. rnn
 61. separable
 62. since
 63. structured
 64. than
 65. third
 66. translation
 67. types
 68. unlisted
 69. van
 70. vectors
 71. wenliang
 72. which
 73. words
 74. yielding
 75. yoon
 76. yoshua
 77. zbigniew
 78. Å‚ukasz
 79. Ïµlstrain

ğŸ“„ æ–‡æœ¬å†…å®¹é¢„è§ˆ (å‰500å­—ç¬¦):
--------------------------------------------------
Provided proper attribution is provided, Google hereby grants permission to
reproduce the tables and figures in this paper solely for use in journalistic or
scholarly works.
Attention Is All You Need
Ashish Vaswaniâˆ—
Google Brain
avaswani@google.comNoam Shazeerâˆ—
Google Brain
noam@google.comNiki Parmarâˆ—
Google Research
nikip@google.comJakob Uszkoreitâˆ—
Google Research
usz@google.com
Llion Jonesâˆ—
Google Research
llion@google.comAidan N. Gomezâˆ— â€ 
University of Toronto
aidan@cs.toronto.eduÅukasz Kaise
--------------------------------------------------

ğŸ“„ æ–‡æœ¬å†…å®¹é¢„è§ˆ (å500å­—ç¬¦):
--------------------------------------------------
fect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the
sentence. We give two such examples above, from two different heads from the encoder self-attention
at layer 5 of 6. The heads clearly learned to perform different tasks.
15
--------------------------------------------------

âš ï¸ JSONåºåˆ—åŒ–å¤±è´¥: 'dict' object has no attribute 'metadata'
