#!/usr/bin/env python3
"""
æ­¥éª¤8: æœ€ç»ˆæ±‡æ€»ä¸æŠ¥å‘Š - å¢å¼ºç‰ˆ
================================

åŠŸèƒ½ï¼š
1. ä»æ­¥éª¤7çš„ç»“æœæˆ–å®éªŒæ–‡ä»¶å¤¹ä¸­è¯»å–æ‰€æœ‰æ­¥éª¤ç»“æœ
2. ç”Ÿæˆå®Œæ•´çš„æµæ°´çº¿æ‰§è¡Œæ±‡æ€»æŠ¥å‘Š
3. åˆ†æå„æ­¥éª¤æ€§èƒ½å’ŒæˆåŠŸç‡
4. ä¿å­˜åˆ°åŒä¸€ä¸ªå®éªŒæ–‡ä»¶å¤¹

ç”¨æ³•: 
- python step8.py <step7è¾“å‡ºæ–‡ä»¶.txt>
- python step8.py <å®éªŒæ–‡ä»¶å¤¹è·¯å¾„>

æ–°åŠŸèƒ½ï¼š
- âœ… è‡ªåŠ¨è¯†åˆ«å¹¶ä½¿ç”¨step7çš„å®éªŒæ–‡ä»¶å¤¹
- âœ… å®Œæ•´çš„æµæ°´çº¿æ‰§è¡ŒæŠ¥å‘Š
- âœ… è¯¦ç»†çš„æ€§èƒ½åˆ†æå’Œç»Ÿè®¡
- âœ… ç»Ÿä¸€çš„å®éªŒç®¡ç†
- âœ… æ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼
"""

import sys
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List
from collections import defaultdict
import logging

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
sys.path.append(str(Path(__file__).parent))

# å¯¼å…¥æ ¸å¿ƒå¤„ç†æ¨¡å—
from config.settings import load_config_from_yaml

# å¯¼å…¥å®éªŒç®¡ç†å™¨
from utils.experiment_manager import ExperimentManager
from utils.helpers import FileHelper

logger = logging.getLogger(__name__)

def load_step7_result(step7_file_or_dir: str) -> tuple:
    """
    ä»æ­¥éª¤7çš„è¾“å‡ºæ–‡ä»¶æˆ–å®éªŒæ–‡ä»¶å¤¹ä¸­åŠ è½½ç»“æœ
    
    Args:
        step7_file_or_dir: æ­¥éª¤7çš„è¾“å‡ºæ–‡ä»¶æˆ–å®éªŒæ–‡ä»¶å¤¹è·¯å¾„
        
    Returns:
        tuple: (step7_result, experiment_manager)
    """
    step7_path = Path(step7_file_or_dir)
    
    if step7_path.is_file():
        # æƒ…å†µ1ï¼šç›´æ¥æŒ‡å®šäº†step7çš„è¾“å‡ºæ–‡ä»¶
        if step7_path.name.startswith("step7") and step7_path.suffix == ".txt":
            experiment_dir = step7_path.parent
            experiment_manager = ExperimentManager.load_experiment(str(experiment_dir))
            
            # ä»txtæ–‡ä»¶åŠ è½½ç»“æœ
            step7_result = load_result_from_txt(str(step7_path))
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {step7_path}")
            
    elif step7_path.is_dir():
        # æƒ…å†µ2ï¼šç›´æ¥æŒ‡å®šäº†å®éªŒæ–‡ä»¶å¤¹
        experiment_manager = ExperimentManager.load_experiment(str(step7_path))
        
        # æŸ¥æ‰¾step7çš„è¾“å‡ºæ–‡ä»¶
        step7_txt_path = experiment_manager.get_step_output_path(7, "txt")
        if not step7_txt_path.exists():
            raise FileNotFoundError(f"å®éªŒæ–‡ä»¶å¤¹ä¸­æ‰¾ä¸åˆ°step7è¾“å‡ºæ–‡ä»¶: {step7_txt_path}")
        
        step7_result = load_result_from_txt(str(step7_txt_path))
        
    else:
        raise FileNotFoundError(f"è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {step7_path}")
    
    return step7_result, experiment_manager

def load_result_from_txt(input_file: str) -> Dict[str, Any]:
    """ä»txtæ–‡ä»¶ä¸­åŠ è½½ç»“æœæ•°æ®"""
    with open(input_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    start_marker = "# JSON_DATA_START\n"
    end_marker = "\n# JSON_DATA_END"
    
    start_idx = content.find(start_marker)
    end_idx = content.find(end_marker)
    
    if start_idx == -1 or end_idx == -1:
        raise ValueError("æ— æ³•ä»txtæ–‡ä»¶ä¸­è§£ææ•°æ®")
    
    json_str = content[start_idx + len(start_marker):end_idx]
    return json.loads(json_str)

def load_all_steps_data(experiment_manager: ExperimentManager) -> Dict[str, Any]:
    """
    åŠ è½½æ‰€æœ‰æ­¥éª¤çš„æ•°æ®
    
    Args:
        experiment_manager: å®éªŒç®¡ç†å™¨
        
    Returns:
        Dict[str, Any]: åŒ…å«æ‰€æœ‰æ­¥éª¤æ•°æ®çš„å­—å…¸
    """
    print("ğŸ“‚ åŠ è½½æ‰€æœ‰æ­¥éª¤çš„æ•°æ®...")
    
    all_steps_data = {}
    step_timings = {}
    
    # Stepä¿¡æ¯æ˜ å°„
    step_info = {
        1: {"name": "æ–‡æ¡£åŠ è½½ä¸å‘é‡åŒ–å­˜å‚¨", "file_prefix": "step1"},
        2: {"name": "æ–‡æ¡£åˆ†å—ä¸æ¦‚å¿µæå–", "file_prefix": "step2"},
        3: {"name": "æ¦‚å¿µæå–ä¸æ˜ å°„", "file_prefix": "step3"},
        4: {"name": "æ¦‚å¿µåˆå¹¶ä¸ä¼˜åŒ–", "file_prefix": "step4"},
        5: {"name": "æ¦‚å¿µæ£€ç´¢ä¸æ˜ å°„", "file_prefix": "step5"},
        6: {"name": "è¯æ®æå–ä¸è´¨é‡è¯„ä¼°", "file_prefix": "step6"},
        7: {"name": "é—®ç­”ç”Ÿæˆ", "file_prefix": "step7"}
    }
    
    for step_num, step_info_item in step_info.items():
        try:
            step_txt_path = experiment_manager.get_step_output_path(step_num, "txt")
            
            if step_txt_path.exists():
                step_result = load_result_from_txt(str(step_txt_path))
                all_steps_data[f"step{step_num}"] = step_result
                step_timings[f"step{step_num}"] = step_result.get("processing_time", 0.0)
                print(f"   âœ… åŠ è½½æ­¥éª¤{step_num}: {step_info_item['name']}")
            else:
                print(f"   âš ï¸ æ­¥éª¤{step_num}æ–‡ä»¶ä¸å­˜åœ¨: {step_txt_path}")
                all_steps_data[f"step{step_num}"] = {"success": False, "error": "æ–‡ä»¶ä¸å­˜åœ¨"}
                step_timings[f"step{step_num}"] = 0.0
                
        except Exception as e:
            print(f"   âŒ åŠ è½½æ­¥éª¤{step_num}å¤±è´¥: {e}")
            all_steps_data[f"step{step_num}"] = {"success": False, "error": str(e)}
            step_timings[f"step{step_num}"] = 0.0
    
    print(f"   âœ… å…±åŠ è½½ {len([k for k, v in all_steps_data.items() if v.get('success')])} ä¸ªæˆåŠŸæ­¥éª¤")
    
    return {
        "step_results": all_steps_data,
        "step_timings": step_timings
    }

def analyze_pipeline_performance(step_results: Dict[str, Any], 
                               step_timings: Dict[str, Any]) -> Dict[str, Any]:
    """
    åˆ†ææµæ°´çº¿æ€§èƒ½
    
    Args:
        step_results: æ­¥éª¤ç»“æœ
        step_timings: æ­¥éª¤æ—¶é—´
        
    Returns:
        Dict[str, Any]: æ€§èƒ½åˆ†æç»“æœ
    """
    print("ğŸ“ˆ åˆ†ææµæ°´çº¿æ€§èƒ½...")
    
    # åŸºæœ¬ç»Ÿè®¡
    total_steps = len(step_results)
    successful_steps = len([r for r in step_results.values() if r.get("success")])
    failed_steps = len([r for r in step_results.values() if r.get("success") == False])
    skipped_steps = len([r for r in step_results.values() if r.get("skipped")])
    
    total_time = sum(step_timings.values())
    avg_time_per_step = total_time / total_steps if total_steps > 0 else 0
    
    # æ­¥éª¤çŠ¶æ€åˆ†æ
    step_status = {}
    for step_key, result in step_results.items():
        if result.get("success"):
            status = "success"
        elif result.get("skipped"):
            status = "skipped"
        else:
            status = "failed"
        step_status[step_key] = status
    
    # æ—¶é—´åˆ†æ
    slowest_step = max(step_timings.items(), key=lambda x: x[1]) if step_timings else ("", 0)
    fastest_step = min(step_timings.items(), key=lambda x: x[1]) if step_timings else ("", 0)
    
    # æ•°æ®æµåˆ†æ
    data_flow = extract_data_flow_metrics(step_results)
    
    return {
        "basic_stats": {
            "total_steps": total_steps,
            "successful_steps": successful_steps,
            "failed_steps": failed_steps,
            "skipped_steps": skipped_steps,
            "success_rate": successful_steps / total_steps if total_steps > 0 else 0
        },
        "timing_stats": {
            "total_time": total_time,
            "avg_time_per_step": avg_time_per_step,
            "slowest_step": slowest_step,
            "fastest_step": fastest_step
        },
        "step_status": step_status,
        "data_flow": data_flow
    }

def extract_data_flow_metrics(step_results: Dict[str, Any]) -> Dict[str, Any]:
    """
    æå–æ•°æ®æµæŒ‡æ ‡
    
    Args:
        step_results: æ­¥éª¤ç»“æœ
        
    Returns:
        Dict[str, Any]: æ•°æ®æµæŒ‡æ ‡
    """
    data_flow = {}
    
    # Step1: æ–‡æ¡£ä¿¡æ¯
    step1_result = step_results.get("step1", {})
    if step1_result.get("success"):
        stats = step1_result.get("statistics", {})
        data_flow["document_length"] = step1_result.get("document", {}).get("metadata", {}).get("text_length", 0)
        data_flow["total_chunks"] = stats.get("total_chunks", 0)
        data_flow["vector_nodes"] = step1_result.get("vector_info", {}).get("vectorized_nodes", 0)
    
    # Step2: åˆ†å—å’Œæ¦‚å¿µ
    step2_result = step_results.get("step2", {})
    if step2_result.get("success"):
        stats = step2_result.get("statistics", {})
        data_flow["processed_chunks"] = stats.get("total_chunks", 0)
        data_flow["extracted_concepts"] = stats.get("unique_concepts", 0)
    
    # Step3: æ¦‚å¿µåˆ†æ
    step3_result = step_results.get("step3", {})
    if step3_result.get("success"):
        stats = step3_result.get("statistics", {})
        data_flow["analyzed_concepts"] = stats.get("unique_concepts", 0)
        data_flow["high_quality_concepts"] = stats.get("high_quality_count", 0)
    
    # Step4: æ¦‚å¿µåˆå¹¶
    step4_result = step_results.get("step4", {})
    if step4_result.get("success"):
        stats = step4_result.get("statistics", {})
        data_flow["merged_concepts"] = stats.get("merged_concept_count", 0)
        data_flow["compression_ratio"] = step4_result.get("input_statistics", {}).get("compression_ratio", 0)
    
    # Step5: æ¦‚å¿µæ£€ç´¢
    step5_result = step_results.get("step5", {})
    if step5_result.get("success"):
        stats = step5_result.get("statistics", {})
        data_flow["total_retrievals"] = stats.get("total_retrievals", 0)
        data_flow["retrieval_coverage"] = stats.get("retrieval_coverage", 0)
    
    # Step6: è¯æ®æå–
    step6_result = step_results.get("step6", {})
    if step6_result.get("success"):
        stats = step6_result.get("statistics", {})
        data_flow["total_evidence"] = stats.get("total_evidence", 0)
        data_flow["concepts_with_evidence"] = stats.get("concepts_with_evidence", 0)
    
    # Step7: é—®ç­”ç”Ÿæˆ
    step7_result = step_results.get("step7", {})
    if step7_result.get("success") and not step7_result.get("skipped"):
        stats = step7_result.get("statistics", {})
        data_flow["total_qa_pairs"] = stats.get("total_qa_pairs", 0)
        data_flow["processed_evidences"] = stats.get("processed_evidences", 0)
    
    return data_flow

def generate_final_summary(step_results: Dict[str, Any], 
                         step_timings: Dict[str, Any],
                         performance_analysis: Dict[str, Any]) -> Dict[str, Any]:
    """
    ç”Ÿæˆæœ€ç»ˆæ±‡æ€»
    
    Args:
        step_results: æ­¥éª¤ç»“æœ
        step_timings: æ­¥éª¤æ—¶é—´
        performance_analysis: æ€§èƒ½åˆ†æ
        
    Returns:
        Dict[str, Any]: æœ€ç»ˆæ±‡æ€»ç»“æœ
    """
    print("ğŸ“Š ç”Ÿæˆæœ€ç»ˆæ±‡æ€»...")
    
    # æ­¥éª¤åç§°æ˜ å°„
    step_names = {
        "step1": "æ–‡æ¡£åŠ è½½ä¸å‘é‡åŒ–å­˜å‚¨",
        "step2": "æ–‡æ¡£åˆ†å—ä¸æ¦‚å¿µæå–", 
        "step3": "æ¦‚å¿µæå–ä¸æ˜ å°„",
        "step4": "æ¦‚å¿µåˆå¹¶ä¸ä¼˜åŒ–",
        "step5": "æ¦‚å¿µæ£€ç´¢ä¸æ˜ å°„",
        "step6": "è¯æ®æå–ä¸è´¨é‡è¯„ä¼°",
        "step7": "é—®ç­”ç”Ÿæˆ"
    }
    
    # ç”Ÿæˆæ­¥éª¤è¯¦æƒ…
    step_details = []
    for step_key in ["step1", "step2", "step3", "step4", "step5", "step6", "step7"]:
        step_result = step_results.get(step_key, {})
        step_time = step_timings.get(step_key, 0.0)
        step_name = step_names.get(step_key, f"æ­¥éª¤{step_key[-1]}")
        
        if step_result.get("success"):
            status = "âœ… æˆåŠŸ"
            status_code = "success"
        elif step_result.get("skipped"):
            status = "â­ï¸ è·³è¿‡"
            status_code = "skipped"
        else:
            status = "âŒ å¤±è´¥"
            status_code = "failed"
        
        step_details.append({
            "step_key": step_key,
            "step_name": step_name,
            "status": status,
            "status_code": status_code,
            "processing_time": step_time,
            "error": step_result.get("error", None) if not step_result.get("success") else None
        })
    
    # å…³é”®æŒ‡æ ‡æå–
    data_flow = performance_analysis["data_flow"]
    key_metrics = {
        "document_processing": {
            "input_document_length": data_flow.get("document_length", 0),
            "generated_chunks": data_flow.get("total_chunks", 0),
            "vectorized_nodes": data_flow.get("vector_nodes", 0)
        },
        "concept_processing": {
            "extracted_concepts": data_flow.get("extracted_concepts", 0),
            "high_quality_concepts": data_flow.get("high_quality_concepts", 0),
            "merged_concepts": data_flow.get("merged_concepts", 0),
            "compression_ratio": data_flow.get("compression_ratio", 0)
        },
        "evidence_and_qa": {
            "total_evidence": data_flow.get("total_evidence", 0),
            "concepts_with_evidence": data_flow.get("concepts_with_evidence", 0),
            "total_qa_pairs": data_flow.get("total_qa_pairs", 0),
            "processed_evidences": data_flow.get("processed_evidences", 0)
        }
    }
    
    # ç”Ÿæˆæ€»ç»“ä¿¡æ¯
    basic_stats = performance_analysis["basic_stats"]
    timing_stats = performance_analysis["timing_stats"]
    
    summary_info = {
        "pipeline_status": "complete" if basic_stats["failed_steps"] == 0 else "partial",
        "overall_success": basic_stats["failed_steps"] == 0,
        "execution_summary": {
            "total_steps": basic_stats["total_steps"],
            "successful_steps": basic_stats["successful_steps"],
            "failed_steps": basic_stats["failed_steps"],
            "skipped_steps": basic_stats["skipped_steps"],
            "success_rate": basic_stats["success_rate"]
        },
        "performance_summary": {
            "total_processing_time": timing_stats["total_time"],
            "average_time_per_step": timing_stats["avg_time_per_step"],
            "slowest_step": timing_stats["slowest_step"],
            "fastest_step": timing_stats["fastest_step"]
        }
    }
    
    return {
        "step_details": step_details,
        "key_metrics": key_metrics,
        "summary_info": summary_info,
        "data_flow_metrics": data_flow
    }

def process_step8_final_summary(step7_result: Dict[str, Any],
                               all_steps_data: Dict[str, Any],
                               config_path: str = "config.yml") -> Dict[str, Any]:
    """
    æ‰§è¡Œæ­¥éª¤8çš„æœ€ç»ˆæ±‡æ€»å¤„ç†
    
    Args:
        step7_result: æ­¥éª¤7çš„ç»“æœ
        all_steps_data: æ‰€æœ‰æ­¥éª¤çš„æ•°æ®
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        Dict[str, Any]: æ­¥éª¤8çš„å¤„ç†ç»“æœ
    """
    start_time = time.time()
    
    try:
        # 1. åŠ è½½é…ç½®
        print("ğŸ“‹ åŠ è½½é…ç½®...")
        config = load_config_from_yaml(config_path)
        
        # 2. æ·»åŠ step7ç»“æœåˆ°æ‰€æœ‰æ­¥éª¤æ•°æ®ä¸­
        all_steps_data["step_results"]["step7"] = step7_result
        all_steps_data["step_timings"]["step7"] = step7_result.get("processing_time", 0.0)
        
        # 3. åˆ†ææµæ°´çº¿æ€§èƒ½
        print("ğŸ“ˆ åˆ†ææµæ°´çº¿æ€§èƒ½...")
        performance_analysis = analyze_pipeline_performance(
            all_steps_data["step_results"],
            all_steps_data["step_timings"]
        )
        
        # 4. ç”Ÿæˆæœ€ç»ˆæ±‡æ€»
        print("ğŸ“Š ç”Ÿæˆæœ€ç»ˆæ±‡æ€»...")
        final_summary = generate_final_summary(
            all_steps_data["step_results"],
            all_steps_data["step_timings"],
            performance_analysis
        )
        
        processing_time = time.time() - start_time
        
        # 5. æ„å»ºç»“æœ
        result = {
            "success": True,
            "step": 8,
            "step_name": "æœ€ç»ˆæ±‡æ€»ä¸æŠ¥å‘Š",
            "pipeline_summary": final_summary,
            "performance_analysis": performance_analysis,
            "all_step_results": all_steps_data["step_results"],
            "all_step_timings": all_steps_data["step_timings"],
            "processing_time": processing_time,
            "timestamp": datetime.now().isoformat(),
            "config_used": {
                "total_steps_analyzed": len(all_steps_data["step_results"]),
                "experiment_management": True
            }
        }
        
        return result
        
    except Exception as e:
        error_msg = f"æ­¥éª¤8å¤„ç†å¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        
        result = {
            "success": False,
            "step": 8,
            "step_name": "æœ€ç»ˆæ±‡æ€»ä¸æŠ¥å‘Š",
            "error": error_msg,
            "processing_time": time.time() - start_time,
            "timestamp": datetime.now().isoformat()
        }
        
        return result

def main():
    if len(sys.argv) != 2:
        print("ç”¨æ³•: python step8.py <step7è¾“å‡ºæ–‡ä»¶æˆ–å®éªŒæ–‡ä»¶å¤¹>")
        print("ç¤ºä¾‹:")
        print("  python step8.py experiments/20241204_143052_attention_paper/step7_qa_generation.txt")
        print("  python step8.py experiments/20241204_143052_attention_paper/")
        print("\næ–°åŠŸèƒ½:")
        print("âœ… è‡ªåŠ¨è¯†åˆ«å®éªŒæ–‡ä»¶å¤¹")
        print("âœ… å®Œæ•´çš„æµæ°´çº¿æ‰§è¡ŒæŠ¥å‘Š")
        print("âœ… è¯¦ç»†çš„æ€§èƒ½åˆ†æå’Œç»Ÿè®¡")
        print("âœ… ç»Ÿä¸€çš„å®éªŒç®¡ç†")
        sys.exit(1)
    
    input_path = sys.argv[1]
    
    print(f"ğŸš€ æ­¥éª¤8: æœ€ç»ˆæ±‡æ€»ä¸æŠ¥å‘Š (å¢å¼ºç‰ˆ)")
    print(f"ğŸ“„ è¾“å…¥: {input_path}")
    print("="*60)
    
    try:
        # 1. åŠ è½½æ­¥éª¤7ç»“æœå’Œå®éªŒç®¡ç†å™¨
        print("ğŸ“‚ åŠ è½½æ­¥éª¤7ç»“æœ...")
        step7_result, experiment_manager = load_step7_result(input_path)
        
        print(f"âœ… å·²åŠ è½½å®éªŒ: {experiment_manager.experiment_name}")
        print(f"ğŸ“ å®éªŒç›®å½•: {experiment_manager.experiment_dir}")
        print()
        
        # 2. åŠ è½½æ‰€æœ‰æ­¥éª¤çš„æ•°æ®
        print("ğŸ“‚ åŠ è½½æ‰€æœ‰æ­¥éª¤æ•°æ®...")
        all_steps_data = load_all_steps_data(experiment_manager)
        
        # 3. æ‰§è¡Œæ­¥éª¤8å¤„ç†
        print("ğŸ”„ å¼€å§‹æ­¥éª¤8å¤„ç†...")
        result = process_step8_final_summary(step7_result, all_steps_data)
        
        # 4. ä¿å­˜ç»“æœåˆ°å®éªŒæ–‡ä»¶å¤¹
        print("ğŸ’¾ ä¿å­˜å®éªŒç»“æœ...")
        saved_files = experiment_manager.save_step_result(
            step_num=8,
            result=result,
            save_formats=['txt', 'json']
        )
        
        if result.get("success"):
            print(f"\nâœ… æ­¥éª¤8å®Œæˆ!")
            print(f"ğŸ“ å®éªŒç›®å½•: {experiment_manager.experiment_dir}")
            print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶:")
            for format_type, file_path in saved_files.items():
                print(f"   - {format_type.upper()}: {file_path}")
            
            # æ˜¾ç¤ºå¤„ç†ç»“æœæ‘˜è¦
            pipeline_summary = result.get("pipeline_summary", {})
            performance_analysis = result.get("performance_analysis", {})
            
            summary_info = pipeline_summary.get("summary_info", {})
            execution_summary = summary_info.get("execution_summary", {})
            performance_summary = summary_info.get("performance_summary", {})
            
            print(f"\nğŸ‰ æµæ°´çº¿æ‰§è¡Œæ±‡æ€»:")
            print(f"   - æµæ°´çº¿çŠ¶æ€: {'âœ… å®Œæ•´' if summary_info.get('overall_success') else 'âš ï¸ éƒ¨åˆ†æˆåŠŸ'}")
            print(f"   - æ€»æ­¥éª¤æ•°: {execution_summary.get('total_steps', 0)}")
            print(f"   - æˆåŠŸæ­¥éª¤: {execution_summary.get('successful_steps', 0)}")
            print(f"   - å¤±è´¥æ­¥éª¤: {execution_summary.get('failed_steps', 0)}")
            print(f"   - è·³è¿‡æ­¥éª¤: {execution_summary.get('skipped_steps', 0)}")
            print(f"   - æˆåŠŸç‡: {execution_summary.get('success_rate', 0):.1%}")
            print(f"   - æ€»å¤„ç†æ—¶é—´: {performance_summary.get('total_processing_time', 0):.2f} ç§’")
            print(f"   - å¹³å‡æ¯æ­¥æ—¶é—´: {performance_summary.get('average_time_per_step', 0):.2f} ç§’")
            
            # æ˜¾ç¤ºæ•°æ®æµæŒ‡æ ‡
            key_metrics = pipeline_summary.get("key_metrics", {})
            
            doc_metrics = key_metrics.get("document_processing", {})
            concept_metrics = key_metrics.get("concept_processing", {})
            qa_metrics = key_metrics.get("evidence_and_qa", {})
            
            print(f"\nğŸ“Š æ•°æ®æµæŒ‡æ ‡:")
            print(f"   ğŸ“„ æ–‡æ¡£å¤„ç†:")
            print(f"      - è¾“å…¥æ–‡æ¡£é•¿åº¦: {doc_metrics.get('input_document_length', 0):,} å­—ç¬¦")
            print(f"      - ç”Ÿæˆåˆ†å—æ•°: {doc_metrics.get('generated_chunks', 0)}")
            print(f"      - å‘é‡åŒ–èŠ‚ç‚¹æ•°: {doc_metrics.get('vectorized_nodes', 0)}")
            
            print(f"   ğŸ§  æ¦‚å¿µå¤„ç†:")
            print(f"      - æå–æ¦‚å¿µæ•°: {concept_metrics.get('extracted_concepts', 0)}")
            print(f"      - é«˜è´¨é‡æ¦‚å¿µæ•°: {concept_metrics.get('high_quality_concepts', 0)}")
            print(f"      - åˆå¹¶åæ¦‚å¿µæ•°: {concept_metrics.get('merged_concepts', 0)}")
            print(f"      - å‹ç¼©æ¯”: {concept_metrics.get('compression_ratio', 0):.2f}")
            
            print(f"   ğŸ“‹ è¯æ®ä¸é—®ç­”:")
            print(f"      - æ€»è¯æ®æ•°: {qa_metrics.get('total_evidence', 0)}")
            print(f"      - æœ‰è¯æ®æ¦‚å¿µæ•°: {qa_metrics.get('concepts_with_evidence', 0)}")
            print(f"      - ç”Ÿæˆé—®ç­”å¯¹æ•°: {qa_metrics.get('total_qa_pairs', 0)}")
            print(f"      - å¤„ç†è¯æ®æ•°: {qa_metrics.get('processed_evidences', 0)}")
            
            # æ˜¾ç¤ºæ­¥éª¤è¯¦æƒ…
            step_details = pipeline_summary.get("step_details", [])
            print(f"\nğŸ“‹ å„æ­¥éª¤æ‰§è¡ŒçŠ¶æ€:")
            for step_detail in step_details:
                print(f"   {step_detail['step_name']}: {step_detail['status']}, è€—æ—¶: {step_detail['processing_time']:.2f} ç§’")
                if step_detail.get('error'):
                    print(f"      é”™è¯¯: {step_detail['error']}")
            
            # æ˜¾ç¤ºå®éªŒçŠ¶æ€
            summary = experiment_manager.get_experiment_summary()
            print(f"\nğŸ§ª å®éªŒçŠ¶æ€:")
            print(f"   - å®éªŒID: {summary['experiment_id']}")
            print(f"   - å·²å®Œæˆæ­¥éª¤: {summary['steps_completed']}/{summary['total_steps']}")
            print(f"   - å½“å‰çŠ¶æ€: {summary['status']}")
            
            # æœ€ç»ˆå®Œæˆæç¤º
            print(f"\nğŸ‰ æ­å–œï¼å®Œæ•´çš„æ–‡æ¡£å¤„ç†æµæ°´çº¿å·²æ‰§è¡Œå®Œæˆ!")
            print(f"   æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š: cat {saved_files['txt']}")
            print(f"   å®éªŒå®Œæ•´ç›®å½•: ls {experiment_manager.experiment_dir}")
            print(f"   JSONæ•°æ®: {saved_files['json']}")
                
        else:
            print(f"âŒ æ­¥éª¤8å¤±è´¥: {result.get('error')}")
            
            # å³ä½¿å¤±è´¥ä¹Ÿä¿å­˜é”™è¯¯ä¿¡æ¯
            experiment_manager.save_step_result(
                step_num=8,
                result=result,
                save_formats=['txt']
            )
            sys.exit(1)
            
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        # å¦‚æœæœ‰å®éªŒç®¡ç†å™¨ï¼Œä¿å­˜é”™è¯¯ä¿¡æ¯
        if 'experiment_manager' in locals():
            error_result = {
                "step": 8,
                "step_name": "æœ€ç»ˆæ±‡æ€»ä¸æŠ¥å‘Š",
                "success": False,
                "error": str(e),
                "traceback": traceback.format_exc(),
                "processing_time": 0.0,
                "timestamp": datetime.now().isoformat()
            }
            
            try:
                experiment_manager.save_step_result(8, error_result, ['txt'])
                print(f"ğŸ“„ é”™è¯¯ä¿¡æ¯å·²ä¿å­˜åˆ°å®éªŒç›®å½•: {experiment_manager.experiment_dir}")
            except:
                pass
        
        sys.exit(1)

if __name__ == "__main__":
    main()
